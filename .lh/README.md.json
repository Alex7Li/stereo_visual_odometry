{
    "sourceFile": "README.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 28,
            "patches": [
                {
                    "date": 1644160646979,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1644160676326,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,5 +6,5 @@\n \n Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n-parameters in the calib_yaml file\n\\ No newline at end of file\n+parameters in the calib_yaml file.\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644160808680,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,5 +6,6 @@\n \n Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n-parameters in the calib_yaml file.\n\\ No newline at end of file\n+parameters in the calib_yaml file.\n+Then we setup a stereo VO object. \n\\ No newline at end of file\n"
                },
                {
                    "date": 1644160874241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,5 +7,5 @@\n Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n parameters in the calib_yaml file.\n-Then we setup a stereo VO object. \n\\ No newline at end of file\n+Then we setup a stereo VO object with these projection matricies.\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644161070609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,5 +7,8 @@\n Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n parameters in the calib_yaml file.\n-Then we setup a stereo VO object with these projection matricies.\n\\ No newline at end of file\n+Then we setup a stereo VO object with these projection matricies, and set up a callback\n+function to periodically update the stereo VO\n+object with cvImages of two images obtained from the camera.\n+\n"
                },
                {
                    "date": 1644161168333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,12 +3,16 @@\n roslaunch stereo_visual_odometry powerranger.launch \n \n # Structure\n \n-Entry point: `stereo_vo.cpp`\n+## Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n parameters in the calib_yaml file.\n Then we setup a stereo VO object with these projection matricies, and set up a callback\n function to periodically update the stereo VO\n object with cvImages of two images obtained from the camera.\n \n+The setup is done in \n+#### StereoVO::run()\n+\n+\n"
                },
                {
                    "date": 1644161305684,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,7 +12,12 @@\n function to periodically update the stereo VO\n object with cvImages of two images obtained from the camera.\n \n The setup is done in \n-#### StereoVO::run()\n+### StereoVO::run()\n \n+first we call `matchingFeatures` to track the \n+most relevant features with FAST detector. We\n+use `bucketingFeatures` to ensure a good spread\n+of features on the image, and use `circularMatching` to ensure that all\n+the points are on all 4 images [l/r, past/present]\n \n"
                },
                {
                    "date": 1644161401629,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,13 +11,13 @@\n Then we setup a stereo VO object with these projection matricies, and set up a callback\n function to periodically update the stereo VO\n object with cvImages of two images obtained from the camera.\n \n-The setup is done in \n ### StereoVO::run()\n \n-first we call `matchingFeatures` to track the \n+first we call `matchingFeatures` to track the\n most relevant features with FAST detector. We\n use `bucketingFeatures` to ensure a good spread\n of features on the image, and use `circularMatching` to ensure that all\n the points are on all 4 images [l/r, past/present]\n \n+Next we \n\\ No newline at end of file\n"
                },
                {
                    "date": 1644161907909,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,5 +19,12 @@\n use `bucketingFeatures` to ensure a good spread\n of features on the image, and use `circularMatching` to ensure that all\n the points are on all 4 images [l/r, past/present]\n \n-Next we \n\\ No newline at end of file\n+This gives us a set of 2d points in camera coordinates, so a triangulation is used between the\n+two cameras to map the coordinates into 3d.\n+\n+However, this is surely a noisy transform. We\n+use RANSAC in `trackingFrame2Frame` to find the least square transformation from camera into real world coordinates.\n+\n+If the rotation is small (for some reason it's detected with euler angles instead of the actual\n+angle change, maybe just bad design?)\n"
                },
                {
                    "date": 1644162327109,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,5 +26,8 @@\n However, this is surely a noisy transform. We\n use RANSAC in `trackingFrame2Frame` to find the least square transformation from camera into real world coordinates.\n \n If the rotation is small (for some reason it's detected with euler angles instead of the actual\n-angle change, maybe just bad design?)\n+angle change, maybe just bad design?), then we\n+call `integrateOdometryStereo`, which updates the\n+current frame pose according to the given transform.\n+It doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense because (despite it's name), scale measures the magnitude of the translation vector.\n"
                },
                {
                    "date": 1644162333853,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,5 +29,5 @@\n If the rotation is small (for some reason it's detected with euler angles instead of the actual\n angle change, maybe just bad design?), then we\n call `integrateOdometryStereo`, which updates the\n current frame pose according to the given transform.\n-It doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense because (despite it's name), scale measures the magnitude of the translation vector.\n+It doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n"
                },
                {
                    "date": 1644162474577,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,4 +30,6 @@\n angle change, maybe just bad design?), then we\n call `integrateOdometryStereo`, which updates the\n current frame pose according to the given transform.\n It doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n+\n+Finally, we send our transform out to update the camera and odometry systems.\n"
                },
                {
                    "date": 1644162566499,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,7 +29,7 @@\n If the rotation is small (for some reason it's detected with euler angles instead of the actual\n angle change, maybe just bad design?), then we\n call `integrateOdometryStereo`, which updates the\n current frame pose according to the given transform.\n-It doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n+`integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n Finally, we send our transform out to update the camera and odometry systems.\n"
                },
                {
                    "date": 1644162631621,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,7 +29,11 @@\n If the rotation is small (for some reason it's detected with euler angles instead of the actual\n angle change, maybe just bad design?), then we\n call `integrateOdometryStereo`, which updates the\n current frame pose according to the given transform.\n+---\n+**NOTE**\n `integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n+---\n+\n Finally, we send our transform out to update the camera and odometry systems.\n"
                },
                {
                    "date": 1644162734289,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,11 +29,8 @@\n If the rotation is small (for some reason it's detected with euler angles instead of the actual\n angle change, maybe just bad design?), then we\n call `integrateOdometryStereo`, which updates the\n current frame pose according to the given transform.\n----\n-**NOTE**\n-`integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big though. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n----\n+> üìù `integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n Finally, we send our transform out to update the camera and odometry systems.\n"
                },
                {
                    "date": 1644285238582,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,17 +3,17 @@\n roslaunch stereo_visual_odometry powerranger.launch \n \n # Structure\n \n-## Entry point: `stereo_vo.cpp`\n+# Entry point: `stereo_vo.cpp`\n \n We first initialize the projection matrix parameters by using the\n parameters in the calib_yaml file.\n Then we setup a stereo VO object with these projection matricies, and set up a callback\n function to periodically update the stereo VO\n object with cvImages of two images obtained from the camera.\n \n-### StereoVO::run()\n+## StereoVO::run()\n \n first we call `matchingFeatures` to track the\n most relevant features with FAST detector. We\n use `bucketingFeatures` to ensure a good spread\n@@ -33,4 +33,6 @@\n \n > üìù `integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n Finally, we send our transform out to update the camera and odometry systems.\n+\n+# Matching features\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644294903591,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,5 +34,17 @@\n > üìù `integrateOdometryStereo` doesn't do this if the `scale` variable is very small or big. Checking if scale is small makes very little sense though, because (despite it's name), scale measures the magnitude of the translation vector.\n \n Finally, we send our transform out to update the camera and odometry systems.\n \n-# Matching features\n\\ No newline at end of file\n+## Matching features\n+\n+This method takes in 4 images and generates all the features\n+that are in all 4.\n+\n+### Feature Detection\n+\n+We use the FAST feature detector to detect keypoints and return their xy coordinates in the image.\n+\n+### Bucketing Features\n+\n+\n+\n"
                },
                {
                    "date": 1644295000349,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,6 +45,6 @@\n We use the FAST feature detector to detect keypoints and return their xy coordinates in the image.\n \n ### Bucketing Features\n \n+Make a grid of buckets and add at most a constant number of features per bucket, preferring to remove young features.\n \n-\n"
                },
                {
                    "date": 1644295018058,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,6 +45,8 @@\n We use the FAST feature detector to detect keypoints and return their xy coordinates in the image.\n \n ### Bucketing Features\n \n-Make a grid of buckets and add at most a constant number of features per bucket, preferring to remove young features.\n+Make a grid of buckets and add at most a constant number of features per bucket, preferring to remove features detected\n+more frames in the past.\n \n+\n"
                },
                {
                    "date": 1644295055575,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,5 +48,5 @@\n \n Make a grid of buckets and add at most a constant number of features per bucket, preferring to remove features detected\n more frames in the past.\n \n-\n+### Circular Matching\n"
                },
                {
                    "date": 1644295697406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n Finally, we send our transform out to update the camera and odometry systems.\n \n ## Matching features\n \n-This method takes in 4 images and generates all the features\n+This method takes in 4 images and generates a set of features\n that are in all 4.\n \n ### Feature Detection\n \n@@ -49,4 +49,8 @@\n Make a grid of buckets and add at most a constant number of features per bucket, preferring to remove features detected\n more frames in the past.\n \n ### Circular Matching\n+\n+Use the Lucas-Kanade flow algorithm to find out, for 4 image pairs, where the feature\n+points move when going from one image to another.\n+\n"
                },
                {
                    "date": 1644295713144,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,10 +36,9 @@\n Finally, we send our transform out to update the camera and odometry systems.\n \n ## Matching features\n \n-This method takes in 4 images and generates a set of features\n-that are in all 4.\n+This method takes in 4 images and generates a set of features that are in all 4 in the given 4 array pointers.\n \n ### Feature Detection\n \n We use the FAST feature detector to detect keypoints and return their xy coordinates in the image.\n"
                },
                {
                    "date": 1644296203156,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,6 +50,10 @@\n \n ### Circular Matching\n \n Use the Lucas-Kanade flow algorithm to find out, for 4 image pairs, where the feature\n-points move when going from one image to another.\n+points move when going from one image to another, as well as a status vector for features whose movement could not be detected.\n \n+#### Delete Unmatched Features Circle\n+\n+Updates the ages of the points (this function is not really a place to do it in good style IMO), then do something that\n+looks like a bug but seems to intend to erase all the points that aren't in each image.\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644296607797,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,5 +55,9 @@\n \n #### Delete Unmatched Features Circle\n \n Updates the ages of the points (this function is not really a place to do it in good style IMO), then do something that\n-looks like a bug but seems to intend to erase all the points that aren't in each image.\n\\ No newline at end of file\n+looks like a bug but seems to intend to erase all the points that aren't in each image.\n+\n+### CheckValidMatch, removeInvalidPoints\n+\n+The circular matching of the 4 images  is a endomorphism of feature points in the t0 image to features that we expect to be the identity. To enforce this, we restrict the domain.\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644296729604,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,5 +59,9 @@\n looks like a bug but seems to intend to erase all the points that aren't in each image.\n \n ### CheckValidMatch, removeInvalidPoints\n \n-The circular matching of the 4 images  is a endomorphism of feature points in the t0 image to features that we expect to be the identity. To enforce this, we restrict the domain.\n\\ No newline at end of file\n+The circular matching of the 4 images is an endomorphism of feature points in the t0 image to features that we expect to be the identity. We restrict the domain to enforce this intuition.\n+\n+## TrackingFrame2Frame\n+\n+use RANSAC in `trackingFrame2Frame` to find the least square transformation from camera into real world coordinates.\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644296996470,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,5 +63,9 @@\n The circular matching of the 4 images is an endomorphism of feature points in the t0 image to features that we expect to be the identity. We restrict the domain to enforce this intuition.\n \n ## TrackingFrame2Frame\n \n-use RANSAC in `trackingFrame2Frame` to find the least square transformation from camera into real world coordinates.\n\\ No newline at end of file\n+use RANSAC in to find the least square transformation from camera into real world coordinates, storing it as a rotation matrix.\n+\n+## RotationMatrixToEulerAngles\n+\n+Quickly figure out the angle of the rotation \n\\ No newline at end of file\n"
                },
                {
                    "date": 1644297388599,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,5 +67,14 @@\n use RANSAC in to find the least square transformation from camera into real world coordinates, storing it as a rotation matrix.\n \n ## RotationMatrixToEulerAngles\n \n-Quickly figure out the angle of the rotation \n\\ No newline at end of file\n+Figure out the angle through which the unit x, y, and z vectors rotate under the rotation from RANSAC.\n+\n+##  Integrate Odometry Stereo\n+\n+Given the rotation and translation matrix,\n+that have been predicted for this frame,\n+we construct a 4d matrix mapping a homogenous 3d point into another homogenous 3d point. \n+\n+We also compute how big the translation is,\n+and report an error with the frame if it's too big. And apparently too small??? \n\\ No newline at end of file\n"
                },
                {
                    "date": 1644297440924,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,8 +73,9 @@\n ##  Integrate Odometry Stereo\n \n Given the rotation and translation matrix,\n that have been predicted for this frame,\n-we construct a 4d matrix mapping a homogenous 3d point into another homogenous 3d point. \n+we construct a 4d matrix mapping a homogenous 3d point into another homogenous 3d point. Then, we use this transform to map\n+the current pose to the next frame pose.\n \n\\ No newline at end of file\n We also compute how big the translation is,\n-and report an error with the frame if it's too big. And apparently too small??? \n+and report an error with the frame if it's too big. And apparently too small??\n\\ No newline at end of file\n"
                },
                {
                    "date": 1644297516052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,5 +77,9 @@\n we construct a 4d matrix mapping a homogenous 3d point into another homogenous 3d point. Then, we use this transform to map\n the current pose to the next frame pose.\n \n We also compute how big the translation is,\n-and report an error with the frame if it's too big. And apparently too small??\n\\ No newline at end of file\n+and report an error with the frame if it's too big. And apparently too small??\n+\n+Also idk what it has to do with integration\n+\n+\n"
                }
            ],
            "date": 1644160646979,
            "name": "Commit-0",
            "content": "# stereo_visual_odometry\nStereo Visual Odometry in ROS  \nroslaunch stereo_visual_odometry powerranger.launch \n\n# Structure\n\nEntry point: `stereo_vo.cpp`\n\nWe first initialize the projection matrix parameters by using the\nparameters in the calib_yaml file"
        }
    ]
}
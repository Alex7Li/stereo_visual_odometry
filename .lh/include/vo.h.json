{
    "sourceFile": "include/vo.h",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 43,
            "patches": [
                {
                    "date": 1647548759479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1647548877100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n #include <opencv2/video/tracking.hpp>\n #include <sstream>\n #include <string>\n #include <vector>\n- \n+\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647549593379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,10 +277,10 @@\n  * @return matchingStatus An array parallel to the points arrays which is true\n  *      at points that were matched correctly.\n  */\n std::vector<uchar> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n-                        const cv::Mat img_2,\n-                        const cv::Mat img_3, std::vector<cv::Point2f> & points_0,\n+                        const cv::Mat img_2, const cv::Mat img_3,\n+                        std::vector<cv::Point2f> & points_0,\n                         std::vector<cv::Point2f> & points_1,\n                         std::vector<cv::Point2f> & points_2,\n                         std::vector<cv::Point2f> & points_3,\n                         std::vector<cv::Point2f> & points_0_return);\n"
                },
                {
                    "date": 1647657129554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n #include <opencv2/video/tracking.hpp>\n #include <sstream>\n #include <string>\n #include <vector>\n-\n+#define dbg(x) std::cerr >> \" >>>> \" << #x << \" = \" << x << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647657791453,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n #include <opencv2/video/tracking.hpp>\n #include <sstream>\n #include <string>\n #include <vector>\n-#define dbg(x) std::cerr >> \" >>>> \" << #x << \" = \" << x << std::endl;\n+#define dbg(x) std::cerr >> \" >>> \" << #x << \" = \" << x << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647657801029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n #include <opencv2/video/tracking.hpp>\n #include <sstream>\n #include <string>\n #include <vector>\n-#define dbg(x) std::cerr >> \" >>> \" << #x << \" = \" << x << std::endl;\n+#define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659610932,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,9 +96,9 @@\n \n /**\n  * @brief Minimum confidence for the robot to report \n  */\n-const int FAST_THRESHOLD = 20;\n+const int FAST_THRESHOLD = 10;\n \n \n /**\n  * @brief A set of locations for image features and their ages.\n"
                },
                {
                    "date": 1647659764587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,8 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n+#define dbgv(a) for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << x << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659784102,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbgv(a) for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << x << std::endl;\n+#define dbgv(a) std::cerr << \" >>> \" << #x << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << x; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659791427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbgv(a) std::cerr << \" >>> \" << #x << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << x; std::cerr << std::endl;\n+#define dbgv(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << a; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659801287,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbgv(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); x++) std::cerr << a; std::cerr << std::endl;\n+#define dbgv(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw]; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659864193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbgv(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw]; std::cerr << std::endl;\n+#define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw]; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647659879190,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n #include <sstream>\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw]; std::cerr << std::endl;\n+#define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1647660253608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,9 +277,9 @@\n  * the circular matching.\n  * @return matchingStatus An array parallel to the points arrays which is true\n  *      at points that were matched correctly.\n  */\n-std::vector<uchar> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n+std::vector<bool> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n                         const cv::Mat img_2, const cv::Mat img_3,\n                         std::vector<cv::Point2f> & points_0,\n                         std::vector<cv::Point2f> & points_1,\n                         std::vector<cv::Point2f> & points_2,\n"
                },
                {
                    "date": 1648064733192,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,14 +130,8 @@\n    */\n   int size() { return points.size(); }\n   \n   /**\n-   * @brief Sort all of the features and their ages based on the\n-   * (x, y) tuple\n-   * \n-   */\n-  void sortFeatures();\n-  /**\n    * @brief Updates the feature set to only include a subset of the\n    * original features which give a good spread throughout the image.\n    *\n    * @param image only use for getting dimension of the image.\n"
                },
                {
                    "date": 1648065006435,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,8 +35,9 @@\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n #define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n+#define dbgs(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1648065018025,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n #define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n-#define dbgs(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n+#define dbgs(a) std::cerr << \" >>> \" << a << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n"
                },
                {
                    "date": 1648065097940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n #include <string>\n #include <vector>\n #define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n #define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n-#define dbgs(a) std::cerr << \" >>> \" << a << std::endl;\n+#define dbgstr(a) std::cerr << \" >>> \" << a << std::endl;\n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n@@ -83,9 +83,10 @@\n  * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n  * buckets.\n  */\n // TODO @Future change to different bucket sizes per axis\n-const int BUCKETS_PER_AXIS = 10;\n+const int BUCKETS_PER_ROW = 10;\n+const int BUCKETS_PER_COL = 10;\n /**\n  * @brief Maximum number of features per bucket\n  */\n const int FEATURES_PER_BUCKET = 4;\n"
                },
                {
                    "date": 1648065541697,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,9 +82,8 @@\n  * @brief Number of buckets along each axis of the image.\n  * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n  * buckets.\n  */\n-// TODO @Future change to different bucket sizes per axis\n const int BUCKETS_PER_ROW = 10;\n const int BUCKETS_PER_COL = 10;\n /**\n  * @brief Maximum number of features per bucket\n"
                },
                {
                    "date": 1648065549883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,13 +135,11 @@\n    * @brief Updates the feature set to only include a subset of the\n    * original features which give a good spread throughout the image.\n    *\n    * @param image only use for getting dimension of the image.\n-   *\n-   * @param bucket_size bucket size in pixel is bucket_size*bucket_size.\n    */\n \n-  void filterByBucketLocation(const cv::Mat &image, const int bucket_size);\n+  void filterByBucketLocation(const cv::Mat &image);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648065772633,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,11 +135,14 @@\n    * @brief Updates the feature set to only include a subset of the\n    * original features which give a good spread throughout the image.\n    *\n    * @param image only use for getting dimension of the image.\n+   * @param buckets_per_[row|col] The number of buckets for points along\n+   *   each row and column of the image. Usually equal to BUCKETS_PER_ROW\n+   *   or BUCKETS_PER_COL (testing is the exception).\n    */\n \n-  void filterByBucketLocation(const cv::Mat &image);\n+  void filterByBucketLocation(const cv::Mat &image, const int buckets_per_row, const int buckets_per_col);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648065932267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,14 +135,11 @@\n    * @brief Updates the feature set to only include a subset of the\n    * original features which give a good spread throughout the image.\n    *\n    * @param image only use for getting dimension of the image.\n-   * @param buckets_per_[row|col] The number of buckets for points along\n-   *   each row and column of the image. Usually equal to BUCKETS_PER_ROW\n-   *   or BUCKETS_PER_COL (testing is the exception).\n    */\n \n-  void filterByBucketLocation(const cv::Mat &image, const int buckets_per_row, const int buckets_per_col);\n+  void filterByBucketLocation(const cv::Mat &image);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648066247441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,8 +138,10 @@\n    * @param image only use for getting dimension of the image.\n    */\n \n   void filterByBucketLocation(const cv::Mat &image);\n+  /* @brief variant with constant parameters passed in, for testing. */\n+  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row) {\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648066259858,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,9 +138,10 @@\n    * @param image only use for getting dimension of the image.\n    */\n \n   void filterByBucketLocation(const cv::Mat &image);\n-  /* @brief variant with constant parameters passed in, for testing. */\n+\n+  /* @brief Variant with constant parameters passed in, for testing. */\n   void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row) {\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n"
                },
                {
                    "date": 1648066328986,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,9 +140,9 @@\n \n   void filterByBucketLocation(const cv::Mat &image);\n \n   /* @brief Variant with constant parameters passed in, for testing. */\n-  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row) {\n+  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row, const int features_per_bucket) {\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648069268305,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,10 +82,10 @@\n  * @brief Number of buckets along each axis of the image.\n  * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n  * buckets.\n  */\n-const int BUCKETS_PER_ROW = 10;\n-const int BUCKETS_PER_COL = 10;\n+const int BUCKETS_ALONG_HEIGHT = 10;\n+const int BUCKETS_ALONG_WIDTH = 10;\n /**\n  * @brief Maximum number of features per bucket\n  */\n const int FEATURES_PER_BUCKET = 4;\n@@ -140,9 +140,9 @@\n \n   void filterByBucketLocation(const cv::Mat &image);\n \n   /* @brief Variant with constant parameters passed in, for testing. */\n-  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row, const int features_per_bucket) {\n+  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row, const int features_per_bucket);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648069280267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n const int BUCKET_START_ROW = 2;\n \n /**\n  * @brief Number of buckets along each axis of the image.\n- * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n+ * In total, there will be BUCKETS_ALONG_HEIGHT * BUCKETS_ALONG_WIDTH\n  * buckets.\n  */\n const int BUCKETS_ALONG_HEIGHT = 10;\n const int BUCKETS_ALONG_WIDTH = 10;\n"
                },
                {
                    "date": 1648069307505,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,9 +140,10 @@\n \n   void filterByBucketLocation(const cv::Mat &image);\n \n   /* @brief Variant with constant parameters passed in, for testing. */\n-  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_per_row, const int buckets_per_col, const int bucket_start_row, const int features_per_bucket);\n+  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_along_height,\n+      const int buckets_along_width, const int bucket_start_row, const int features_per_bucket);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n"
                },
                {
                    "date": 1648071633994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -141,9 +141,9 @@\n   void filterByBucketLocation(const cv::Mat &image);\n \n   /* @brief Variant with constant parameters passed in, for testing. */\n   void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_along_height,\n-      const int buckets_along_width, const int bucket_start_row, const int features_per_bucket);\n+    const int buckets_along_width, const int bucket_start_row, const int features_per_bucket);\n \n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n@@ -320,19 +320,8 @@\n                      const std::vector<cv::Point2f> &points_2,\n                      const int threshold);\n \n /**\n- * @brief Update a vector of points, removing all of the points[i]\n- * in which status[i] is false.\n- * \n- * @param points The vector of points to update.\n- *\n- * @param status A vector indicating which points to remove.\n- */\n-void removeInvalidPoints(std::vector<cv::Point2f> &points,\n-                         const std::vector<bool> &status);\n-\n-/**\n  * @brief Compute the translation and rotation that needs to occur\n  * to obtain the given world points from the camera input points\n  * \n  * @param cameraProjection Camera projection matrix\n"
                },
                {
                    "date": 1648071808427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -317,9 +317,9 @@\n  * @return a vector v where v[i] is true iff |points_1[i] - points_2[i]| <= threshold\n  */\n std::vector<bool> findUnmovedPoints(const std::vector<cv::Point2f> &points_1,\n                      const std::vector<cv::Point2f> &points_2,\n-                     const int threshold);\n+                     const float threshold);\n \n /**\n  * @brief Compute the translation and rotation that needs to occur\n  * to obtain the given world points from the camera input points\n"
                },
                {
                    "date": 1648149616625,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,10 +229,12 @@\n    *\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n+   * @return (rotation, translation): The 3x3 rotation and translation matrix of the robot\n+   * rel\n    */\n-  void stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n+  std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n \n /**\n  * @brief Use the FAST feature detector to accumulate the features in image into\n"
                },
                {
                    "date": 1648149627497,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,9 +230,9 @@\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n    * @return (rotation, translation): The 3x3 rotation and translation matrix of the robot\n-   * rel\n+   * relative to the original location\n    */\n   std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n \n"
                },
                {
                    "date": 1648149702414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,9 +230,9 @@\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n    * @return (rotation, translation): The 3x3 rotation and translation matrix of the robot\n-   * relative to the original location\n+   * relative to the last frame\n    */\n   std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n \n"
                },
                {
                    "date": 1648149715882,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,9 +230,9 @@\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n    * @return (rotation, translation): The 3x3 rotation and translation matrix of the robot\n-   * relative to the last frame\n+   * relative to the previous frame.\n    */\n   std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n \n"
                },
                {
                    "date": 1648149776168,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,9 +229,9 @@\n    *\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n-   * @return (rotation, translation): The 3x3 rotation and translation matrix of the robot\n+   * @return (translation, rotation): The 3x1 translation and 3x3 rotation matrix of the robot\n    * relative to the previous frame.\n    */\n   std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n"
                },
                {
                    "date": 1648150014544,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,9 +229,9 @@\n    *\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n-   * @return (translation, rotation): The 3x1 translation and 3x3 rotation matrix of the robot\n+   * @return (translation, rotation): The 3x1 translation and 3x3 rotation matrix of the robot,\n    * relative to the previous frame.\n    */\n   std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n@@ -285,22 +285,8 @@\n                         std::vector<cv::Point2f> & points_3,\n                         std::vector<cv::Point2f> & points_0_return);\n \n /**\n- * @brief Compute the next pose from the current one\n- * given the rotation and translation in the frame.\n- * Essentially a multiplication of homogeneous transforms.\n- * \n- * @param frame_pose The original position of the robot, will be modified.\n- *\n- * @param rotation The rotation to go through.\n- *\n- * @param translation_stereo The translation to go through.\n- */\n-void integrateOdometryStereo(cv::Mat &frame_pose,\n-                             const cv::Mat &rotation,\n-                             const cv::Mat &translation_stereo);\n-/**\n  * @brief Compute the three euler angles for a given rotation matrix.\n  *\n  * @param R A rotation matrix\n  *\n"
                },
                {
                    "date": 1648150144155,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -346,6 +346,20 @@\n                       std::vector<cv::Point2f> &pointsLeftT0,\n                       std::vector<cv::Point2f> &pointsRightT0,\n                       std::vector<cv::Point2f> &pointsLeftT1,\n                       std::vector<cv::Point2f> &pointsRightT1);\n+\n+/**\n+ * @brief Compute the next pose from the current one\n+ * given the rotation and translation in the frame.\n+ * Essentially a multiplication of homogeneous transforms.\n+ * \n+ * @param frame_pose The original position of the robot, will be modified.\n+ *\n+ * @param rotation The rotation to go through.\n+ *\n+ * @param translation_stereo The translation to go through.\n+ */\n+void integrateOdometryStereo(cv::Mat &frame_pose, const cv::Mat &rotation,\n+                              const cv::Mat &translation_stereo);\n } // namespace visual_odometry\n #endif\n"
                },
                {
                    "date": 1648151448386,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,9 +93,9 @@\n /**\n  * @brief Ignore all features that have been around but not detected\n  * for this many frames.\n  */\n-const int AGE_THRESHOLD = 10;\n+const int AGE_THRESHOLD = 40;\n \n /**\n  * @brief Minimum confidence for the robot to report \n  */\n"
                },
                {
                    "date": 1648151477536,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -93,9 +93,9 @@\n /**\n  * @brief Ignore all features that have been around but not detected\n  * for this many frames.\n  */\n-const int AGE_THRESHOLD = 40;\n+const int AGE_THRESHOLD = 10;\n \n /**\n  * @brief Minimum confidence for the robot to report \n  */\n"
                },
                {
                    "date": 1648151709971,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -203,10 +203,8 @@\n   cv::Mat imageRightT0_, imageLeftT0_;\n   cv::Mat imageRightT1_, imageLeftT1_;\n \n   /* Initial pose variables. */\n-  cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);\n-  cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);\n   cv::Mat frame_pose = cv::Mat::eye(4, 4, CV_64F);\n \n   /* Set of features currently tracked. */\n   FeatureSet currentVOFeatures;\n"
                },
                {
                    "date": 1648587110489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,8 +150,9 @@\n    *\n    * @param image The image to obtain all points from.\n    */\n   void appendFeaturesFromImage(const cv::Mat &image);\n+  void appendGridOfFeatures(const cv::Mat &image);\n };\n \n /**\n  * @brief A class to allow storing a set of at most max_size\n"
                },
                {
                    "date": 1648587132878,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,8 +150,14 @@\n    *\n    * @param image The image to obtain all points from.\n    */\n   void appendFeaturesFromImage(const cv::Mat &image);\n+  /**\n+   * @brief Apply a feature detection algorithm over the image to generate new\n+   * features, and all all such features into this feature set.\n+   *\n+   * @param image Image dimensions are used for getting boundaries of the grid.\n+   */\n   void appendGridOfFeatures(const cv::Mat &image);\n };\n \n /**\n"
                },
                {
                    "date": 1648587150877,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,10 +151,9 @@\n    * @param image The image to obtain all points from.\n    */\n   void appendFeaturesFromImage(const cv::Mat &image);\n   /**\n-   * @brief Apply a feature detection algorithm over the image to generate new\n-   * features, and all all such features into this feature set.\n+   * @brief  Create a grid of feature points to cover a given image\n    *\n    * @param image Image dimensions are used for getting boundaries of the grid.\n    */\n   void appendGridOfFeatures(const cv::Mat &image);\n"
                },
                {
                    "date": 1648602442555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,19 +25,16 @@\n #include <iostream>\n #include <iterator>\n #include <opencv2/calib3d/calib3d.hpp>\n #include <opencv2/features2d/features2d.hpp>\n-#include <opencv2/xfeatures2d.hpp>\n #include <opencv2/highgui/highgui.hpp>\n #include <opencv2/imgproc/imgproc.hpp>\n #include <opencv2/opencv.hpp>\n #include <opencv2/video/tracking.hpp>\n #include <sstream>\n #include <string>\n #include <vector>\n-#define dbg(x) std::cerr << \" >>> \" << #x << \" = \" << x << std::endl;\n-#define dbga(a) std::cerr << \" >>> \" << #a << \" = \"; for(unsigned int xtw = 0; xtw < a.size(); xtw++) std::cerr << a[xtw] << \" \"; std::cerr << std::endl;\n-#define dbgstr(a) std::cerr << \" >>> \" << a << std::endl;\n+ \n /************\n  * ROS ONLY *\n  ************/\n // #include \"ros/ros.h\"\n@@ -71,39 +68,26 @@\n */\n /* END CFS ONLY */\n \n namespace visual_odometry {\n-\n /**\n- * @brief Number of buckets to divide the image into.\n- */\n-const int BUCKET_START_ROW = 2;\n-\n-/**\n  * @brief Number of buckets along each axis of the image.\n- * In total, there will be BUCKETS_ALONG_HEIGHT * BUCKETS_ALONG_WIDTH\n+ * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n  * buckets.\n  */\n-const int BUCKETS_ALONG_HEIGHT = 10;\n-const int BUCKETS_ALONG_WIDTH = 10;\n+// TODO @Future change to different bucket sizes per axis\n+int BUCKETS_PER_AXIS = 10;\n /**\n  * @brief Maximum number of features per bucket\n  */\n-const int FEATURES_PER_BUCKET = 4;\n-\n+int FEATURES_PER_BUCKET = 1;\n /**\n  * @brief Ignore all features that have been around but not detected\n  * for this many frames.\n  */\n-const int AGE_THRESHOLD = 10;\n+int AGE_THRESHOLD = 10;\n \n /**\n- * @brief Minimum confidence for the robot to report \n- */\n-const int FAST_THRESHOLD = 10;\n-\n-\n-/**\n  * @brief A set of locations for image features and their ages.\n  */\n class FeatureSet {\n public:\n@@ -115,49 +99,30 @@\n    * @brief A parallel set to points; ages[i] contains\n    * the number of iterations that points[i] has been around.\n    */\n   std::vector<int> ages;\n-\n-  /**\n-   * @brief A parallel set to points; strengths[i] contains\n-   * the clarity of the features reported by the feature detector.\n-   */\n-  std::vector<int> strengths;\n-\n-  /**\n-   * @brief Return the size of the feature set. Note that\n-   * points.size() == ages.size()\n-   * \n-   * @return size: number of points in the set.\n-   */\n   int size() { return points.size(); }\n-  \n   /**\n    * @brief Updates the feature set to only include a subset of the\n    * original features which give a good spread throughout the image.\n    *\n    * @param image only use for getting dimension of the image.\n+   *\n+   * @param bucket_size bucket size in pixel is bucket_size*bucket_size.\n+   *\n+   * @param features_per_bucket: number of selected features per bucket.\n    */\n \n-  void filterByBucketLocation(const cv::Mat &image);\n+  void filterByBucketLocation(const cv::Mat &image, const int bucket_size,\n+                              const int features_per_bucket);\n \n-  /* @brief Variant with constant parameters passed in, for testing. */\n-  void filterByBucketLocationInternal(const cv::Mat & image, const int buckets_along_height,\n-    const int buckets_along_width, const int bucket_start_row, const int features_per_bucket);\n-\n   /**\n    * @brief Apply a feature detection algorithm over the image to generate new\n    * features, and all all such features into this feature set.\n    *\n    * @param image The image to obtain all points from.\n    */\n   void appendFeaturesFromImage(const cv::Mat &image);\n-  /**\n-   * @brief  Create a grid of feature points to cover a given image\n-   *\n-   * @param image Image dimensions are used for getting boundaries of the grid.\n-   */\n-  void appendGridOfFeatures(const cv::Mat &image);\n };\n \n /**\n  * @brief A class to allow storing a set of at most max_size\n@@ -176,22 +141,14 @@\n   Bucket(int max_size);\n   ~Bucket();\n \n   /**\n-   * @brief Rank how good a feature is based on it's current\n-   * age and strength. Older points that have survived many\n-   * iterations are desirable, as are ones that were detected\n-   * strongly.\n-   */\n-  int compute_score(const int age, const int strength);\n-  /**\n    * @brief Add a feature to the bucket\n    *\n    * @param point The location of the feature to add\n    * @param age The number of iterations since this feature was detected.\n-   * @param strength The strength of the detected feature.\n    */\n-  void add_feature(const cv::Point2f point, const int age, const int strength);\n+  void add_feature(const cv::Point2f point, const int age);\n   \n   /**\n    * @return int The size of the feature set\n    */\n@@ -209,8 +166,10 @@\n   cv::Mat imageRightT0_, imageLeftT0_;\n   cv::Mat imageRightT1_, imageLeftT1_;\n \n   /* Initial pose variables. */\n+  cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);\n+  cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);\n   cv::Mat frame_pose = cv::Mat::eye(4, 4, CV_64F);\n \n   /* Set of features currently tracked. */\n   FeatureSet currentVOFeatures;\n@@ -233,40 +192,37 @@\n    *\n    * @param image_left The left image from stereo camera\n    *\n    * @param image_right The right image from stereo camera\n-   * @return (translation, rotation): The 3x1 translation and 3x3 rotation matrix of the robot,\n-   * relative to the previous frame.\n    */\n-  std::pair<cv::Mat, cv::Mat> stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n+  void stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n };\n \n /**\n  * @brief Use the FAST feature detector to accumulate the features in image into\n  * points.\n  *\n  * @param image The image we're detecting.\n- * @param response_strengths: A vector to fill with the response strength of each newly detected feature.\n  *\n  * @return A vector with the locations of all newly detected features.\n  */\n-std::vector<cv::Point2f> featureDetectionFast(const cv::Mat image, std::vector<float>& response_strengths);\n+std::vector<cv::Point2f> featureDetectionFast(const cv::Mat image);\n \n /**\n  * @brief Given parallel vectors of points, ages, and the status of those points,\n  * update the vectors by removing elements with a invalid status.\n  *\n  * @param points[0..4] vectors of points to update based on status, each with\n  * the same length as status_all.\n  *\n- * @param currentFeatures Current set of features we will need to update.\n+ * @param ages Current ages of each of the points.\n  *\n  * @param status_all a vector with 1 If the point is valid, and 0 if it should be discarded.\n  */\n void deleteFeaturesWithFailureStatus(\n     std::vector<cv::Point2f> &points0, std::vector<cv::Point2f> &points1,\n     std::vector<cv::Point2f> &points2, std::vector<cv::Point2f> &points3,\n-    std::vector<cv::Point2f> &points4, FeatureSet &currentFeatures,\n+    std::vector<cv::Point2f> &points4, std::vector<int> &ages,\n     const std::vector<bool> &status_all);\n \n /**\n  * @brief Perform circular matching on 4 images and\n@@ -280,17 +236,31 @@\n  * the circular matching.\n  * @return matchingStatus An array parallel to the points arrays which is true\n  *      at points that were matched correctly.\n  */\n-std::vector<bool> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n-                        const cv::Mat img_2, const cv::Mat img_3,\n-                        std::vector<cv::Point2f> & points_0,\n+std::vector<uchar> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n+                        const cv::Mat img_2,\n+                        const cv::Mat img_3, std::vector<cv::Point2f> & points_0,\n                         std::vector<cv::Point2f> & points_1,\n                         std::vector<cv::Point2f> & points_2,\n                         std::vector<cv::Point2f> & points_3,\n                         std::vector<cv::Point2f> & points_0_return);\n \n /**\n+ * @brief Compute the next pose from the current one\n+ * given the rotation and translation in the frame.\n+ * Essentially a multiplication of homogeneous transforms.\n+ * \n+ * @param frame_pose The original position of the robot, will be modified.\n+ *\n+ * @param rotation The rotation to go through.\n+ *\n+ * @param translation_stereo The translation to go through.\n+ */\n+void integrateOdometryStereo(cv::Mat &frame_pose,\n+                             const cv::Mat &rotation,\n+                             const cv::Mat &translation_stereo);\n+/**\n  * @brief Compute the three euler angles for a given rotation matrix.\n  *\n  * @param R A rotation matrix\n  *\n@@ -309,11 +279,22 @@\n  * @return a vector v where v[i] is true iff |points_1[i] - points_2[i]| <= threshold\n  */\n std::vector<bool> findUnmovedPoints(const std::vector<cv::Point2f> &points_1,\n                      const std::vector<cv::Point2f> &points_2,\n-                     const float threshold);\n+                     const int threshold);\n \n /**\n+ * @brief Update a vector of points, removing all of the points[i]\n+ * in which status[i] is false.\n+ * \n+ * @param points The vector of points to update.\n+ *\n+ * @param status A vector indicating which points to remove.\n+ */\n+void removeInvalidPoints(std::vector<cv::Point2f> &points,\n+                         const std::vector<bool> &status);\n+\n+/**\n  * @brief Compute the translation and rotation that needs to occur\n  * to obtain the given world points from the camera input points\n  * \n  * @param cameraProjection Camera projection matrix\n@@ -350,20 +331,6 @@\n                       std::vector<cv::Point2f> &pointsLeftT0,\n                       std::vector<cv::Point2f> &pointsRightT0,\n                       std::vector<cv::Point2f> &pointsLeftT1,\n                       std::vector<cv::Point2f> &pointsRightT1);\n-\n-/**\n- * @brief Compute the next pose from the current one\n- * given the rotation and translation in the frame.\n- * Essentially a multiplication of homogeneous transforms.\n- * \n- * @param frame_pose The original position of the robot, will be modified.\n- *\n- * @param rotation The rotation to go through.\n- *\n- * @param translation_stereo The translation to go through.\n- */\n-void integrateOdometryStereo(cv::Mat &frame_pose, const cv::Mat &rotation,\n-                              const cv::Mat &translation_stereo);\n } // namespace visual_odometry\n #endif\n"
                }
            ],
            "date": 1647548759479,
            "name": "Commit-0",
            "content": "/****************************************************************\n *\n * @file \t\tvo.h\n *\n * @brief \t\tThe Visual Odometry class being used for\n translation. The math can be found in Haidar Jamal's Thesis:\n *https://www.ri.cmu.edu/publications/localization-for-lunar-micro-rovers/\n *\n * @version \t1.0\n * @date \t\t02/09/2022\n *\n * @authors \tBen Kolligs, Alex Li\n * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n *\n ****************************************************************/\n#ifndef VO_H__\n#define VO_H__\n#include <ctype.h>\n#include <math.h>\n\n#include <algorithm>\n#include <chrono>\n#include <ctime>\n#include <fstream>\n#include <iostream>\n#include <iterator>\n#include <opencv2/calib3d/calib3d.hpp>\n#include <opencv2/features2d/features2d.hpp>\n#include <opencv2/xfeatures2d.hpp>\n#include <opencv2/highgui/highgui.hpp>\n#include <opencv2/imgproc/imgproc.hpp>\n#include <opencv2/opencv.hpp>\n#include <opencv2/video/tracking.hpp>\n#include <sstream>\n#include <string>\n#include <vector>\n \n/************\n * ROS ONLY *\n ************/\n// #include \"ros/ros.h\"\n// #include \"sensor_msgs/Image.h\"\n// #include \"std_msgs/Int32MultiArray.h\"\n// #include \"nav_msgs/Odometry.h\"\n// #include \"geometry_msgs/Quaternion.h\"\n// #include <tf/transform_broadcaster.h>\n// #include <message_filters/subscriber.h>\n// #include <message_filters/synchronizer.h>\n// #include <message_filters/sync_policies/approximate_time.h>\n// #include <cv_bridge/cv_bridge.h>\n\n// #include <eigen3/Eigen/Dense>\n// #include <eigen3/Eigen/Core>\n// #include <eigen3/Eigen/Geometry> \n/* END ROS ONLY */\n\n/************\n * CFS ONLY *\n ************/\n/*\n#include \"Core\"\n#include \"Dense\"\n\nextern \"C\" {\n#include \"cfe_error.h\"\n#include \"common_types.h\"\n#include \"pe_events.h\"\n}\n*/\n/* END CFS ONLY */\n\nnamespace visual_odometry {\n\n/**\n * @brief Number of buckets to divide the image into.\n */\nconst int BUCKET_START_ROW = 2;\n\n/**\n * @brief Number of buckets along each axis of the image.\n * In total, there will be BUCKETS_PER_AXIS * BUCKETS_PER_AXIS\n * buckets.\n */\n// TODO @Future change to different bucket sizes per axis\nconst int BUCKETS_PER_AXIS = 10;\n/**\n * @brief Maximum number of features per bucket\n */\nconst int FEATURES_PER_BUCKET = 4;\n\n/**\n * @brief Ignore all features that have been around but not detected\n * for this many frames.\n */\nconst int AGE_THRESHOLD = 10;\n\n/**\n * @brief Minimum confidence for the robot to report \n */\nconst int FAST_THRESHOLD = 20;\n\n\n/**\n * @brief A set of locations for image features and their ages.\n */\nclass FeatureSet {\npublic:\n  /**\n   * @brief The points stored in this set of features.\n   */\n  std::vector<cv::Point2f> points;\n  /**\n   * @brief A parallel set to points; ages[i] contains\n   * the number of iterations that points[i] has been around.\n   */\n  std::vector<int> ages;\n\n  /**\n   * @brief A parallel set to points; strengths[i] contains\n   * the clarity of the features reported by the feature detector.\n   */\n  std::vector<int> strengths;\n\n  /**\n   * @brief Return the size of the feature set. Note that\n   * points.size() == ages.size()\n   * \n   * @return size: number of points in the set.\n   */\n  int size() { return points.size(); }\n  \n  /**\n   * @brief Sort all of the features and their ages based on the\n   * (x, y) tuple\n   * \n   */\n  void sortFeatures();\n  /**\n   * @brief Updates the feature set to only include a subset of the\n   * original features which give a good spread throughout the image.\n   *\n   * @param image only use for getting dimension of the image.\n   *\n   * @param bucket_size bucket size in pixel is bucket_size*bucket_size.\n   */\n\n  void filterByBucketLocation(const cv::Mat &image, const int bucket_size);\n\n  /**\n   * @brief Apply a feature detection algorithm over the image to generate new\n   * features, and all all such features into this feature set.\n   *\n   * @param image The image to obtain all points from.\n   */\n  void appendFeaturesFromImage(const cv::Mat &image);\n};\n\n/**\n * @brief A class to allow storing a set of at most max_size\n * image features, and remove outdated features to satisfy this\n * constraint.\n **/\nclass Bucket {\npublic:\n  int max_size;\n\n  /**\n   * @brief The set of features stored in this bucket.\n   */\n  FeatureSet features;\n\n  Bucket(int max_size);\n  ~Bucket();\n\n  /**\n   * @brief Rank how good a feature is based on it's current\n   * age and strength. Older points that have survived many\n   * iterations are desirable, as are ones that were detected\n   * strongly.\n   */\n  int compute_score(const int age, const int strength);\n  /**\n   * @brief Add a feature to the bucket\n   *\n   * @param point The location of the feature to add\n   * @param age The number of iterations since this feature was detected.\n   * @param strength The strength of the detected feature.\n   */\n  void add_feature(const cv::Point2f point, const int age, const int strength);\n  \n  /**\n   * @return int The size of the feature set\n   */\n  int size();\n};\n\nclass VisualOdometry {\nprivate:\n  /* Number of frames seen so far. */\n  int frame_id = 0;\n  /* Projection matrices for the left and right cameras. */\n  cv::Mat leftCameraProjection_, rightCameraProjection_;\n\n  /* Images at current and next time step. */\n  cv::Mat imageRightT0_, imageLeftT0_;\n  cv::Mat imageRightT1_, imageLeftT1_;\n\n  /* Initial pose variables. */\n  cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);\n  cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);\n  cv::Mat frame_pose = cv::Mat::eye(4, 4, CV_64F);\n\n  /* Set of features currently tracked. */\n  FeatureSet currentVOFeatures;\n\npublic:\n  /**\n   * @brief Construct a new Visual Odometry object\n   *\n   * @param ProjMatrl Left camera projection matrix\n   *\n   * @param projMatrr Right camera projection matrix\n   */\n  VisualOdometry(const cv::Mat ProjMatrl, const cv::Mat projMatrr);\n\n  ~VisualOdometry();\n\n  /**\n   * @brief Process one time step of camera imagery and\n   * publish the result.\n   *\n   * @param image_left The left image from stereo camera\n   *\n   * @param image_right The right image from stereo camera\n   */\n  void stereo_callback(const cv::Mat &image_left, const cv::Mat &image_right);\n};\n\n/**\n * @brief Use the FAST feature detector to accumulate the features in image into\n * points.\n *\n * @param image The image we're detecting.\n * @param response_strengths: A vector to fill with the response strength of each newly detected feature.\n *\n * @return A vector with the locations of all newly detected features.\n */\nstd::vector<cv::Point2f> featureDetectionFast(const cv::Mat image, std::vector<float>& response_strengths);\n\n/**\n * @brief Given parallel vectors of points, ages, and the status of those points,\n * update the vectors by removing elements with a invalid status.\n *\n * @param points[0..4] vectors of points to update based on status, each with\n * the same length as status_all.\n *\n * @param currentFeatures Current set of features we will need to update.\n *\n * @param status_all a vector with 1 If the point is valid, and 0 if it should be discarded.\n */\nvoid deleteFeaturesWithFailureStatus(\n    std::vector<cv::Point2f> &points0, std::vector<cv::Point2f> &points1,\n    std::vector<cv::Point2f> &points2, std::vector<cv::Point2f> &points3,\n    std::vector<cv::Point2f> &points4, FeatureSet &currentFeatures,\n    const std::vector<bool> &status_all);\n\n/**\n * @brief Perform circular matching on 4 images and\n * detect points not found in both cameras for both the previous and\n * current frame.\n * @param img_[0,3] Images of the same scene taken by different cameras at different times.\n * @param points_[0,3] Features of the scene detected by the cameras.\n * @param points_0_return The locations of points in points_0 after mapping them to\n * points_1, points_2, points_3, and then back to points_0.\n * @param current_features The current feature set to consider while performing\n * the circular matching.\n * @return matchingStatus An array parallel to the points arrays which is true\n *      at points that were matched correctly.\n */\nstd::vector<uchar> circularMatching(const cv::Mat img_0, const cv::Mat img_1, \n                        const cv::Mat img_2,\n                        const cv::Mat img_3, std::vector<cv::Point2f> & points_0,\n                        std::vector<cv::Point2f> & points_1,\n                        std::vector<cv::Point2f> & points_2,\n                        std::vector<cv::Point2f> & points_3,\n                        std::vector<cv::Point2f> & points_0_return);\n\n/**\n * @brief Compute the next pose from the current one\n * given the rotation and translation in the frame.\n * Essentially a multiplication of homogeneous transforms.\n * \n * @param frame_pose The original position of the robot, will be modified.\n *\n * @param rotation The rotation to go through.\n *\n * @param translation_stereo The translation to go through.\n */\nvoid integrateOdometryStereo(cv::Mat &frame_pose,\n                             const cv::Mat &rotation,\n                             const cv::Mat &translation_stereo);\n/**\n * @brief Compute the three euler angles for a given rotation matrix.\n *\n * @param R A rotation matrix\n *\n * @return cv::Vec3f (x, y, z) euler angles for R.\n */\ncv::Vec3f rotationMatrixToEulerAngles(const cv::Mat &R);\n\n/**\n * @brief Given two vectors of points, find the locations where they\n * differ.\n * \n * @param points_[1..2] The vectors of points to compare.\n *\n * @param threshold The distance at which to consider the points moved.\n *\n * @return a vector v where v[i] is true iff |points_1[i] - points_2[i]| <= threshold\n */\nstd::vector<bool> findUnmovedPoints(const std::vector<cv::Point2f> &points_1,\n                     const std::vector<cv::Point2f> &points_2,\n                     const int threshold);\n\n/**\n * @brief Update a vector of points, removing all of the points[i]\n * in which status[i] is false.\n * \n * @param points The vector of points to update.\n *\n * @param status A vector indicating which points to remove.\n */\nvoid removeInvalidPoints(std::vector<cv::Point2f> &points,\n                         const std::vector<bool> &status);\n\n/**\n * @brief Compute the translation and rotation that needs to occur\n * to obtain the given world points from the camera input points\n * \n * @param cameraProjection Camera projection matrix\n *\n * @param cameraPoints Points from the perspective of the camera\n *\n * @param worldPoints Same points in the real world\n *\n * @param rotation Matrix to store the estimated rotation output in.\n *\n * @param translation Matrix to store the estimated translation in.\n */\nvoid cameraToWorld(const cv::Mat &cameraProjection,\n                const std::vector<cv::Point2f> &cameraPoints,\n                const cv::Mat &worldPoints, cv::Mat &rotation,\n                cv::Mat &translation);\n/**\n * @brief Given four images and the set of features used in the last\n * iteration, this method finds new features in the images and appends\n * each of the locations of the features in each image to 4 parallel vectors.\n * \n * Calls many of the above functions in a pipeline.\n * appendFeaturesFromImage -> filterByBucketLocation -> circularMatching -> \n * deleteFeaturesWithFailureStatus -> findUnmovedPoints -> removeInvalidPoints\n * \n * @param image[(Left)|(Right)][01]: Images from the left/right cameras at the last/current timestep.\n * @param currentVOFeatures: The set of currently tracked features, stored as a position in the LeftT0 image, will\n * be updated with newly detected features.\n * @param points[(Left)|(Right)][01]: references to 4 empty vectors of points to fill up with feature positions.\n */\nvoid matchingFeatures(const cv::Mat &imageLeftT0, const cv::Mat &imageRightT0,\n                      const cv::Mat &imageLeftT1, const cv::Mat &imageRightT1,\n                      FeatureSet &currentVOFeatures,\n                      std::vector<cv::Point2f> &pointsLeftT0,\n                      std::vector<cv::Point2f> &pointsRightT0,\n                      std::vector<cv::Point2f> &pointsLeftT1,\n                      std::vector<cv::Point2f> &pointsRightT1);\n} // namespace visual_odometry\n#endif\n"
        }
    ]
}
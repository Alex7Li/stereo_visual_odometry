{
    "sourceFile": "src/pose_estimation_node.cpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 63,
            "patches": [
                {
                    "date": 1648602688933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1648602723602,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,15 +83,15 @@\n void PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n     if (!frame_id)\n     {\n-        imageLeft_t0 = rosImage2CvMat(image_left);\n-        imageRight_t0 = rosImage2CvMat(image_right);\n+        imageLeft_t0 = image_left;\n+        imageRight_t0 = image_right;\n         frame_id++;\n         return;\n     }\n-    imageLeft_t1 = rosImage2CvMat(image_left);\n-    imageRight_t1 = rosImage2CvMat(image_right);\n+    imageLeft_t1 = image_left;\n+    imageRight_t1 = image_right;\n \n     frame_id++;\n \n     // start the vo pipeline after orientation is initialized\n"
                },
                {
                    "date": 1648603143888,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,59 +159,59 @@\n             }\n         }\n     }\n \n-    if (vo_usable)\n-    {\n-    \tdebug(\"[node]: Using VO update\");\n-    \t// rotate vo translation to rover frame\n-    \tEigen::Matrix<double,3,1> vo_trans_rover_frame = q_bc._transformVector(vo_translation);\n-    \t// rotate vo translation in rover frame to global frame\n-        Eigen::Quaternion<double> rot_ib = vo_rot.slerp(0.5, current_rot);\n-    \tEigen::Matrix<double,3,1> vo_trans_global_frame = rot_ib._transformVector(vo_trans_rover_frame);\n-    \t// add to global position\n-    \tglobal_pos += vo_trans_global_frame;\n-    } else \n-    {\n-    \tdebug(\"[node]: Using encoder update\");\n-    \t// add to global position\n-    \tglobal_pos += encoders_translation;\n-    }\n+  //   if (vo_usable)\n+  //   {\n+  //   \tdebug(\"[node]: Using VO update\");\n+  //   \t// rotate vo translation to rover frame\n+  //   \tEigen::Matrix<double,3,1> vo_trans_rover_frame = q_bc._transformVector(vo_translation);\n+  //   \t// rotate vo translation in rover frame to global frame\n+  //       Eigen::Quaternion<double> rot_ib = vo_rot.slerp(0.5, current_rot);\n+  //   \tEigen::Matrix<double,3,1> vo_trans_global_frame = rot_ib._transformVector(vo_trans_rover_frame);\n+  //   \t// add to global position\n+  //   \tglobal_pos += vo_trans_global_frame;\n+  //   } else \n+  //   {\n+  //   \tdebug(\"[node]: Using encoder update\");\n+  //   \t// add to global position\n+  //   \tglobal_pos += encoders_translation;\n+  //   }\n \n-    if (logging_path)\n-    {\n-        static auto time_init = std::chrono::steady_clock::now();\n-        double time_now = (double)(std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now()- time_init).count())/1000;\n-        output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n-        debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n-    }\n+  //   if (logging_path)\n+  //   {\n+  //       static auto time_init = std::chrono::steady_clock::now();\n+  //       double time_now = (double)(std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now()- time_init).count())/1000;\n+  //       output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n+  //       debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n+  //   }\n \t\n-\tstd::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n+\t// std::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n \n-    // update rotation for vo.\n-    vo_rot = current_rot; // later on do slerp between current and previous \n-    vo_translation << 0,0,0;\n-    encoders_translation << 0,0,0;\n+  //   // update rotation for vo.\n+  //   vo_rot = current_rot; // later on do slerp between current and previous \n+  //   vo_translation << 0,0,0;\n+  //   encoders_translation << 0,0,0;\n \n-    // publish transforms\n-    static tf::TransformBroadcaster br;\n-    tf::Transform transform;\n-    transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n-    tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n-    transform.setRotation(q);\n-    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n+  //   // publish transforms\n+  //   static tf::TransformBroadcaster br;\n+  //   tf::Transform transform;\n+  //   transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n+  //   tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n+  //   transform.setRotation(q);\n+  //   br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n \n-\tnav_msgs::Odometry odom;\n-\todom.header.stamp = ros::Time::now();\n-\todom.header.frame_id = \"map\";\n-\todom.pose.pose.position.x = global_pos[0];\n-\todom.pose.pose.position.y = global_pos[1];\n-\todom.pose.pose.position.z = global_pos[2];\n-\todom.pose.pose.orientation.x = current_rot.x();\n-\todom.pose.pose.orientation.y = current_rot.y();\n-\todom.pose.pose.orientation.z = current_rot.z();\n-\todom.pose.pose.orientation.w = current_rot.w();\n-\t(*pub_ptr).publish(odom);\n+\t// nav_msgs::Odometry odom;\n+\t// odom.header.stamp = ros::Time::now();\n+\t// odom.header.frame_id = \"map\";\n+\t// odom.pose.pose.position.x = global_pos[0];\n+\t// odom.pose.pose.position.y = global_pos[1];\n+\t// odom.pose.pose.position.z = global_pos[2];\n+\t// odom.pose.pose.orientation.x = current_rot.x();\n+\t// odom.pose.pose.orientation.y = current_rot.y();\n+\t// odom.pose.pose.orientation.z = current_rot.z();\n+\t// odom.pose.pose.orientation.w = current_rot.w();\n+\t// (*pub_ptr).publish(odom);\n }\n \n \n int main(int argc, char **argv)\n"
                },
                {
                    "date": 1648603162977,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,13 +139,13 @@\n             vo_usable = false;\n         }\n         else \n         {\n-            Eigen::Quaternion<double> q;\n-            cv_rotm_to_eigen_quat(q, rotation);\n+            // Eigen::Quaternion<double> q;\n+            // cv_rotm_to_eigen_quat(q, rotation);\n \n-            vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n-            vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n+            // vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n+            // vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n \n             // checking validity of VO\n             double scale_translation = vo_translation.norm();\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n"
                },
                {
                    "date": 1648603198792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,84 +1,84 @@\n #include \"pose_estimation_node.h\"\n #include <stdexcept>\n \n // quat from ekf node\n-void PoseEstimator::quat_callback(const::geometry_msgs::Quaternion::ConstPtr& msg)\n-{\n-\tcurrent_rot.w() = msg->w;\n-\tcurrent_rot.x() = msg->x;\n-\tcurrent_rot.y() = msg->y;\n-\tcurrent_rot.z() = msg->z;\n-\tif (!orientation_init)\n-    {\n-        orientation_init = true;\n-\t\tvo_rot = current_rot;\n-    }\n-}\n+// void PoseEstimator::quat_callback(const::geometry_msgs::Quaternion::ConstPtr& msg)\n+// {\n+// \tcurrent_rot.w() = msg->w;\n+// \tcurrent_rot.x() = msg->x;\n+// \tcurrent_rot.y() = msg->y;\n+// \tcurrent_rot.z() = msg->z;\n+// \tif (!orientation_init)\n+//     {\n+//         orientation_init = true;\n+// \t\tvo_rot = current_rot;\n+//     }\n+// }\n \n // encoders callback\n-void PoseEstimator::encoders_callback(const std_msgs::Int32MultiArray::ConstPtr& msg)\n-{    \n-\tint ticks_l_curr = (msg->data[0]+msg->data[2])/2.0; // total ticks left wheel\n-\tint ticks_r_curr = (msg->data[1]+msg->data[3])/2.0; // total ticks right wheel\n+// void PoseEstimator::encoders_callback(const std_msgs::Int32MultiArray::ConstPtr& msg)\n+// {    \n+// \tint ticks_l_curr = (msg->data[0]+msg->data[2])/2.0; // total ticks left wheel\n+// \tint ticks_r_curr = (msg->data[1]+msg->data[3])/2.0; // total ticks right wheel\n \t\n-\tif (first_time_enc)\n-\t{\n-\t\tticks_l_prev = ticks_l_curr;\n-\t\tticks_r_prev = ticks_r_curr;\n-\t\tfirst_time_enc = false;\n-\t\treturn;\n-\t}\n+// \tif (first_time_enc)\n+// \t{\n+// \t\tticks_l_prev = ticks_l_curr;\n+// \t\tticks_r_prev = ticks_r_curr;\n+// \t\tfirst_time_enc = false;\n+// \t\treturn;\n+// \t}\n \n-    if (!orientation_init) return; \n+//     if (!orientation_init) return; \n \n-\t// instantaneous distance moved by robot\t\n-\tdouble Dl = (ticks_l_curr-ticks_l_prev)/ticks_per_m;\n-\tdouble Dr = (ticks_r_curr-ticks_r_prev)/ticks_per_m;\n-\tdouble Dc = (Dl+Dr)/2.0;\n-\tEigen::Matrix<double,3,1> dpos(Dc,0,0);\n+// \t// instantaneous distance moved by robot\t\n+// \tdouble Dl = (ticks_l_curr-ticks_l_prev)/ticks_per_m;\n+// \tdouble Dr = (ticks_r_curr-ticks_r_prev)/ticks_per_m;\n+// \tdouble Dc = (Dl+Dr)/2.0;\n+// \tEigen::Matrix<double,3,1> dpos(Dc,0,0);\n \n-\t// Store previous set of readings\n-\tticks_l_prev = ticks_l_curr;\n-\tticks_r_prev = ticks_r_curr;\n+// \t// Store previous set of readings\n+// \tticks_l_prev = ticks_l_curr;\n+// \tticks_r_prev = ticks_r_curr;\n \n-\t// update encoders prediction\n-\tencoders_translation += current_rot._transformVector(dpos);\n+// \t// update encoders prediction\n+// \tencoders_translation += current_rot._transformVector(dpos);\n \n-    // using encoder update only\n-    if (!use_vo)\n-    {\n-        global_pos += encoders_translation;\n-        encoders_translation << 0,0,0;\n+//     // using encoder update only\n+//     if (!use_vo)\n+//     {\n+//         global_pos += encoders_translation;\n+//         encoders_translation << 0,0,0;\n \n-        debug(\"[node]: Using encoder update\");\n-        // publish transforms\n-        static tf::TransformBroadcaster br;\n-        tf::Transform transform;\n-        transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n-        tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n-        transform.setRotation(q);\n-        br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n-    }\n-}\n+//         debug(\"[node]: Using encoder update\");\n+//         // publish transforms\n+//         static tf::TransformBroadcaster br;\n+//         tf::Transform transform;\n+//         transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n+//         tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n+//         transform.setRotation(q);\n+//         br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n+//     }\n+// }\n \n \n PoseEstimator::PoseEstimator(cv::Mat projMatrl_, cv::Mat projMatrr_)\n {\n     projMatrl = projMatrl_;\n     projMatrr = projMatrr_;\n }\n \n-cv::Mat PoseEstimator::rosImage2CvMat(const sensor_msgs::ImageConstPtr img) {\n-    cv_bridge::CvImagePtr cv_ptr;\n-    try {\n-            cv_ptr = cv_bridge::toCvCopy(img, sensor_msgs::image_encodings::MONO8);\n-    } catch (cv_bridge::Exception &e) {\n-            std::cerr << \"exception\" << std::endl;\n-            return cv::Mat();\n-    }\n-    return cv_ptr->image;\n-}\n+// cv::Mat PoseEstimator::rosImage2CvMat(const sensor_msgs::ImageConstPtr img) {\n+//     cv_bridge::CvImagePtr cv_ptr;\n+//     try {\n+//             cv_ptr = cv_bridge::toCvCopy(img, sensor_msgs::image_encodings::MONO8);\n+//     } catch (cv_bridge::Exception &e) {\n+//             std::cerr << \"exception\" << std::endl;\n+//             return cv::Mat();\n+//     }\n+//     return cv_ptr->image;\n+// }\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n void PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n@@ -142,9 +142,9 @@\n         {\n             // Eigen::Quaternion<double> q;\n             // cv_rotm_to_eigen_quat(q, rotation);\n \n-            // vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n+            vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n             // vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n \n             // checking validity of VO\n             double scale_translation = vo_translation.norm();\n"
                },
                {
                    "date": 1648603255378,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,13 +142,13 @@\n         {\n             // Eigen::Quaternion<double> q;\n             // cv_rotm_to_eigen_quat(q, rotation);\n \n-            vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n+            // vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n             // vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n \n             // checking validity of VO\n-            double scale_translation = vo_translation.norm();\n+            double scale_translation = translation.norm();\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n"
                },
                {
                    "date": 1648603285476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,9 +146,9 @@\n             // vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n             // vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n \n             // checking validity of VO\n-            double scale_translation = translation.norm();\n+            double scale_translation =cv::norm(translation);\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n"
                },
                {
                    "date": 1648603294237,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,9 +146,9 @@\n             // vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n             // vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n \n             // checking validity of VO\n-            double scale_translation =cv::norm(translation);\n+            double scale_translation = cv::norm(translation);\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n"
                },
                {
                    "date": 1648603306416,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,9 @@\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n-            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(vo_translation(2)) > 0.04)\n+            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(translation(2)) > 0.04)\n             {\n                 debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n             }\n"
                },
                {
                    "date": 1648603316117,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,9 @@\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n-            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(translation(2)) > 0.04)\n+            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(translation.at<float>(2)) > 0.04)\n             {\n                 debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n             }\n"
                },
                {
                    "date": 1648603328356,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,9 @@\n             cv::Rodrigues(rotation, rotation_rodrigues);  \n             double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n \n             // Translation might be too big or too small, as well as rotation\n-            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(translation.at<float>(2)) > 0.04)\n+            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 )\n             {\n                 debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n             }\n"
                },
                {
                    "date": 1648603355450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,9 +158,9 @@\n                 vo_usable = false;\n             }\n         }\n     }\n-\n+    return scale_translation;\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n"
                },
                {
                    "date": 1648603399076,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,9 +158,8 @@\n                 vo_usable = false;\n             }\n         }\n     }\n-    return scale_translation;\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n@@ -184,9 +183,9 @@\n   //       output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n   //       debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n   //   }\n \t\n-\t// std::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n+\tstd::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n \n   //   // update rotation for vo.\n   //   vo_rot = current_rot; // later on do slerp between current and previous \n   //   vo_translation << 0,0,0;\n@@ -202,15 +201,15 @@\n \n \t// nav_msgs::Odometry odom;\n \t// odom.header.stamp = ros::Time::now();\n \t// odom.header.frame_id = \"map\";\n-\t// odom.pose.pose.position.x = global_pos[0];\n-\t// odom.pose.pose.position.y = global_pos[1];\n-\t// odom.pose.pose.position.z = global_pos[2];\n-\t// odom.pose.pose.orientation.x = current_rot.x();\n-\t// odom.pose.pose.orientation.y = current_rot.y();\n-\t// odom.pose.pose.orientation.z = current_rot.z();\n-\t// odom.pose.pose.orientation.w = current_rot.w();\n+\todom.pose.pose.position.x = global_pos[0];\n+\todom.pose.pose.position.y = global_pos[1];\n+\todom.pose.pose.position.z = global_pos[2];\n+\todom.pose.pose.orientation.x = current_rot.x();\n+\todom.pose.pose.orientation.y = current_rot.y();\n+\todom.pose.pose.orientation.z = current_rot.z();\n+\todom.pose.pose.orientation.w = current_rot.w();\n \t// (*pub_ptr).publish(odom);\n }\n \n \n"
                },
                {
                    "date": 1648603410533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -204,12 +204,12 @@\n \t// odom.header.frame_id = \"map\";\n \todom.pose.pose.position.x = global_pos[0];\n \todom.pose.pose.position.y = global_pos[1];\n \todom.pose.pose.position.z = global_pos[2];\n-\todom.pose.pose.orientation.x = current_rot.x();\n-\todom.pose.pose.orientation.y = current_rot.y();\n-\todom.pose.pose.orientation.z = current_rot.z();\n-\todom.pose.pose.orientation.w = current_rot.w();\n+\t// odom.pose.pose.orientation.x = current_rot.x();\n+\t// odom.pose.pose.orientation.y = current_rot.y();\n+\t// odom.pose.pose.orientation.z = current_rot.z();\n+\t// odom.pose.pose.orientation.w = current_rot.w();\n \t// (*pub_ptr).publish(odom);\n }\n \n \n"
                },
                {
                    "date": 1648603433726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,9 +183,9 @@\n   //       output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n   //       debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n   //   }\n \t\n-\tstd::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n+\tstd::cout << translation[0] << \", \" << translation[1] << \", \" << tranlation[2] << std::endl;\n \n   //   // update rotation for vo.\n   //   vo_rot = current_rot; // later on do slerp between current and previous \n   //   vo_translation << 0,0,0;\n@@ -201,11 +201,11 @@\n \n \t// nav_msgs::Odometry odom;\n \t// odom.header.stamp = ros::Time::now();\n \t// odom.header.frame_id = \"map\";\n-\todom.pose.pose.position.x = global_pos[0];\n-\todom.pose.pose.position.y = global_pos[1];\n-\todom.pose.pose.position.z = global_pos[2];\n+\t// odom.pose.pose.position.x = global_pos[0];\n+\t// odom.pose.pose.position.y = global_pos[1];\n+\t// odom.pose.pose.position.z = global_pos[2];\n \t// odom.pose.pose.orientation.x = current_rot.x();\n \t// odom.pose.pose.orientation.y = current_rot.y();\n \t// odom.pose.pose.orientation.z = current_rot.z();\n \t// odom.pose.pose.orientation.w = current_rot.w();\n"
                },
                {
                    "date": 1648603446664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,8 +158,11 @@\n                 vo_usable = false;\n             }\n         }\n     }\n+\t  if (vo_usable) {\n+      std::cout << translation[0] << \", \" << translation[1] << \", \" << tranlation[2] << std::endl;\n+    }\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n@@ -183,9 +186,8 @@\n   //       output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n   //       debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n   //   }\n \t\n-\tstd::cout << translation[0] << \", \" << translation[1] << \", \" << tranlation[2] << std::endl;\n \n   //   // update rotation for vo.\n   //   vo_rot = current_rot; // later on do slerp between current and previous \n   //   vo_translation << 0,0,0;\n"
                },
                {
                    "date": 1648603476876,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,9 +159,9 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      std::cout << translation[0] << \", \" << translation[1] << \", \" << tranlation[2] << std::endl;\n+      std::cout << translation.at<float>(0) << \", \" << translation.at<float>(1) << \", \" << tranlation.at<float>(2) << std::endl;\n     }\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n"
                },
                {
                    "date": 1648603482934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,9 +159,9 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      std::cout << translation.at<float>(0) << \", \" << translation.at<float>(1) << \", \" << tranlation.at<float>(2) << std::endl;\n+      std::cout << translation.at<float>(0) << \", \" << translation.at<float>(1) << \", \" << translation.at<float>(2) << std::endl;\n     }\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n"
                },
                {
                    "date": 1648603562515,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -214,76 +214,76 @@\n \t// (*pub_ptr).publish(odom);\n }\n \n \n-int main(int argc, char **argv)\n-{\n+// int main(int argc, char **argv)\n+// {\n \n-    ros::init(argc, argv, \"stereo_vo_node\");\n+//     ros::init(argc, argv, \"stereo_vo_node\");\n \n-    ros::NodeHandle n;\n+//     ros::NodeHandle n;\n \n-    std::string filename; //TODO correct the name\n-    if (!(n.getParam(\"calib_yaml\",filename)))\n-    {\n-        std::cerr << \"no calib yaml\" << std::endl;\n-        throw;\n-    }\n-    cv::FileStorage fs(filename, cv::FileStorage::READ);\n-    if(!(fs.isOpened()))\n-    {\n-        std::cerr << \"cv failed to load yaml\" << std::endl;\n-        throw;\n-    }\n-    float fx, fy, cx, cy, bf; // Projection matrix parameters\n-    fs[\"fx\"] >> fx;\n-    fs[\"fy\"] >> fy;\n-    fs[\"cx\"] >> cx;\n-    fs[\"cy\"] >> cy;\n-    fs[\"bf\"] >> bf;\n+//     std::string filename; //TODO correct the name\n+//     if (!(n.getParam(\"calib_yaml\",filename)))\n+//     {\n+//         std::cerr << \"no calib yaml\" << std::endl;\n+//         throw;\n+//     }\n+//     cv::FileStorage fs(filename, cv::FileStorage::READ);\n+//     if(!(fs.isOpened()))\n+//     {\n+//         std::cerr << \"cv failed to load yaml\" << std::endl;\n+//         throw;\n+//     }\n+//     float fx, fy, cx, cy, bf; // Projection matrix parameters\n+//     fs[\"fx\"] >> fx;\n+//     fs[\"fy\"] >> fy;\n+//     fs[\"cx\"] >> cx;\n+//     fs[\"cy\"] >> cy;\n+//     fs[\"bf\"] >> bf;\n \n-    cv::Mat projMatrl = (cv::Mat_<float>(3, 4) << fx, 0., cx, 0., 0., fy, cy, 0., 0,  0., 1., 0.);\n-    cv::Mat projMatrr = (cv::Mat_<float>(3, 4) << fx, 0., cx, bf, 0., fy, cy, 0., 0,  0., 1., 0.);\n+//     cv::Mat projMatrl = (cv::Mat_<float>(3, 4) << fx, 0., cx, 0., 0., fy, cy, 0., 0,  0., 1., 0.);\n+//     cv::Mat projMatrr = (cv::Mat_<float>(3, 4) << fx, 0., cx, bf, 0., fy, cy, 0., 0,  0., 1., 0.);\n \n-    // initialize pose estimator object\n-    PoseEstimator pose_estimator(projMatrl, projMatrr);\n+//     // initialize pose estimator object\n+//     PoseEstimator pose_estimator(projMatrl, projMatrr);\n \n-    // mode of operation\n-    n.param<bool>(\"use_vo\", pose_estimator.use_vo, true); // accel white noise\n+//     // mode of operation\n+//     n.param<bool>(\"use_vo\", pose_estimator.use_vo, true); // accel white noise\n \n-    // log path\n-    n.param<bool>(\"logging_path\", pose_estimator.logging_path, false); // accel white noise\n+//     // log path\n+//     n.param<bool>(\"logging_path\", pose_estimator.logging_path, false); // accel white noise\n \n-    // using message_filters to get stereo callback on one topic\n-    message_filters::Subscriber<sensor_msgs::Image> image1_sub(n, \"/stereo/left/image_rect_color\", 1);\n-    message_filters::Subscriber<sensor_msgs::Image> image2_sub(n, \"/stereo/right/image_rect_color\", 1);\n+//     // using message_filters to get stereo callback on one topic\n+//     message_filters::Subscriber<sensor_msgs::Image> image1_sub(n, \"/stereo/left/image_rect_color\", 1);\n+//     message_filters::Subscriber<sensor_msgs::Image> image2_sub(n, \"/stereo/right/image_rect_color\", 1);\n \n-    typedef message_filters::sync_policies::ApproximateTime<sensor_msgs::Image, sensor_msgs::Image> MySyncPolicy;\n+//     typedef message_filters::sync_policies::ApproximateTime<sensor_msgs::Image, sensor_msgs::Image> MySyncPolicy;\n \n-    // ApproximateTime takes a queue size as its constructor argument, hence MySyncPolicy(10)\n-    message_filters::Synchronizer<MySyncPolicy> sync(MySyncPolicy(1), image1_sub, image2_sub);\n+//     // ApproximateTime takes a queue size as its constructor argument, hence MySyncPolicy(10)\n+//     message_filters::Synchronizer<MySyncPolicy> sync(MySyncPolicy(1), image1_sub, image2_sub);\n \n-    // use images only if vo is being used\n-    if (pose_estimator.use_vo)\n-    {\n-        sync.registerCallback(boost::bind(&PoseEstimator::stereo_callback, &pose_estimator, _1, _2));\n-    }\n+//     // use images only if vo is being used\n+//     if (pose_estimator.use_vo)\n+//     {\n+//         sync.registerCallback(boost::bind(&PoseEstimator::stereo_callback, &pose_estimator, _1, _2));\n+//     }\n \n-    // wheel encoders\n-\tros::Subscriber sub_encoders = n.subscribe(\"wheels\", 0, &PoseEstimator::encoders_callback, &pose_estimator);\n+//     // wheel encoders\n+// \tros::Subscriber sub_encoders = n.subscribe(\"wheels\", 0, &PoseEstimator::encoders_callback, &pose_estimator);\n \n-    // orienation from orientation ekf\n-    ros::Subscriber sub_quat = n.subscribe(\"quat\", 0, &PoseEstimator::quat_callback, &pose_estimator);\n+//     // orienation from orientation ekf\n+//     ros::Subscriber sub_quat = n.subscribe(\"quat\", 0, &PoseEstimator::quat_callback, &pose_estimator);\n \n-\tros::Publisher odom_pub = n.advertise<nav_msgs::Odometry>(\"odom\", 1);\n-\tpose_estimator.pub_ptr = &odom_pub;\n+// \tros::Publisher odom_pub = n.advertise<nav_msgs::Odometry>(\"odom\", 1);\n+// \tpose_estimator.pub_ptr = &odom_pub;\n \n-    debug(\"Pose Estimator Initialized!\");\n+//     debug(\"Pose Estimator Initialized!\");\n \n-    if (pose_estimator.logging_path) pose_estimator.output_file.open(\"/home/morphin/Desktop/output.txt\");\n+//     if (pose_estimator.logging_path) pose_estimator.output_file.open(\"/home/morphin/Desktop/output.txt\");\n \n-    ros::spin();\n+//     ros::spin();\n \n-    if (pose_estimator.logging_path) pose_estimator.output_file.close();\n+//     if (pose_estimator.logging_path) pose_estimator.output_file.close();\n \n-    return 0;\n-}\n+//     return 0;\n+// }\n"
                },
                {
                    "date": 1648603765035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -100,9 +100,9 @@\n }\n \n void PoseEstimator::run()\n {\n-    debug(\"[node]: frame id \" + std::to_string(frame_id));\n+    // debug(\"[node]: frame id \" + std::to_string(frame_id));\n     std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n     matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n                       pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1);  \n \n"
                },
                {
                    "date": 1648603789373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,9 +119,9 @@\n     // if not enough features don't use vo\n     bool vo_usable = true;\n     if (currentVOFeatures.size() < features_threshold)\n     {\n-        debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n+        // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n \t\tvo_usable = false;        \n     } else \n     {\n \t    // Triangulate 3D Points\n@@ -134,9 +134,9 @@\n \n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n-            debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n+            // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n         }\n         else \n         {\n@@ -153,9 +153,9 @@\n \n             // Translation might be too big or too small, as well as rotation\n             if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 )\n             {\n-                debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n+                // debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n             }\n         }\n     }\n"
                },
                {
                    "date": 1648604032545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,8 +155,9 @@\n             if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 )\n             {\n                 // debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n+                cout << \"rejected\\n\";\n             }\n         }\n     }\n \t  if (vo_usable) {\n"
                },
                {
                    "date": 1648604038782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n             if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 )\n             {\n                 // debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n-                cout << \"rejected\\n\";\n+                std::cout << \"rejected\";\n             }\n         }\n     }\n \t  if (vo_usable) {\n"
                },
                {
                    "date": 1648604063948,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n             if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 )\n             {\n                 // debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                 vo_usable = false;\n-                std::cout << \"rejected\";\n+                std::cout << \"rejected\" << scale_translation << \" \" << abs(angle);\n             }\n         }\n     }\n \t  if (vo_usable) {\n"
                },
                {
                    "date": 1648604088254,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,8 +136,9 @@\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n+            std::cout << \"rejected \" << inliers\n         }\n         else \n         {\n             // Eigen::Quaternion<double> q;\n"
                },
                {
                    "date": 1648604129734,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n-            std::cout << \"rejected \" << inliers\n+            std::cout << \"rejected \" << inliers << std::endl;\n         }\n         else \n         {\n             // Eigen::Quaternion<double> q;\n"
                },
                {
                    "date": 1648604153404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n-            std::cout << \"rejected \" << inliers << std::endl;\n+            std::cout << \"rejcted  \" << inliers; << std::endl;\n         }\n         else \n         {\n             // Eigen::Quaternion<double> q;\n"
                },
                {
                    "date": 1648604160036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n-            std::cout << \"rejcted  \" << inliers; << std::endl;\n+            std::cout << \"rejcted  \" << inliers << std::endl;\n         }\n         else \n         {\n             // Eigen::Quaternion<double> q;\n"
                },
                {
                    "date": 1648604180800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,9 +120,10 @@\n     bool vo_usable = true;\n     if (currentVOFeatures.size() < features_threshold)\n     {\n         // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n-\t\tvo_usable = false;        \n+      vo_usable = false;        \n+      std::cout << \"rejected \" << currentVOFeatures() << \"\\n\";\n     } else \n     {\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n"
                },
                {
                    "date": 1648604188085,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,9 +121,9 @@\n     if (currentVOFeatures.size() < features_threshold)\n     {\n         // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n       vo_usable = false;        \n-      std::cout << \"rejected \" << currentVOFeatures() << \"\\n\";\n+      std::cout << \"rejected \" << currentVOFeatures.size() << \"\\n\";\n     } else \n     {\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n"
                },
                {
                    "date": 1648604201489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n             vo_usable = false;\n-            std::cout << \"rejcted  \" << inliers << std::endl;\n+            std::cout << \"rejcted2  \" << inliers << std::endl;\n         }\n         else \n         {\n             // Eigen::Quaternion<double> q;\n"
                },
                {
                    "date": 1648604231470,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,8 +117,9 @@\n     //displayTracking(imageLeft_t1, pointsLeft_t0, pointsLeft_t1);\n \n     // if not enough features don't use vo\n     bool vo_usable = true;\n+    cout << \"EHLLO\" << std::endl;\n     if (currentVOFeatures.size() < features_threshold)\n     {\n         // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n       vo_usable = false;        \n"
                },
                {
                    "date": 1648604273476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,9 +94,9 @@\n \n     frame_id++;\n \n     // start the vo pipeline after orientation is initialized\n-    if (!orientation_init) return; \n+    // if (!orientation_init) return; \n     else run();\n }\n \n void PoseEstimator::run()\n@@ -117,9 +117,9 @@\n     //displayTracking(imageLeft_t1, pointsLeft_t0, pointsLeft_t1);\n \n     // if not enough features don't use vo\n     bool vo_usable = true;\n-    cout << \"EHLLO\" << std::endl;\n+    std::cout << \"EHLLO\" << std::endl;\n     if (currentVOFeatures.size() < features_threshold)\n     {\n         // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n       vo_usable = false;        \n"
                },
                {
                    "date": 1648604280704,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,9 +95,10 @@\n     frame_id++;\n \n     // start the vo pipeline after orientation is initialized\n     // if (!orientation_init) return; \n-    else run();\n+    // else \n+    run();\n }\n \n void PoseEstimator::run()\n {\n"
                },
                {
                    "date": 1648604450736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n //     return cv_ptr->image;\n // }\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n-void PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n+pair<float, float> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n"
                },
                {
                    "date": 1648604475482,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n //     return cv_ptr->image;\n // }\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n-pair<float, float> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n+pair<double, double> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n"
                },
                {
                    "date": 1648604493328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n //     return cv_ptr->image;\n // }\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n-pair<double, double> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n+std::pair<double, double> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n@@ -96,12 +96,12 @@\n \n     // start the vo pipeline after orientation is initialized\n     // if (!orientation_init) return; \n     // else \n-    run();\n+    return run();\n }\n \n-void PoseEstimator::run()\n+std::pair<double, double> PoseEstimator::run()\n {\n     // debug(\"[node]: frame id \" + std::to_string(frame_id));\n     std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n     matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n"
                },
                {
                    "date": 1648604571882,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,9 +164,9 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      std::cout << translation.at<float>(0) << \", \" << translation.at<float>(1) << \", \" << translation.at<float>(2) << std::endl;\n+      return (translation.at<double>(0), translation.at<double>(1));\n     }\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n"
                },
                {
                    "date": 1648604577793,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,9 +164,9 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      return (translation.at<double>(0), translation.at<double>(1));\n+      return make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n"
                },
                {
                    "date": 1648604589508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,10 +164,11 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      return make_pair(translation.at<double>(0), translation.at<double>(1));\n+      return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n+      return std::make_pair(0, 0);\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n"
                },
                {
                    "date": 1648604669885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,9 +86,9 @@\n     {\n         imageLeft_t0 = image_left;\n         imageRight_t0 = image_right;\n         frame_id++;\n-        return;\n+        return make_pair(0, 0);\n     }\n     imageLeft_t1 = image_left;\n     imageRight_t1 = image_right;\n \n"
                },
                {
                    "date": 1648604675535,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,9 +86,9 @@\n     {\n         imageLeft_t0 = image_left;\n         imageRight_t0 = image_right;\n         frame_id++;\n-        return make_pair(0, 0);\n+        return std::make_pair(0, 0);\n     }\n     imageLeft_t1 = image_left;\n     imageRight_t1 = image_right;\n \n"
                },
                {
                    "date": 1648669477678,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,14 +118,12 @@\n     //displayTracking(imageLeft_t1, pointsLeft_t0, pointsLeft_t1);\n \n     // if not enough features don't use vo\n     bool vo_usable = true;\n-    std::cout << \"EHLLO\" << std::endl;\n     if (currentVOFeatures.size() < features_threshold)\n     {\n         // debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n       vo_usable = false;        \n-      std::cout << \"rejected \" << currentVOFeatures.size() << \"\\n\";\n     } else \n     {\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n"
                },
                {
                    "date": 1648673776035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n-\n+      dbg(points4D_t0);\n \t    // PnP: computes rotation and translation between previous 3D points and next features\n \t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n \n         // PnP may not converge\n"
                },
                {
                    "date": 1648673818042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n-      dbg(points4D_t0);\n+      dbg(points3D_t0);\n \t    // PnP: computes rotation and translation between previous 3D points and next features\n \t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n \n         // PnP may not converge\n"
                },
                {
                    "date": 1648674035823,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,11 +128,11 @@\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n-      dbg(points3D_t0);\n \t    // PnP: computes rotation and translation between previous 3D points and next features\n \t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n+      dbg(inliers);\n \n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n"
                },
                {
                    "date": 1648684026659,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n \t    // PnP: computes rotation and translation between previous 3D points and next features\n \t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n-      dbg(inliers);\n+      dbg(points3D_t0);\n \n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n"
                },
                {
                    "date": 1648684360825,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,8 @@\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n \t    // PnP: computes rotation and translation between previous 3D points and next features\n \t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n-      dbg(points3D_t0);\n \n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n"
                },
                {
                    "date": 1648684411989,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,8 +160,9 @@\n                 std::cout << \"rejected\" << scale_translation << \" \" << abs(angle);\n             }\n         }\n     }\n+    dbg(translation);\n \t  if (vo_usable) {\n       return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n       return std::make_pair(0, 0);\n"
                },
                {
                    "date": 1648684524919,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,13 +160,13 @@\n                 std::cout << \"rejected\" << scale_translation << \" \" << abs(angle);\n             }\n         }\n     }\n-    dbg(translation);\n \t  if (vo_usable) {\n       return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n-      return std::make_pair(0, 0);\n+    dbg(translation);\n+    return std::make_pair(0, 0);\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n"
                },
                {
                    "date": 1648684535479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,11 +161,11 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n+      dbg(translation);\n       return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n-    dbg(translation);\n     return std::make_pair(0, 0);\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n"
                },
                {
                    "date": 1648685099237,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,11 +128,17 @@\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n-\t    // PnP: computes rotation and translation between previous 3D points and next features\n-\t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n-\n+      dbg(projMatrl);\n+      dbg(pointsLeft_t1);\n+      dbg(points3D_t0);\n+      // PnP: computes rotation and translation between previous 3D points and next features\n+      int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n+      dbg(rotation);\n+      dbg(translation);\n+      dbg(rotation);\n+      dbg(translation);\n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n"
                },
                {
                    "date": 1648685105673,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,12 +133,11 @@\n       dbg(pointsLeft_t1);\n       dbg(points3D_t0);\n       // PnP: computes rotation and translation between previous 3D points and next features\n       int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n+      dbg(inliers);\n       dbg(rotation);\n       dbg(translation);\n-      dbg(rotation);\n-      dbg(translation);\n         // PnP may not converge\n         if (inliers < features_threshold)\n         {\n             // debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n"
                },
                {
                    "date": 1648685391573,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,11 +128,9 @@\n \t    // Triangulate 3D Points\n \t    cv::Mat points3D_t0, points4D_t0;\n \t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n \t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n-      dbg(projMatrl);\n-      dbg(pointsLeft_t1);\n-      dbg(points3D_t0);\n+      dbg(pointsLeft_t0.size());\n       // PnP: computes rotation and translation between previous 3D points and next features\n       int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n       dbg(inliers);\n       dbg(rotation);\n"
                },
                {
                    "date": 1648685470001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,9 +164,8 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      dbg(translation);\n       return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n     return std::make_pair(0, 0);\n   //   if (vo_usable)\n"
                },
                {
                    "date": 1648686061257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,9 +79,9 @@\n //     return cv_ptr->image;\n // }\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n-std::pair<double, double> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n+std::pair<cv::Mat, cv::Mat> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n"
                },
                {
                    "date": 1648686070240,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,8 +81,10 @@\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n std::pair<cv::Mat, cv::Mat> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n+    cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);\n+    cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n         imageRight_t0 = image_right;\n"
                },
                {
                    "date": 1648686084115,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -81,16 +81,16 @@\n \n // void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\n std::pair<cv::Mat, cv::Mat> PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n {\n-    cv::Mat rotation = cv::Mat::eye(3, 3, CV_64F);\n-    cv::Mat translation = cv::Mat::zeros(3, 1, CV_64F);\n+    cv::Mat no_rotation = cv::Mat::eye(3, 3, CV_64F);\n+    cv::Mat no_translation = cv::Mat::zeros(3, 1, CV_64F);\n     if (!frame_id)\n     {\n         imageLeft_t0 = image_left;\n         imageRight_t0 = image_right;\n         frame_id++;\n-        return std::make_pair(0, 0);\n+        return std::make_pair(translation, rotation);\n     }\n     imageLeft_t1 = image_left;\n     imageRight_t1 = image_right;\n \n"
                },
                {
                    "date": 1648686089463,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -88,9 +88,9 @@\n     {\n         imageLeft_t0 = image_left;\n         imageRight_t0 = image_right;\n         frame_id++;\n-        return std::make_pair(translation, rotation);\n+        return std::make_pair(no_translation, no_rotation);\n     }\n     imageLeft_t1 = image_left;\n     imageRight_t1 = image_right;\n \n"
                },
                {
                    "date": 1648686100195,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,9 +101,9 @@\n     // else \n     return run();\n }\n \n-std::pair<double, double> PoseEstimator::run()\n+std::pair<cv::Mat, cv::Mat> PoseEstimator::run()\n {\n     // debug(\"[node]: frame id \" + std::to_string(frame_id));\n     std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n     matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n"
                },
                {
                    "date": 1648686106003,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,8 +103,10 @@\n }\n \n std::pair<cv::Mat, cv::Mat> PoseEstimator::run()\n {\n+    cv::Mat no_rotation = cv::Mat::eye(3, 3, CV_64F);\n+    cv::Mat no_translation = cv::Mat::zeros(3, 1, CV_64F);\n     // debug(\"[node]: frame id \" + std::to_string(frame_id));\n     std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n     matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n                       pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1);  \n"
                },
                {
                    "date": 1648686173618,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,10 +107,10 @@\n     cv::Mat no_rotation = cv::Mat::eye(3, 3, CV_64F);\n     cv::Mat no_translation = cv::Mat::zeros(3, 1, CV_64F);\n     // debug(\"[node]: frame id \" + std::to_string(frame_id));\n     std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n-    matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n-                      pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1);  \n+    matchingFeatures(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n+                     pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1);  \n \n     // visualize previous and next left image\n     // displayTwoImages(imageLeft_t0, imageLeft_t1);\n \n"
                },
                {
                    "date": 1648686306241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -170,9 +170,9 @@\n     }\n \t  if (vo_usable) {\n       return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n     }\n-    return std::make_pair(0, 0);\n+    return std::make_pair(no_translation, no_rotation);\n   //   if (vo_usable)\n   //   {\n   //   \tdebug(\"[node]: Using VO update\");\n   //   \t// rotate vo translation to rover frame\n"
                },
                {
                    "date": 1648686314284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,9 +168,9 @@\n             }\n         }\n     }\n \t  if (vo_usable) {\n-      return std::make_pair(translation.at<double>(0), translation.at<double>(1));\n+      return std::make_pair(translation, rotation);\n     }\n     return std::make_pair(no_translation, no_rotation);\n   //   if (vo_usable)\n   //   {\n"
                }
            ],
            "date": 1648602688933,
            "name": "Commit-0",
            "content": "#include \"pose_estimation_node.h\"\n#include <stdexcept>\n\n// quat from ekf node\nvoid PoseEstimator::quat_callback(const::geometry_msgs::Quaternion::ConstPtr& msg)\n{\n\tcurrent_rot.w() = msg->w;\n\tcurrent_rot.x() = msg->x;\n\tcurrent_rot.y() = msg->y;\n\tcurrent_rot.z() = msg->z;\n\tif (!orientation_init)\n    {\n        orientation_init = true;\n\t\tvo_rot = current_rot;\n    }\n}\n\n// encoders callback\nvoid PoseEstimator::encoders_callback(const std_msgs::Int32MultiArray::ConstPtr& msg)\n{    \n\tint ticks_l_curr = (msg->data[0]+msg->data[2])/2.0; // total ticks left wheel\n\tint ticks_r_curr = (msg->data[1]+msg->data[3])/2.0; // total ticks right wheel\n\t\n\tif (first_time_enc)\n\t{\n\t\tticks_l_prev = ticks_l_curr;\n\t\tticks_r_prev = ticks_r_curr;\n\t\tfirst_time_enc = false;\n\t\treturn;\n\t}\n\n    if (!orientation_init) return; \n\n\t// instantaneous distance moved by robot\t\n\tdouble Dl = (ticks_l_curr-ticks_l_prev)/ticks_per_m;\n\tdouble Dr = (ticks_r_curr-ticks_r_prev)/ticks_per_m;\n\tdouble Dc = (Dl+Dr)/2.0;\n\tEigen::Matrix<double,3,1> dpos(Dc,0,0);\n\n\t// Store previous set of readings\n\tticks_l_prev = ticks_l_curr;\n\tticks_r_prev = ticks_r_curr;\n\n\t// update encoders prediction\n\tencoders_translation += current_rot._transformVector(dpos);\n\n    // using encoder update only\n    if (!use_vo)\n    {\n        global_pos += encoders_translation;\n        encoders_translation << 0,0,0;\n\n        debug(\"[node]: Using encoder update\");\n        // publish transforms\n        static tf::TransformBroadcaster br;\n        tf::Transform transform;\n        transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n        tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n        transform.setRotation(q);\n        br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n    }\n}\n\n\nPoseEstimator::PoseEstimator(cv::Mat projMatrl_, cv::Mat projMatrr_)\n{\n    projMatrl = projMatrl_;\n    projMatrr = projMatrr_;\n}\n\ncv::Mat PoseEstimator::rosImage2CvMat(const sensor_msgs::ImageConstPtr img) {\n    cv_bridge::CvImagePtr cv_ptr;\n    try {\n            cv_ptr = cv_bridge::toCvCopy(img, sensor_msgs::image_encodings::MONO8);\n    } catch (cv_bridge::Exception &e) {\n            std::cerr << \"exception\" << std::endl;\n            return cv::Mat();\n    }\n    return cv_ptr->image;\n}\n\n// void PoseEstimator::stereo_callback(const sensor_msgs::ImageConstPtr& image_left, const sensor_msgs::ImageConstPtr& image_right)\nvoid PoseEstimator::stereo_callback(const cv::Mat& image_left, const cv::Mat& image_right)\n{\n    if (!frame_id)\n    {\n        imageLeft_t0 = rosImage2CvMat(image_left);\n        imageRight_t0 = rosImage2CvMat(image_right);\n        frame_id++;\n        return;\n    }\n    imageLeft_t1 = rosImage2CvMat(image_left);\n    imageRight_t1 = rosImage2CvMat(image_right);\n\n    frame_id++;\n\n    // start the vo pipeline after orientation is initialized\n    if (!orientation_init) return; \n    else run();\n}\n\nvoid PoseEstimator::run()\n{\n    debug(\"[node]: frame id \" + std::to_string(frame_id));\n    std::vector<cv::Point2f> pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1;  \n    matchingFeatures( imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,  currentVOFeatures,\n                      pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1);  \n\n    // visualize previous and next left image\n    // displayTwoImages(imageLeft_t0, imageLeft_t1);\n\n    // set new images as old images\n    imageLeft_t0 = imageLeft_t1;\n    imageRight_t0 = imageRight_t1;\n\n    // visualize feature tracks\n    //displayTracking(imageLeft_t1, pointsLeft_t0, pointsLeft_t1);\n\n    // if not enough features don't use vo\n    bool vo_usable = true;\n    if (currentVOFeatures.size() < features_threshold)\n    {\n        debug(\"[node]: not enough features detected for vo: \" + std::to_string(currentVOFeatures.size())  + \" < \" + std::to_string(features_threshold));\n\t\tvo_usable = false;        \n    } else \n    {\n\t    // Triangulate 3D Points\n\t    cv::Mat points3D_t0, points4D_t0;\n\t    cv::triangulatePoints( projMatrl,  projMatrr,  pointsLeft_t0,  pointsRight_t0,  points4D_t0);\n\t    cv::convertPointsFromHomogeneous(points4D_t0.t(), points3D_t0);\n\n\t    // PnP: computes rotation and translation between previous 3D points and next features\n\t    int inliers = trackingFrame2Frame(projMatrl, projMatrr, pointsLeft_t1, points3D_t0, rotation, translation);\n\n        // PnP may not converge\n        if (inliers < features_threshold)\n        {\n            debug(\"[node]: Not enough inliers from PnP: \" + std::to_string(inliers) + \" < \" + std::to_string(features_threshold));\n            vo_usable = false;\n        }\n        else \n        {\n            Eigen::Quaternion<double> q;\n            cv_rotm_to_eigen_quat(q, rotation);\n\n            vo_translation << translation.at<double>(0), translation.at<double>(1), translation.at<double>(2);\n            vo_translation = -1*(q._transformVector(vo_translation)); // pnp returns T_t1t0, invert to get T_t0t1... \n\n            // checking validity of VO\n            double scale_translation = vo_translation.norm();\n            cv::Rodrigues(rotation, rotation_rodrigues);  \n            double angle = cv::norm(rotation_rodrigues, cv::NORM_L2);\n\n            // Translation might be too big or too small, as well as rotation\n            if (scale_translation < 0.01 || scale_translation > 0.1 || abs(angle) > 0.5 || abs(vo_translation(2)) > 0.04)\n            {\n                debug(\"[node]: VO rejected. Translation too small or too big or rotation too big\");\n                vo_usable = false;\n            }\n        }\n    }\n\n    if (vo_usable)\n    {\n    \tdebug(\"[node]: Using VO update\");\n    \t// rotate vo translation to rover frame\n    \tEigen::Matrix<double,3,1> vo_trans_rover_frame = q_bc._transformVector(vo_translation);\n    \t// rotate vo translation in rover frame to global frame\n        Eigen::Quaternion<double> rot_ib = vo_rot.slerp(0.5, current_rot);\n    \tEigen::Matrix<double,3,1> vo_trans_global_frame = rot_ib._transformVector(vo_trans_rover_frame);\n    \t// add to global position\n    \tglobal_pos += vo_trans_global_frame;\n    } else \n    {\n    \tdebug(\"[node]: Using encoder update\");\n    \t// add to global position\n    \tglobal_pos += encoders_translation;\n    }\n\n    if (logging_path)\n    {\n        static auto time_init = std::chrono::steady_clock::now();\n        double time_now = (double)(std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now()- time_init).count())/1000;\n        output_file << time_now << \", \" << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << \"\\n\";\n        debug(\"t: \" + std::to_string(time_now) + \" s\\n\");\n    }\n\t\n\tstd::cout << global_pos[0] << \", \" << global_pos[1] << \", \" << global_pos[2] << std::endl;\n\n    // update rotation for vo.\n    vo_rot = current_rot; // later on do slerp between current and previous \n    vo_translation << 0,0,0;\n    encoders_translation << 0,0,0;\n\n    // publish transforms\n    static tf::TransformBroadcaster br;\n    tf::Transform transform;\n    transform.setOrigin(tf::Vector3(global_pos[0],global_pos[1],global_pos[2]));\n    tf::Quaternion q (current_rot.x(), current_rot.y(), current_rot.z(), current_rot.w());\n    transform.setRotation(q);\n    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), \"map\", \"yolo\"));\n\n\tnav_msgs::Odometry odom;\n\todom.header.stamp = ros::Time::now();\n\todom.header.frame_id = \"map\";\n\todom.pose.pose.position.x = global_pos[0];\n\todom.pose.pose.position.y = global_pos[1];\n\todom.pose.pose.position.z = global_pos[2];\n\todom.pose.pose.orientation.x = current_rot.x();\n\todom.pose.pose.orientation.y = current_rot.y();\n\todom.pose.pose.orientation.z = current_rot.z();\n\todom.pose.pose.orientation.w = current_rot.w();\n\t(*pub_ptr).publish(odom);\n}\n\n\nint main(int argc, char **argv)\n{\n\n    ros::init(argc, argv, \"stereo_vo_node\");\n\n    ros::NodeHandle n;\n\n    std::string filename; //TODO correct the name\n    if (!(n.getParam(\"calib_yaml\",filename)))\n    {\n        std::cerr << \"no calib yaml\" << std::endl;\n        throw;\n    }\n    cv::FileStorage fs(filename, cv::FileStorage::READ);\n    if(!(fs.isOpened()))\n    {\n        std::cerr << \"cv failed to load yaml\" << std::endl;\n        throw;\n    }\n    float fx, fy, cx, cy, bf; // Projection matrix parameters\n    fs[\"fx\"] >> fx;\n    fs[\"fy\"] >> fy;\n    fs[\"cx\"] >> cx;\n    fs[\"cy\"] >> cy;\n    fs[\"bf\"] >> bf;\n\n    cv::Mat projMatrl = (cv::Mat_<float>(3, 4) << fx, 0., cx, 0., 0., fy, cy, 0., 0,  0., 1., 0.);\n    cv::Mat projMatrr = (cv::Mat_<float>(3, 4) << fx, 0., cx, bf, 0., fy, cy, 0., 0,  0., 1., 0.);\n\n    // initialize pose estimator object\n    PoseEstimator pose_estimator(projMatrl, projMatrr);\n\n    // mode of operation\n    n.param<bool>(\"use_vo\", pose_estimator.use_vo, true); // accel white noise\n\n    // log path\n    n.param<bool>(\"logging_path\", pose_estimator.logging_path, false); // accel white noise\n\n    // using message_filters to get stereo callback on one topic\n    message_filters::Subscriber<sensor_msgs::Image> image1_sub(n, \"/stereo/left/image_rect_color\", 1);\n    message_filters::Subscriber<sensor_msgs::Image> image2_sub(n, \"/stereo/right/image_rect_color\", 1);\n\n    typedef message_filters::sync_policies::ApproximateTime<sensor_msgs::Image, sensor_msgs::Image> MySyncPolicy;\n\n    // ApproximateTime takes a queue size as its constructor argument, hence MySyncPolicy(10)\n    message_filters::Synchronizer<MySyncPolicy> sync(MySyncPolicy(1), image1_sub, image2_sub);\n\n    // use images only if vo is being used\n    if (pose_estimator.use_vo)\n    {\n        sync.registerCallback(boost::bind(&PoseEstimator::stereo_callback, &pose_estimator, _1, _2));\n    }\n\n    // wheel encoders\n\tros::Subscriber sub_encoders = n.subscribe(\"wheels\", 0, &PoseEstimator::encoders_callback, &pose_estimator);\n\n    // orienation from orientation ekf\n    ros::Subscriber sub_quat = n.subscribe(\"quat\", 0, &PoseEstimator::quat_callback, &pose_estimator);\n\n\tros::Publisher odom_pub = n.advertise<nav_msgs::Odometry>(\"odom\", 1);\n\tpose_estimator.pub_ptr = &odom_pub;\n\n    debug(\"Pose Estimator Initialized!\");\n\n    if (pose_estimator.logging_path) pose_estimator.output_file.open(\"/home/morphin/Desktop/output.txt\");\n\n    ros::spin();\n\n    if (pose_estimator.logging_path) pose_estimator.output_file.close();\n\n    return 0;\n}\n"
        }
    ]
}
{
    "sourceFile": "src/vo.cpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 115,
            "patches": [
                {
                    "date": 1645281156817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1645281344960,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,13 +82,8 @@\n \n     imageLeftT1_ = image_left;\n     imageRightT1_ = image_right;\n \n-    // run the pipeline\n-    run();\n-}\n-\n-void VisualOdometry::run() {\n     std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n         pointsRightT1;\n     matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                      currentVOFeatures, pointsLeftT0, pointsRightT0,\n"
                },
                {
                    "date": 1645281383075,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,8 +72,10 @@\n VisualOdometry::~VisualOdometry() {\n }\n void VisualOdometry::stereo_callback(const cv::Mat& image_left,\n                      const cv::Mat& image_right) {\n+    // Wait until we have two time steps of data to begin predicting\n+    // the change in pose.\n     if (!frame_id) {\n         imageLeftT0_ = image_left;\n         imageRightT0_ = image_right;\n         frame_id++;\n"
                },
                {
                    "date": 1645281393068,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,10 +72,10 @@\n VisualOdometry::~VisualOdometry() {\n }\n void VisualOdometry::stereo_callback(const cv::Mat& image_left,\n                      const cv::Mat& image_right) {\n-    // Wait until we have two time steps of data to begin predicting\n-    // the change in pose.\n+    // Wait until we have at least two time steps of data\n+    // to begin predicting the change in pose.\n     if (!frame_id) {\n         imageLeftT0_ = image_left;\n         imageRightT0_ = image_right;\n         frame_id++;\n"
                },
                {
                    "date": 1645281448561,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,8 +70,9 @@\n }\n \n VisualOdometry::~VisualOdometry() {\n }\n+\n void VisualOdometry::stereo_callback(const cv::Mat& image_left,\n                      const cv::Mat& image_right) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n@@ -96,10 +97,10 @@\n     imageRightT0_ = imageRightT1_;\n \n     if (currentVOFeatures.size() < 5)   // TODO should this be AND?\n     {\n-        std::cout << \"not enough features matched for pose estimation\"\n-                  << std::endl;\n+        // There are not enough features to do pose estimation, so\n+        // exit early.\n         frame_id++;\n         return;\n     }\n \n"
                },
                {
                    "date": 1645281457368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,9 +95,9 @@\n     // set new images as old images\n     imageLeftT0_ = imageLeftT1_;\n     imageRightT0_ = imageRightT1_;\n \n-    if (currentVOFeatures.size() < 5)   // TODO should this be AND?\n+    if (currentVOFeatures.size() < 5)\n     {\n         // There are not enough features to do pose estimation, so\n         // exit early.\n         frame_id++;\n"
                },
                {
                    "date": 1645281485375,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,10 +97,10 @@\n     imageRightT0_ = imageRightT1_;\n \n     if (currentVOFeatures.size() < 5)\n     {\n-        // There are not enough features to do pose estimation, so\n-        // exit early.\n+        // There are not enough features to fully determine\n+        // equations for pose estimation, so exit early.\n         frame_id++;\n         return;\n     }\n \n"
                },
                {
                    "date": 1645282103077,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,20 +72,20 @@\n VisualOdometry::~VisualOdometry() {\n }\n \n void VisualOdometry::stereo_callback(const cv::Mat& image_left,\n-                     const cv::Mat& image_right) {\n+                     const cv::Mat& imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n     if (!frame_id) {\n         imageLeftT0_ = image_left;\n-        imageRightT0_ = image_right;\n+        imageRightT0_ = imageRight;\n         frame_id++;\n         return;\n     }\n \n     imageLeftT1_ = image_left;\n-    imageRightT1_ = image_right;\n+    imageRightT1_ = imageRight;\n \n     std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n         pointsRightT1;\n     matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n"
                },
                {
                    "date": 1645282109637,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,20 +71,20 @@\n \n VisualOdometry::~VisualOdometry() {\n }\n \n-void VisualOdometry::stereo_callback(const cv::Mat& image_left,\n+void VisualOdometry::stereo_callback(const cv::Mat& imageLeft,\n                      const cv::Mat& imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n     if (!frame_id) {\n-        imageLeftT0_ = image_left;\n+        imageLeftT0_ = imageLeft;\n         imageRightT0_ = imageRight;\n         frame_id++;\n         return;\n     }\n \n-    imageLeftT1_ = image_left;\n+    imageLeftT1_ = imageLeft;\n     imageRightT1_ = imageRight;\n \n     std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n         pointsRightT1;\n"
                },
                {
                    "date": 1645282148889,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,9 @@\n     matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                      currentVOFeatures, pointsLeftT0, pointsRightT0,\n                      pointsLeftT1, pointsRightT1);\n \n-    // set new images as old images\n+    // Set new images as old images.\n     imageLeftT0_ = imageLeftT1_;\n     imageRightT0_ = imageRightT1_;\n \n     if (currentVOFeatures.size() < 5)\n"
                },
                {
                    "date": 1645282174379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,9 +98,9 @@\n \n     if (currentVOFeatures.size() < 5)\n     {\n         // There are not enough features to fully determine\n-        // equations for pose estimation, so exit early.\n+        // equations for pose estimation, so presume nothing and exit.\n         frame_id++;\n         return;\n     }\n \n"
                },
                {
                    "date": 1645282191294,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,8 +87,9 @@\n     imageRightT1_ = imageRight;\n \n     std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n         pointsRightT1;\n+    \n     matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                      currentVOFeatures, pointsLeftT0, pointsRightT0,\n                      pointsLeftT1, pointsRightT1);\n \n"
                },
                {
                    "date": 1645282220756,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -448,18 +448,19 @@\n \n     cv::Rodrigues(rvec, rotation);\n }\n \n-void matchingFeatures(cv::Mat& imageLeft_t0, cv::Mat& imageRight_t0,\n-                      cv::Mat& imageLeft_t1, cv::Mat& imageRight_t1,\n+void matchingFeatures(const cv::Mat& imageLeft_t0, const cv::Mat& imageRight_t0,\n+                      const cv::Mat& imageLeft_t1, const cv::Mat& imageRight_t1,\n                       FeatureSet& currentVOFeatures,\n                       std::vector<cv::Point2f>& pointsLeftT0,\n                       std::vector<cv::Point2f>& pointsRightT0,\n                       std::vector<cv::Point2f>& pointsLeftT1,\n                       std::vector<cv::Point2f>& pointsRightT1) {\n     // ----------------------------\n     // Feature detection using FAST\n     // ----------------------------\n+    \n     std::vector<cv::Point2f>\n         pointsLeftReturn_t0;   // feature points to check cicular mathcing\n                                // validation\n \n"
                },
                {
                    "date": 1645282236227,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -448,9 +448,9 @@\n \n     cv::Rodrigues(rvec, rotation);\n }\n \n-void matchingFeatures(const cv::Mat& imageLeft_t0, const cv::Mat& imageRight_t0,\n+statuc void matchingFeatures(const cv::Mat& imageLeft_t0, const cv::Mat& imageRight_t0,\n                       const cv::Mat& imageLeft_t1, const cv::Mat& imageRight_t1,\n                       FeatureSet& currentVOFeatures,\n                       std::vector<cv::Point2f>& pointsLeftT0,\n                       std::vector<cv::Point2f>& pointsRightT0,\n"
                },
                {
                    "date": 1645282271552,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,55 +15,8 @@\n  ****************************************************************/\n #include \"vo.h\"\n \n namespace visual_odometry {\n-// --------------------------------\n-// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/bucket.cpp\n-// --------------------------------\n-Bucket::Bucket(int size) {\n-    max_size = size;\n-}\n-Bucket::~Bucket() {\n-}\n-\n-int Bucket::size() {\n-    return features.points.size();\n-}\n-\n-void Bucket::add_feature(cv::Point2f point, int age) {\n-    // won't add feature with age > 10;\n-    int age_threshold = 10;\n-    if (age < age_threshold) {\n-        // insert any feature before bucket is full\n-        if (size() < max_size) {\n-            features.points.push_back(point);\n-            features.ages.push_back(age);\n-        } else\n-        // insert feature with old age and remove youngest one\n-        {\n-            int age_min = features.ages[0];\n-            int age_min_idx = 0;\n-\n-            for (int i = 0; i < size(); i++) {\n-                if (age < age_min) {\n-                    age_min = age;\n-                    age_min_idx = i;\n-                }\n-            }\n-            features.points[age_min_idx] = point;\n-            features.ages[age_min_idx] = age;\n-        }\n-    }\n-}\n-\n-void Bucket::get_features(FeatureSet& current_features) {\n-    current_features.points.insert(current_features.points.end(),\n-                                   features.points.begin(),\n-                                   features.points.end());\n-    current_features.ages.insert(current_features.ages.end(),\n-                                 features.ages.begin(), features.ages.end());\n-}\n-\n VisualOdometry::VisualOdometry(cv::Mat leftCameraProjection,\n                                cv::Mat rightCameraProjection) {\n     leftCameraProjection_ = leftCameraProjection;\n     rightCameraProjection_ = rightCameraProjection;\n"
                },
                {
                    "date": 1645282529503,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,10 @@\n }\n \n VisualOdometry::~VisualOdometry() {\n }\n+void VisualOdometry::stereo_callback_(const cv::Mat& imageLeft,\n+                     const cv::Mat& imageRight) {\n \n void VisualOdometry::stereo_callback(const cv::Mat& imageLeft,\n                      const cv::Mat& imageRight) {\n     // Wait until we have at least two time steps of data\n"
                },
                {
                    "date": 1645283541119,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,48 +17,46 @@\n \n namespace visual_odometry {\n VisualOdometry::VisualOdometry(cv::Mat leftCameraProjection,\n                                cv::Mat rightCameraProjection) {\n-    leftCameraProjection_ = leftCameraProjection;\n-    rightCameraProjection_ = rightCameraProjection;\n+  leftCameraProjection_ = leftCameraProjection;\n+  rightCameraProjection_ = rightCameraProjection;\n }\n \n-VisualOdometry::~VisualOdometry() {\n-}\n-void VisualOdometry::stereo_callback_(const cv::Mat& imageLeft,\n-                     const cv::Mat& imageRight) {\n+VisualOdometry::~VisualOdometry() {}\n+void VisualOdometry::stereo_callback_(const cv::Mat &imageLeft,\n+                                      const cv::Mat &imageRight) {\n \n-void VisualOdometry::stereo_callback(const cv::Mat& imageLeft,\n-                     const cv::Mat& imageRight) {\n+  void VisualOdometry::stereo_callback(const cv::Mat &imageLeft,\n+                                       const cv::Mat &imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n     if (!frame_id) {\n-        imageLeftT0_ = imageLeft;\n-        imageRightT0_ = imageRight;\n-        frame_id++;\n-        return;\n+      imageLeftT0_ = imageLeft;\n+      imageRightT0_ = imageRight;\n+      frame_id++;\n+      return;\n     }\n \n     imageLeftT1_ = imageLeft;\n     imageRightT1_ = imageRight;\n \n     std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n         pointsRightT1;\n-    \n+\n     matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                      currentVOFeatures, pointsLeftT0, pointsRightT0,\n                      pointsLeftT1, pointsRightT1);\n \n     // Set new images as old images.\n     imageLeftT0_ = imageLeftT1_;\n     imageRightT0_ = imageRightT1_;\n \n-    if (currentVOFeatures.size() < 5)\n-    {\n-        // There are not enough features to fully determine\n-        // equations for pose estimation, so presume nothing and exit.\n-        frame_id++;\n-        return;\n+    if (currentVOFeatures.size() < 5) {\n+      // There are not enough features to fully determine\n+      // equations for pose estimation, so presume nothing and exit.\n+      frame_id++;\n+      return;\n     }\n \n     // ---------------------\n     // Triangulate 3D Points\n@@ -81,12 +79,12 @@\n     // ------------------------------------------------\n     cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n     if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n         abs(rotation_euler[2]) < 0.1) {\n-        integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n+      integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n \n     } else {\n-        std::cout << \"Too large rotation\" << std::endl;\n+      std::cout << \"Too large rotation\" << std::endl;\n     }\n     cv::Mat xyz = frame_pose.col(3).clone();\n     cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n \n@@ -111,79 +109,78 @@\n     //     transform.setOrigin(tf::Vector3(0.0, 0.0, 0.0));\n     //     tf::Quaternion q2(0.5, -0.5, 0.5, -0.5);\n     //     transform.setRotation(q2);\n     //     br.sendTransform(\n-    //         tf::StampedTransform(transform, ros::Time::now(), \"map\", \"odom\"));\n+    //         tf::StampedTransform(transform, ros::Time::now(), \"map\",\n+    //         \"odom\"));\n     // }\n     frame_id++;\n-}\n+  }\n \n-// --------------------------------\n-// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n-// --------------------------------\n+  // --------------------------------\n+  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n+  // --------------------------------\n \n-\n-void featureDetectionFast(cv::Mat image, std::vector<cv::Point2f>& points) {\n+  void featureDetectionFast(cv::Mat image, std::vector<cv::Point2f> & points) {\n     // uses FAST as for feature dection, modify parameters as necessary\n     std::vector<cv::KeyPoint> keypoints;\n     int fast_threshold = 20;\n     bool nonmaxSuppression = true;\n     cv::FAST(image, keypoints, fast_threshold, nonmaxSuppression);\n     cv::KeyPoint::convert(keypoints, points, std::vector<int>());\n-}\n+  }\n \n-void deleteUnmatchFeaturesCircle(\n-    std::vector<cv::Point2f>& points0, std::vector<cv::Point2f>& points1,\n-    std::vector<cv::Point2f>& points2, std::vector<cv::Point2f>& points3,\n-    std::vector<cv::Point2f>& points0_return, std::vector<uchar>& status0,\n-    std::vector<uchar>& status1, std::vector<uchar>& status2,\n-    std::vector<uchar>& status3, std::vector<int>& ages) {\n+  void deleteUnmatchFeaturesCircle(\n+      std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n+      std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n+      std::vector<cv::Point2f> & points0_return, std::vector<uchar> & status0,\n+      std::vector<uchar> & status1, std::vector<uchar> & status2,\n+      std::vector<uchar> & status3, std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     for (int i = 0; i < ages.size(); ++i) {\n-        ages[i] += 1;\n+      ages[i] += 1;\n     }\n \n     int indexCorrection = 0;\n     for (int i = 0; i < status3.size(); i++) {\n-        cv::Point2f pt0 = points0.at(i - indexCorrection);\n-        cv::Point2f pt1 = points1.at(i - indexCorrection);\n-        cv::Point2f pt2 = points2.at(i - indexCorrection);\n-        cv::Point2f pt3 = points3.at(i - indexCorrection);\n-        cv::Point2f pt0_r = points0_return.at(i - indexCorrection);\n+      cv::Point2f pt0 = points0.at(i - indexCorrection);\n+      cv::Point2f pt1 = points1.at(i - indexCorrection);\n+      cv::Point2f pt2 = points2.at(i - indexCorrection);\n+      cv::Point2f pt3 = points3.at(i - indexCorrection);\n+      cv::Point2f pt0_r = points0_return.at(i - indexCorrection);\n \n-        if ((status3.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n-            (status2.at(i) == 0) || (pt2.x < 0) || (pt2.y < 0) ||\n-            (status1.at(i) == 0) || (pt1.x < 0) || (pt1.y < 0) ||\n-            (status0.at(i) == 0) || (pt0.x < 0) || (pt0.y < 0)) {\n-            if ((pt0.x < 0) || (pt0.y < 0) || (pt1.x < 0) || (pt1.y < 0) ||\n-                (pt2.x < 0) || (pt2.y < 0) || (pt3.x < 0) || (pt3.y < 0)) {\n-                status3.at(i) = 0;\n-            }\n-            points0.erase(points0.begin() + (i - indexCorrection));\n-            points1.erase(points1.begin() + (i - indexCorrection));\n-            points2.erase(points2.begin() + (i - indexCorrection));\n-            points3.erase(points3.begin() + (i - indexCorrection));\n-            points0_return.erase(points0_return.begin() +\n-                                 (i - indexCorrection));\n+      if ((status3.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n+          (status2.at(i) == 0) || (pt2.x < 0) || (pt2.y < 0) ||\n+          (status1.at(i) == 0) || (pt1.x < 0) || (pt1.y < 0) ||\n+          (status0.at(i) == 0) || (pt0.x < 0) || (pt0.y < 0)) {\n+        if ((pt0.x < 0) || (pt0.y < 0) || (pt1.x < 0) || (pt1.y < 0) ||\n+            (pt2.x < 0) || (pt2.y < 0) || (pt3.x < 0) || (pt3.y < 0)) {\n+          status3.at(i) = 0;\n+        }\n+        points0.erase(points0.begin() + (i - indexCorrection));\n+        points1.erase(points1.begin() + (i - indexCorrection));\n+        points2.erase(points2.begin() + (i - indexCorrection));\n+        points3.erase(points3.begin() + (i - indexCorrection));\n+        points0_return.erase(points0_return.begin() + (i - indexCorrection));\n \n-            ages.erase(ages.begin() + (i - indexCorrection));\n-            indexCorrection++;\n-        }\n+        ages.erase(ages.begin() + (i - indexCorrection));\n+        indexCorrection++;\n+      }\n     }\n-}\n+  }\n \n-void circularMatching(cv::Mat img_l_0, cv::Mat img_r_0, cv::Mat img_l_1,\n-                      cv::Mat img_r_1, std::vector<cv::Point2f>& points_l_0,\n-                      std::vector<cv::Point2f>& points_r_0,\n-                      std::vector<cv::Point2f>& points_l_1,\n-                      std::vector<cv::Point2f>& points_r_1,\n-                      std::vector<cv::Point2f>& points_l_0_return,\n-                      FeatureSet& current_features) {\n+  void circularMatching(cv::Mat img_l_0, cv::Mat img_r_0, cv::Mat img_l_1,\n+                        cv::Mat img_r_1, std::vector<cv::Point2f> & points_l_0,\n+                        std::vector<cv::Point2f> & points_r_0,\n+                        std::vector<cv::Point2f> & points_l_1,\n+                        std::vector<cv::Point2f> & points_r_1,\n+                        std::vector<cv::Point2f> & points_l_0_return,\n+                        FeatureSet & current_features) {\n     std::vector<float> err;\n \n     cv::Size winSize =\n-        cv::Size(20, 20);   // Lucas-Kanade optical flow window size\n+        cv::Size(20, 20); // Lucas-Kanade optical flow window size\n     cv::TermCriteria termcrit = cv::TermCriteria(\n         cv::TermCriteria::COUNT + cv::TermCriteria::EPS, 30, 0.01);\n \n     std::vector<uchar> status0;\n@@ -203,12 +200,12 @@\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n-}\n+  }\n \n-void bucketingFeatures(cv::Mat& image, FeatureSet& current_features,\n-                       int bucket_size, int features_per_bucket) {\n+  void bucketingFeatures(cv::Mat & image, FeatureSet & current_features,\n+                         int bucket_size, int features_per_bucket) {\n     int image_height = image.rows;\n     int image_width = image.cols;\n     int buckets_nums_height = image_height / bucket_size;\n     int buckets_nums_width = image_width / bucket_size;\n@@ -218,55 +215,55 @@\n \n     // initialize all the buckets\n     for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n          buckets_idx_height++) {\n-        for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n-             buckets_idx_width++) {\n-            Buckets.push_back(Bucket(features_per_bucket));\n-        }\n+      for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n+           buckets_idx_width++) {\n+        Buckets.push_back(Bucket(features_per_bucket));\n+      }\n     }\n \n     // bucket all current features into buckets by their location\n     int buckets_nums_height_idx, buckets_nums_width_idx, buckets_idx;\n     for (int i = 0; i < current_features.points.size(); ++i) {\n-        buckets_nums_height_idx = current_features.points[i].y / bucket_size;\n-        buckets_nums_width_idx = current_features.points[i].x / bucket_size;\n-        buckets_idx = buckets_nums_height_idx * buckets_nums_width +\n-                      buckets_nums_width_idx;\n-        Buckets[buckets_idx].add_feature(current_features.points[i],\n-                                         current_features.ages[i]);\n+      buckets_nums_height_idx = current_features.points[i].y / bucket_size;\n+      buckets_nums_width_idx = current_features.points[i].x / bucket_size;\n+      buckets_idx =\n+          buckets_nums_height_idx * buckets_nums_width + buckets_nums_width_idx;\n+      Buckets[buckets_idx].add_feature(current_features.points[i],\n+                                       current_features.ages[i]);\n     }\n \n     // get features back from buckets\n     current_features.clear();\n     for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n          buckets_idx_height++) {\n-        for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n-             buckets_idx_width++) {\n-            buckets_idx =\n-                buckets_idx_height * buckets_nums_width + buckets_idx_width;\n-            Buckets[buckets_idx].get_features(current_features);\n-        }\n+      for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n+           buckets_idx_width++) {\n+        buckets_idx =\n+            buckets_idx_height * buckets_nums_width + buckets_idx_width;\n+        Buckets[buckets_idx].get_features(current_features);\n+      }\n     }\n-}\n+  }\n \n-void appendNewFeatures(cv::Mat& image, FeatureSet& current_features) {\n+  void appendNewFeatures(cv::Mat & image, FeatureSet & current_features) {\n     std::vector<cv::Point2f> points_new;\n     featureDetectionFast(image, points_new);\n     current_features.points.insert(current_features.points.end(),\n                                    points_new.begin(), points_new.end());\n     std::vector<int> ages_new(points_new.size(), 0);\n     current_features.ages.insert(current_features.ages.end(), ages_new.begin(),\n                                  ages_new.end());\n-}\n+  }\n \n-// --------------------------------\n-// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n-// --------------------------------\n+  // --------------------------------\n+  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n+  // --------------------------------\n \n-void integrateOdometryStereo(int frame_i, cv::Mat& frame_pose,\n-                             const cv::Mat& rotation,\n-                             const cv::Mat& translation_stereo) {\n+  void integrateOdometryStereo(int frame_i, cv::Mat &frame_pose,\n+                               const cv::Mat &rotation,\n+                               const cv::Mat &translation_stereo) {\n     // std::cout << \"rotation\" << rotation << std::endl;\n     // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n \n     cv::Mat rigid_body_transformation;\n@@ -293,89 +290,88 @@\n     // if ((scale>0.1)&&(translation_stereo.at<double>(2) >\n     // translation_stereo.at<double>(0)) && (translation_stereo.at<double>(2) >\n     // translation_stereo.at<double>(1)))\n     /// if (scale > 0.05 && scale < 10)\n-    if (scale > 0.001 && scale < 10)   // WHY DO WE NEED THIS\n+    if (scale > 0.001 && scale < 10) // WHY DO WE NEED THIS\n     {\n-        // std::cout << \"Rpose\" << Rpose << std::endl;\n+      // std::cout << \"Rpose\" << Rpose << std::endl;\n \n-        frame_pose = frame_pose * rigid_body_transformation;\n+      frame_pose = frame_pose * rigid_body_transformation;\n \n     } else {\n-        std::cout << \"[WARNING] scale below 0.1, or incorrect translation\"\n-                  << std::endl;\n+      std::cout << \"[WARNING] scale below 0.1, or incorrect translation\"\n+                << std::endl;\n     }\n-}\n+  }\n \n-bool isRotationMatrix(cv::Mat& R) {\n+  bool isRotationMatrix(cv::Mat & R) {\n     cv::Mat Rt;\n     transpose(R, Rt);\n     cv::Mat shouldBeIdentity = Rt * R;\n     cv::Mat I = cv::Mat::eye(3, 3, shouldBeIdentity.type());\n \n     return norm(I, shouldBeIdentity) < 1e-6;\n-}\n+  }\n \n-// Calculates rotation matrix to euler angles\n-// The result is the same as MATLAB except the order\n-// of the euler angles ( x and z are swapped ).\n-cv::Vec3f rotationMatrixToEulerAngles(cv::Mat& R) {\n+  // Calculates rotation matrix to euler angles\n+  // The result is the same as MATLAB except the order\n+  // of the euler angles ( x and z are swapped ).\n+  cv::Vec3f rotationMatrixToEulerAngles(cv::Mat & R) {\n     assert(isRotationMatrix(R));\n \n     float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                     R.at<double>(1, 0) * R.at<double>(1, 0));\n \n-    bool singular = sy < 1e-6;   // If\n+    bool singular = sy < 1e-6; // If\n \n     float x, y, z;\n     if (!singular) {\n-        x = atan2(R.at<double>(2, 1), R.at<double>(2, 2));\n-        y = atan2(-R.at<double>(2, 0), sy);\n-        z = atan2(R.at<double>(1, 0), R.at<double>(0, 0));\n+      x = atan2(R.at<double>(2, 1), R.at<double>(2, 2));\n+      y = atan2(-R.at<double>(2, 0), sy);\n+      z = atan2(R.at<double>(1, 0), R.at<double>(0, 0));\n     } else {\n-        x = atan2(-R.at<double>(1, 2), R.at<double>(1, 1));\n-        y = atan2(-R.at<double>(2, 0), sy);\n-        z = 0;\n+      x = atan2(-R.at<double>(1, 2), R.at<double>(1, 1));\n+      y = atan2(-R.at<double>(2, 0), sy);\n+      z = 0;\n     }\n     return cv::Vec3f(x, y, z);\n-}\n+  }\n \n-// --------------------------------\n-// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n-// --------------------------------\n+  // --------------------------------\n+  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n+  // --------------------------------\n \n-void checkValidMatch(std::vector<cv::Point2f>& points,\n-                     std::vector<cv::Point2f>& points_return,\n-                     std::vector<bool>& status, int threshold) {\n+  void checkValidMatch(std::vector<cv::Point2f> & points,\n+                       std::vector<cv::Point2f> & points_return,\n+                       std::vector<bool> & status, int threshold) {\n     int offset;\n     for (int i = 0; i < points.size(); i++) {\n-        offset = std::max(std::abs(points[i].x - points_return[i].x),\n-                          std::abs(points[i].y - points_return[i].y));\n-        if (offset > threshold) {\n-            status.push_back(false);\n-        } else {\n-            status.push_back(true);\n-        }\n+      offset = std::max(std::abs(points[i].x - points_return[i].x),\n+                        std::abs(points[i].y - points_return[i].y));\n+      if (offset > threshold) {\n+        status.push_back(false);\n+      } else {\n+        status.push_back(true);\n+      }\n     }\n-}\n+  }\n \n-void removeInvalidPoints(std::vector<cv::Point2f>& points,\n-                         const std::vector<bool>& status) {\n+  void removeInvalidPoints(std::vector<cv::Point2f> & points,\n+                           const std::vector<bool> &status) {\n     int index = 0;\n     for (int i = 0; i < status.size(); i++) {\n-        if (status[i] == false) {\n-            points.erase(points.begin() + index);\n-        } else {\n-            index++;\n-        }\n+      if (status[i] == false) {\n+        points.erase(points.begin() + index);\n+      } else {\n+        index++;\n+      }\n     }\n-}\n+  }\n \n-void trackingFrame2Frame(cv::Mat& leftCameraProjection_,\n-                         cv::Mat& rightCameraProjection_,\n-                         std::vector<cv::Point2f>& pointsLeftT1,\n-                         cv::Mat& points3D_T0, cv::Mat& rotation,\n-                         cv::Mat& translation, bool mono_rotation) {\n+  void trackingFrame2Frame(\n+      cv::Mat & leftCameraProjection_, cv::Mat & rightCameraProjection_,\n+      std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n+      cv::Mat & rotation, cv::Mat & translation, bool mono_rotation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n     cv::Mat intrinsic_matrix =\n@@ -388,12 +384,12 @@\n          leftCameraProjection_.at<float>(1, 1),\n          leftCameraProjection_.at<float>(1, 2),\n          leftCameraProjection_.at<float>(1, 3));\n \n-    int iterationsCount = 500;   // number of Ransac iterations.\n+    int iterationsCount = 500; // number of Ransac iterations.\n     float reprojectionError =\n-        .5;   // maximum allowed distance to consider it an inlier.\n-    float confidence = 0.999;   // RANSAC successful confidence.\n+        .5; // maximum allowed distance to consider it an inlier.\n+    float confidence = 0.999; // RANSAC successful confidence.\n     bool useExtrinsicGuess = true;\n     int flags = cv::SOLVEPNP_ITERATIVE;\n \n     cv::Mat inliers;\n@@ -401,38 +397,37 @@\n                        rvec, translation, useExtrinsicGuess, iterationsCount,\n                        reprojectionError, confidence, inliers, flags);\n \n     cv::Rodrigues(rvec, rotation);\n-}\n+  }\n \n-statuc void matchingFeatures(const cv::Mat& imageLeft_t0, const cv::Mat& imageRight_t0,\n-                      const cv::Mat& imageLeft_t1, const cv::Mat& imageRight_t1,\n-                      FeatureSet& currentVOFeatures,\n-                      std::vector<cv::Point2f>& pointsLeftT0,\n-                      std::vector<cv::Point2f>& pointsRightT0,\n-                      std::vector<cv::Point2f>& pointsLeftT1,\n-                      std::vector<cv::Point2f>& pointsRightT1) {\n+  statuc void matchingFeatures(\n+      const cv::Mat &imageLeft_t0, const cv::Mat &imageRight_t0,\n+      const cv::Mat &imageLeft_t1, const cv::Mat &imageRight_t1,\n+      FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n+      std::vector<cv::Point2f> &pointsRightT0,\n+      std::vector<cv::Point2f> &pointsLeftT1,\n+      std::vector<cv::Point2f> &pointsRightT1) {\n     // ----------------------------\n     // Feature detection using FAST\n     // ----------------------------\n-    \n-    std::vector<cv::Point2f>\n-        pointsLeftReturn_t0;   // feature points to check cicular mathcing\n-                               // validation\n \n+    std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n+                                                  // cicular mathcing validation\n+\n     // add new features if current number of features is below a threshold. TODO\n     // PARAM\n     if (currentVOFeatures.size() < 2000) {\n-        // append new features with old features\n-        appendNewFeatures(imageLeft_t0, currentVOFeatures);\n+      // append new features with old features\n+      appendNewFeatures(imageLeft_t0, currentVOFeatures);\n     }\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n     int bucket_size =\n-        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / 10;   // TODO PARAM\n-    int features_per_bucket = 1;                               // TODO PARAM\n+        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / 10; // TODO PARAM\n+    int features_per_bucket = 1;                             // TODO PARAM\n     std::cout << \"number of features before bucketing: \"\n               << currentVOFeatures.points.size() << std::endl;\n \n     // filter features in currentVOFeatures so that one per bucket\n@@ -440,20 +435,20 @@\n                       features_per_bucket);\n     pointsLeftT0 = currentVOFeatures.points;\n \n     circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n-                     pointsLeftT0, pointsRightT0, pointsLeftT1,\n-                     pointsRightT1, pointsLeftReturn_t0, currentVOFeatures);\n+                     pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n+                     pointsLeftReturn_t0, currentVOFeatures);\n \n     // check if circled back points are in range of original points\n     std::vector<bool> status;\n     checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n     removeInvalidPoints(pointsLeftT0,\n-                        status);   // can combine into one function\n+                        status); // can combine into one function\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n     removeInvalidPoints(pointsRightT1, status);\n \n     // update current tracked points\n     currentVOFeatures.points = pointsLeftT1;\n-}\n-}   // namespace visual_odometry\n+  }\n+} // namespace visual_odometry\n"
                },
                {
                    "date": 1645283552316,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -409,9 +409,8 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     // ----------------------------\n     // Feature detection using FAST\n     // ----------------------------\n-\n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n"
                },
                {
                    "date": 1645283571272,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -406,11 +406,8 @@\n       FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n       std::vector<cv::Point2f> &pointsRightT0,\n       std::vector<cv::Point2f> &pointsLeftT1,\n       std::vector<cv::Point2f> &pointsRightT1) {\n-    // ----------------------------\n-    // Feature detection using FAST\n-    // ----------------------------\n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n"
                },
                {
                    "date": 1645283579494,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -399,9 +399,9 @@\n \n     cv::Rodrigues(rvec, rotation);\n   }\n \n-  statuc void matchingFeatures(\n+  static void matchingFeatures(\n       const cv::Mat &imageLeft_t0, const cv::Mat &imageRight_t0,\n       const cv::Mat &imageLeft_t1, const cv::Mat &imageRight_t1,\n       FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n       std::vector<cv::Point2f> &pointsRightT0,\n"
                },
                {
                    "date": 1645283586744,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -406,9 +406,9 @@\n       FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n       std::vector<cv::Point2f> &pointsRightT0,\n       std::vector<cv::Point2f> &pointsLeftT1,\n       std::vector<cv::Point2f> &pointsRightT1) {\n-    std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n+      std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n     // PARAM\n"
                },
                {
                    "date": 1645283654633,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,33 +87,33 @@\n     }\n     cv::Mat xyz = frame_pose.col(3).clone();\n     cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n \n-    // publish\n-    // if (true) {\n-    //     std::cout << xyz << std::endl;\n-    //     static tf::TransformBroadcaster br;\n+    publish\n+    if (true) {\n+        std::cout << xyz << std::endl;\n+        static tf::TransformBroadcaster br;\n \n-    //     tf::Transform transform;\n-    //     transform.setOrigin(tf::Vector3(xyz.at<double>(0), xyz.at<double>(1),\n-    //                                     xyz.at<double>(2)));\n-    //     tf::Quaternion q;\n-    //     tf::Matrix3x3 R_tf(\n-    //         R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2),\n-    //         R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2),\n-    //         R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2));\n-    //     R_tf.getRotation(q);\n-    //     transform.setRotation(q);\n-    //     br.sendTransform(tf::StampedTransform(transform, ros::Time::now(),\n-    //                                           \"odom\", \"camera\"));\n+        tf::Transform transform;\n+        transform.setOrigin(tf::Vector3(xyz.at<double>(0), xyz.at<double>(1),\n+                                        xyz.at<double>(2)));\n+        tf::Quaternion q;\n+        tf::Matrix3x3 R_tf(\n+            R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2),\n+            R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2),\n+            R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2));\n+        R_tf.getRotation(q);\n+        transform.setRotation(q);\n+        br.sendTransform(tf::StampedTransform(transform, ros::Time::now(),\n+                                              \"odom\", \"camera\"));\n \n-    //     transform.setOrigin(tf::Vector3(0.0, 0.0, 0.0));\n-    //     tf::Quaternion q2(0.5, -0.5, 0.5, -0.5);\n-    //     transform.setRotation(q2);\n-    //     br.sendTransform(\n-    //         tf::StampedTransform(transform, ros::Time::now(), \"map\",\n-    //         \"odom\"));\n-    // }\n+        transform.setOrigin(tf::Vector3(0.0, 0.0, 0.0));\n+        tf::Quaternion q2(0.5, -0.5, 0.5, -0.5);\n+        transform.setRotation(q2);\n+        br.sendTransform(\n+            tf::StampedTransform(transform, ros::Time::now(), \"map\",\n+            \"odom\"));\n+    }\n     frame_id++;\n   }\n \n   // --------------------------------\n@@ -406,9 +406,10 @@\n       FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n       std::vector<cv::Point2f> &pointsRightT0,\n       std::vector<cv::Point2f> &pointsLeftT1,\n       std::vector<cv::Point2f> &pointsRightT1) {\n-      std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n+    \n+    std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n     // PARAM\n"
                },
                {
                    "date": 1645283665101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,11 +87,10 @@\n     }\n     cv::Mat xyz = frame_pose.col(3).clone();\n     cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n \n-    publish\n+    // publish\n     if (true) {\n-        std::cout << xyz << std::endl;\n         static tf::TransformBroadcaster br;\n \n         tf::Transform transform;\n         transform.setOrigin(tf::Vector3(xyz.at<double>(0), xyz.at<double>(1),\n"
                },
                {
                    "date": 1645288132631,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -201,51 +201,9 @@\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n   }\n \n-  void bucketingFeatures(cv::Mat & image, FeatureSet & current_features,\n-                         int bucket_size, int features_per_bucket) {\n-    int image_height = image.rows;\n-    int image_width = image.cols;\n-    int buckets_nums_height = image_height / bucket_size;\n-    int buckets_nums_width = image_width / bucket_size;\n-    int buckets_number = buckets_nums_height * buckets_nums_width;\n \n-    std::vector<Bucket> Buckets;\n-\n-    // initialize all the buckets\n-    for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n-         buckets_idx_height++) {\n-      for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n-           buckets_idx_width++) {\n-        Buckets.push_back(Bucket(features_per_bucket));\n-      }\n-    }\n-\n-    // bucket all current features into buckets by their location\n-    int buckets_nums_height_idx, buckets_nums_width_idx, buckets_idx;\n-    for (int i = 0; i < current_features.points.size(); ++i) {\n-      buckets_nums_height_idx = current_features.points[i].y / bucket_size;\n-      buckets_nums_width_idx = current_features.points[i].x / bucket_size;\n-      buckets_idx =\n-          buckets_nums_height_idx * buckets_nums_width + buckets_nums_width_idx;\n-      Buckets[buckets_idx].add_feature(current_features.points[i],\n-                                       current_features.ages[i]);\n-    }\n-\n-    // get features back from buckets\n-    current_features.clear();\n-    for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n-         buckets_idx_height++) {\n-      for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n-           buckets_idx_width++) {\n-        buckets_idx =\n-            buckets_idx_height * buckets_nums_width + buckets_idx_width;\n-        Buckets[buckets_idx].get_features(current_features);\n-      }\n-    }\n-  }\n-\n   void appendNewFeatures(cv::Mat & image, FeatureSet & current_features) {\n     std::vector<cv::Point2f> points_new;\n     featureDetectionFast(image, points_new);\n     current_features.points.insert(current_features.points.end(),\n"
                },
                {
                    "date": 1645288655820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -384,9 +384,9 @@\n     std::cout << \"number of features before bucketing: \"\n               << currentVOFeatures.points.size() << std::endl;\n \n     // filter features in currentVOFeatures so that one per bucket\n-    bucketingFeatures(imageLeft_t0, currentVOFeatures, bucket_size,\n+    currentVOFeatures.bucketingFeatures(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n     pointsLeftT0 = currentVOFeatures.points;\n \n     circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n"
                },
                {
                    "date": 1645288674653,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -383,9 +383,9 @@\n     int features_per_bucket = 1;                             // TODO PARAM\n     std::cout << \"number of features before bucketing: \"\n               << currentVOFeatures.points.size() << std::endl;\n \n-    // filter features in currentVOFeatures so that one per bucket\n+    // filter features in currentVOFeatures to leave just one per bucket\n     currentVOFeatures.bucketingFeatures(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n     pointsLeftT0 = currentVOFeatures.points;\n \n"
                },
                {
                    "date": 1645288718526,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -384,10 +384,11 @@\n     std::cout << \"number of features before bucketing: \"\n               << currentVOFeatures.points.size() << std::endl;\n \n     // filter features in currentVOFeatures to leave just one per bucket\n-    currentVOFeatures.bucketingFeatures(imageLeft_t0, bucket_size,\n+    currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n+\n     pointsLeftT0 = currentVOFeatures.points;\n \n     circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                      pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n"
                },
                {
                    "date": 1645288770842,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -380,10 +380,8 @@\n     // --------------------------------------------------------\n     int bucket_size =\n         std::min(imageLeft_t0.rows, imageLeft_t0.cols) / 10; // TODO PARAM\n     int features_per_bucket = 1;                             // TODO PARAM\n-    std::cout << \"number of features before bucketing: \"\n-              << currentVOFeatures.points.size() << std::endl;\n \n     // filter features in currentVOFeatures to leave just one per bucket\n     currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n"
                },
                {
                    "date": 1645288857511,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -369,9 +369,9 @@\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n     // PARAM\n-    if (currentVOFeatures.size() < 2000) {\n+    if (currentVOFeatures.size() < VisualOdometry::MAX_FEATURES_TO_RUN_MATCHING) {\n       // append new features with old features\n       appendNewFeatures(imageLeft_t0, currentVOFeatures);\n     }\n \n"
                },
                {
                    "date": 1645288863974,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -369,9 +369,9 @@\n                                                   // cicular mathcing validation\n \n     // add new features if current number of features is below a threshold. TODO\n     // PARAM\n-    if (currentVOFeatures.size() < VisualOdometry::MAX_FEATURES_TO_RUN_MATCHING) {\n+    if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n       // append new features with old features\n       appendNewFeatures(imageLeft_t0, currentVOFeatures);\n     }\n \n"
                },
                {
                    "date": 1645289040781,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -367,10 +367,9 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n-    // add new features if current number of features is below a threshold. TODO\n-    // PARAM\n+    // add new features if current number of features is below a threshold.\n     if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n       // append new features with old features\n       appendNewFeatures(imageLeft_t0, currentVOFeatures);\n     }\n"
                },
                {
                    "date": 1645289220409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -367,9 +367,9 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n-    // add new features if current number of features is below a threshold.\n+    // Add new features if current number of features is below a threshold.\n     if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n       // append new features with old features\n       appendNewFeatures(imageLeft_t0, currentVOFeatures);\n     }\n@@ -377,10 +377,10 @@\n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n     int bucket_size =\n-        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / 10; // TODO PARAM\n-    int features_per_bucket = 1;                             // TODO PARAM\n+        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n+    int features_per_bucket = FEATURES_PER_BUCKET;\n \n     // filter features in currentVOFeatures to leave just one per bucket\n     currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n"
                },
                {
                    "date": 1645289279457,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -366,15 +366,11 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n+    // append new features with old features\n+    appendNewFeatures(imageLeft_t0, currentVOFeatures);\n \n-    // Add new features if current number of features is below a threshold.\n-    if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n-      // append new features with old features\n-      appendNewFeatures(imageLeft_t0, currentVOFeatures);\n-    }\n-\n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n     int bucket_size =\n"
                },
                {
                    "date": 1645289299321,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -366,9 +366,9 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n-    // append new features with old features\n+    // Append new features with old features.\n     appendNewFeatures(imageLeft_t0, currentVOFeatures);\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n@@ -376,9 +376,9 @@\n     int bucket_size =\n         std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n     int features_per_bucket = FEATURES_PER_BUCKET;\n \n-    // filter features in currentVOFeatures to leave just one per bucket\n+    // Filter features in currentVOFeatures to leave just one per bucket.\n     currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n \n     pointsLeftT0 = currentVOFeatures.points;\n"
                },
                {
                    "date": 1645289338528,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -366,19 +366,23 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n-    // Append new features with old features.\n-    appendNewFeatures(imageLeft_t0, currentVOFeatures);\n \n+    // Add new features if current number of features is below a threshold.\n+    if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n+      // append new features with old features\n+      appendNewFeatures(imageLeft_t0, currentVOFeatures);\n+    }\n+\n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n     int bucket_size =\n         std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n     int features_per_bucket = FEATURES_PER_BUCKET;\n \n-    // Filter features in currentVOFeatures to leave just one per bucket.\n+    // filter features in currentVOFeatures to leave just one per bucket\n     currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n \n     pointsLeftT0 = currentVOFeatures.points;\n"
                },
                {
                    "date": 1645289383711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -367,13 +367,10 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n \n-    // Add new features if current number of features is below a threshold.\n-    if (currentVOFeatures.size() < MAX_FEATURES_TO_RUN_MATCHING) {\n-      // append new features with old features\n-      appendNewFeatures(imageLeft_t0, currentVOFeatures);\n-    }\n+    // append new features with old features\n+    appendNewFeatures(imageLeft_t0, currentVOFeatures);\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1645289541822,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -366,9 +366,8 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n-\n     // append new features with old features\n     appendNewFeatures(imageLeft_t0, currentVOFeatures);\n \n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1645289691120,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -202,18 +202,8 @@\n                                 status3, current_features.ages);\n   }\n \n \n-  void appendNewFeatures(cv::Mat & image, FeatureSet & current_features) {\n-    std::vector<cv::Point2f> points_new;\n-    featureDetectionFast(image, points_new);\n-    current_features.points.insert(current_features.points.end(),\n-                                   points_new.begin(), points_new.end());\n-    std::vector<int> ages_new(points_new.size(), 0);\n-    current_features.ages.insert(current_features.ages.end(), ages_new.begin(),\n-                                 ages_new.end());\n-  }\n-\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n   // --------------------------------\n \n@@ -367,9 +357,9 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // cicular mathcing validation\n     // append new features with old features\n-    appendNewFeatures(imageLeft_t0, currentVOFeatures);\n+    currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching\n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1645289905178,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,22 +249,12 @@\n                 << std::endl;\n     }\n   }\n \n-  bool isRotationMatrix(cv::Mat & R) {\n-    cv::Mat Rt;\n-    transpose(R, Rt);\n-    cv::Mat shouldBeIdentity = Rt * R;\n-    cv::Mat I = cv::Mat::eye(3, 3, shouldBeIdentity.type());\n-\n-    return norm(I, shouldBeIdentity) < 1e-6;\n-  }\n-\n   // Calculates rotation matrix to euler angles\n   // The result is the same as MATLAB except the order\n   // of the euler angles ( x and z are swapped ).\n   cv::Vec3f rotationMatrixToEulerAngles(cv::Mat & R) {\n-    assert(isRotationMatrix(R));\n \n     float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                     R.at<double>(1, 0) * R.at<double>(1, 0));\n \n"
                },
                {
                    "date": 1645289979985,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,13 +253,12 @@\n   // Calculates rotation matrix to euler angles\n   // The result is the same as MATLAB except the order\n   // of the euler angles ( x and z are swapped ).\n   cv::Vec3f rotationMatrixToEulerAngles(cv::Mat & R) {\n-\n     float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                     R.at<double>(1, 0) * R.at<double>(1, 0));\n \n-    bool singular = sy < 1e-6; // If\n+    bool singular = sy < 1e-6;\n \n     float x, y, z;\n     if (!singular) {\n       x = atan2(R.at<double>(2, 1), R.at<double>(2, 2));\n"
                },
                {
                    "date": 1645290035199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -229,11 +229,8 @@\n                             (translation_stereo.at<double>(1)) +\n                         (translation_stereo.at<double>(2)) *\n                             (translation_stereo.at<double>(2)));\n \n-    // frame_pose = frame_pose * rigid_body_transformation;\n-    std::cout << \"scale: \" << scale << std::endl;\n-\n     rigid_body_transformation = rigid_body_transformation.inv();\n     // if ((scale>0.1)&&(translation_stereo.at<double>(2) >\n     // translation_stereo.at<double>(0)) && (translation_stereo.at<double>(2) >\n     // translation_stereo.at<double>(1)))\n"
                },
                {
                    "date": 1645290040334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,12 +230,8 @@\n                         (translation_stereo.at<double>(2)) *\n                             (translation_stereo.at<double>(2)));\n \n     rigid_body_transformation = rigid_body_transformation.inv();\n-    // if ((scale>0.1)&&(translation_stereo.at<double>(2) >\n-    // translation_stereo.at<double>(0)) && (translation_stereo.at<double>(2) >\n-    // translation_stereo.at<double>(1)))\n-    /// if (scale > 0.05 && scale < 10)\n     if (scale > 0.001 && scale < 10) // WHY DO WE NEED THIS\n     {\n       // std::cout << \"Rpose\" << Rpose << std::endl;\n \n"
                },
                {
                    "date": 1645290265486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,11 +24,8 @@\n \n VisualOdometry::~VisualOdometry() {}\n void VisualOdometry::stereo_callback_(const cv::Mat &imageLeft,\n                                       const cv::Mat &imageRight) {\n-\n-  void VisualOdometry::stereo_callback(const cv::Mat &imageLeft,\n-                                       const cv::Mat &imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n     if (!frame_id) {\n       imageLeftT0_ = imageLeft;\n"
                },
                {
                    "date": 1645290277038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,10 +15,10 @@\n  ****************************************************************/\n #include \"vo.h\"\n \n namespace visual_odometry {\n-VisualOdometry::VisualOdometry(cv::Mat leftCameraProjection,\n-                               cv::Mat rightCameraProjection) {\n+VisualOdometry::VisualOdometry(const cv::Mat leftCameraProjection,\n+                               const cv::Mat rightCameraProjection) {\n   leftCameraProjection_ = leftCameraProjection;\n   rightCameraProjection_ = rightCameraProjection;\n }\n \n"
                },
                {
                    "date": 1645290284946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,9 +115,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n   // --------------------------------\n \n-  void featureDetectionFast(cv::Mat image, std::vector<cv::Point2f> & points) {\n+  void featureDetectionFast(const cv::Mat image, std::vector<cv::Point2f> & points) {\n     // uses FAST as for feature dection, modify parameters as necessary\n     std::vector<cv::KeyPoint> keypoints;\n     int fast_threshold = 20;\n     bool nonmaxSuppression = true;\n"
                },
                {
                    "date": 1645290412368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -115,17 +115,8 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n   // --------------------------------\n \n-  void featureDetectionFast(const cv::Mat image, std::vector<cv::Point2f> & points) {\n-    // uses FAST as for feature dection, modify parameters as necessary\n-    std::vector<cv::KeyPoint> keypoints;\n-    int fast_threshold = 20;\n-    bool nonmaxSuppression = true;\n-    cv::FAST(image, keypoints, fast_threshold, nonmaxSuppression);\n-    cv::KeyPoint::convert(keypoints, points, std::vector<int>());\n-  }\n-\n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n       std::vector<cv::Point2f> & points0_return, std::vector<uchar> & status0,\n"
                },
                {
                    "date": 1645291902644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -233,9 +233,9 @@\n \n   // Calculates rotation matrix to euler angles\n   // The result is the same as MATLAB except the order\n   // of the euler angles ( x and z are swapped ).\n-  cv::Vec3f rotationMatrixToEulerAngles(cv::Mat & R) {\n+  cv::Vec3f const rotationMatrixToEulerAngles(cv::Mat & R) {\n     float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                     R.at<double>(1, 0) * R.at<double>(1, 0));\n \n     bool singular = sy < 1e-6;\n"
                },
                {
                    "date": 1645292305334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -210,9 +210,9 @@\n \n     // std::cout << \"rigid_body_transformation\" << rigid_body_transformation <<\n     // std::endl;\n \n-    double scale = sqrt((translation_stereo.at<double>(0)) *\n+    const double scale = sqrt((translation_stereo.at<double>(0)) *\n                             (translation_stereo.at<double>(0)) +\n                         (translation_stereo.at<double>(1)) *\n                             (translation_stereo.at<double>(1)) +\n                         (translation_stereo.at<double>(2)) *\n"
                },
                {
                    "date": 1645292358215,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,11 +118,11 @@\n \n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points0_return, std::vector<uchar> & status0,\n-      std::vector<uchar> & status1, std::vector<uchar> & status2,\n-      std::vector<uchar> & status3, std::vector<int> & ages) {\n+      const std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status0,\n+      const std::vector<uchar> & status1, const std::vector<uchar> & status2,\n+      const std::vector<uchar> & status3, std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     for (int i = 0; i < ages.size(); ++i) {\n       ages[i] += 1;\n"
                },
                {
                    "date": 1645292368063,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,9 +118,9 @@\n \n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      const std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status0,\n+      std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status0,\n       const std::vector<uchar> & status1, const std::vector<uchar> & status2,\n       const std::vector<uchar> & status3, std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n"
                },
                {
                    "date": 1645292679311,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,9 +182,9 @@\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n-                         status3, err, winSize, 3, termcrit, 0, 0.001);\n+                         status4, err, winSize, 3, termcrit, 0, 0.001);\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n"
                },
                {
                    "date": 1645293050166,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,8 +183,9 @@\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n+    // TODO: Do all vectors neccessarily have the same length? ~ Alex\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n"
                },
                {
                    "date": 1645293062439,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,9 +183,9 @@\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n-    // TODO: Do all vectors neccessarily have the same length? ~ Alex\n+    // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious ~ Alex\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n"
                },
                {
                    "date": 1645293115405,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,9 +183,10 @@\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n-    // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious ~ Alex\n+    // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+        \n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n"
                },
                {
                    "date": 1645293123864,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -184,9 +184,9 @@\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n     // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n-        \n+    std::vector<uchar> status_all;\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status0, status1, status2,\n                                 status3, current_features.ages);\n"
                },
                {
                    "date": 1645293162726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,12 +185,14 @@\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n     // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n     std::vector<uchar> status_all;\n+    for(int i = 0; i < status3.size(); i++) {\n+      status_all[i] = status[0] | status[1] | status[2] | status[3];\n+    }\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n-                                points_l_0_return, status0, status1, status2,\n-                                status3, current_features.ages);\n+                                points_l_0_return, status_all, current_features.ages);\n   }\n \n \n   // --------------------------------\n"
                },
                {
                    "date": 1645293175412,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,11 +118,10 @@\n \n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status0,\n-      const std::vector<uchar> & status1, const std::vector<uchar> & status2,\n-      const std::vector<uchar> & status3, std::vector<int> & ages) {\n+      std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status_all,\n+      std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     for (int i = 0; i < ages.size(); ++i) {\n       ages[i] += 1;\n"
                },
                {
                    "date": 1645293189197,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -134,15 +134,15 @@\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n       cv::Point2f pt0_r = points0_return.at(i - indexCorrection);\n \n-      if ((status3.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n-          (status2.at(i) == 0) || (pt2.x < 0) || (pt2.y < 0) ||\n-          (status1.at(i) == 0) || (pt1.x < 0) || (pt1.y < 0) ||\n-          (status0.at(i) == 0) || (pt0.x < 0) || (pt0.y < 0)) {\n+      if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n+           (pt2.x < 0) || (pt2.y < 0) ||\n+           (pt1.x < 0) || (pt1.y < 0) ||\n+           (pt0.x < 0) || (pt0.y < 0)) {\n         if ((pt0.x < 0) || (pt0.y < 0) || (pt1.x < 0) || (pt1.y < 0) ||\n             (pt2.x < 0) || (pt2.y < 0) || (pt3.x < 0) || (pt3.y < 0)) {\n-          status3.at(i) = 0;\n+          status_all.at(i) = 0;\n         }\n         points0.erase(points0.begin() + (i - indexCorrection));\n         points1.erase(points1.begin() + (i - indexCorrection));\n         points2.erase(points2.begin() + (i - indexCorrection));\n"
                },
                {
                    "date": 1645293240910,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,12 +138,9 @@\n       if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (pt2.x < 0) || (pt2.y < 0) ||\n            (pt1.x < 0) || (pt1.y < 0) ||\n            (pt0.x < 0) || (pt0.y < 0)) {\n-        if ((pt0.x < 0) || (pt0.y < 0) || (pt1.x < 0) || (pt1.y < 0) ||\n-            (pt2.x < 0) || (pt2.y < 0) || (pt3.x < 0) || (pt3.y < 0)) {\n-          status_all.at(i) = 0;\n-        }\n+        status_all.at(i) = 0;\n         points0.erase(points0.begin() + (i - indexCorrection));\n         points1.erase(points1.begin() + (i - indexCorrection));\n         points2.erase(points2.begin() + (i - indexCorrection));\n         points3.erase(points3.begin() + (i - indexCorrection));\n"
                },
                {
                    "date": 1645293248714,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -127,9 +127,9 @@\n       ages[i] += 1;\n     }\n \n     int indexCorrection = 0;\n-    for (int i = 0; i < status3.size(); i++) {\n+    for (int i = 0; i < status_all.size(); i++) {\n       cv::Point2f pt0 = points0.at(i - indexCorrection);\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n"
                },
                {
                    "date": 1645293257556,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,9 +138,8 @@\n       if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (pt2.x < 0) || (pt2.y < 0) ||\n            (pt1.x < 0) || (pt1.y < 0) ||\n            (pt0.x < 0) || (pt0.y < 0)) {\n-        status_all.at(i) = 0;\n         points0.erase(points0.begin() + (i - indexCorrection));\n         points1.erase(points1.begin() + (i - indexCorrection));\n         points2.erase(points2.begin() + (i - indexCorrection));\n         points3.erase(points3.begin() + (i - indexCorrection));\n"
                },
                {
                    "date": 1645293271622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,9 +178,9 @@\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n-    // TODO: Do all vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+    // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n     std::vector<uchar> status_all;\n     for(int i = 0; i < status3.size(); i++) {\n       status_all[i] = status[0] | status[1] | status[2] | status[3];\n     }\n"
                },
                {
                    "date": 1645293287357,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,9 +178,9 @@\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n-    // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+    // TODO: Why do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n     std::vector<uchar> status_all;\n     for(int i = 0; i < status3.size(); i++) {\n       status_all[i] = status[0] | status[1] | status[2] | status[3];\n     }\n"
                },
                {
                    "date": 1645293294267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -170,17 +170,18 @@\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n     // sparse iterative version of the Lucas-Kanade optical flow in pyramids\n+    // TODO: Why do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n     calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                          status4, err, winSize, 3, termcrit, 0, 0.001);\n-    // TODO: Why do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+\n     std::vector<uchar> status_all;\n     for(int i = 0; i < status3.size(); i++) {\n       status_all[i] = status[0] | status[1] | status[2] | status[3];\n     }\n"
                },
                {
                    "date": 1645293299661,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,10 +169,10 @@\n     std::vector<uchar> status1;\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n-    // sparse iterative version of the Lucas-Kanade optical flow in pyramids\n     // TODO: Why do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+    // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n     calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n                          winSize, 3, termcrit, 0, 0.001);\n"
                },
                {
                    "date": 1645293342375,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,11 +122,8 @@\n       std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status_all,\n       std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n-    for (int i = 0; i < ages.size(); ++i) {\n-      ages[i] += 1;\n-    }\n \n     int indexCorrection = 0;\n     for (int i = 0; i < status_all.size(); i++) {\n       cv::Point2f pt0 = points0.at(i - indexCorrection);\n@@ -187,8 +184,11 @@\n     }\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status_all, current_features.ages);\n+    for (int i = 0; i < current_features.ages.size(); ++i) {\n+      ages[i] += 1;\n+    }\n   }\n \n \n   // --------------------------------\n"
                },
                {
                    "date": 1645293348490,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -185,9 +185,9 @@\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, status_all, current_features.ages);\n     for (int i = 0; i < current_features.ages.size(); ++i) {\n-      ages[i] += 1;\n+      current_features.ages[i] += 1;\n     }\n   }\n \n \n"
                },
                {
                    "date": 1645293382520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,9 +122,8 @@\n       std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status_all,\n       std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n-\n     int indexCorrection = 0;\n     for (int i = 0; i < status_all.size(); i++) {\n       cv::Point2f pt0 = points0.at(i - indexCorrection);\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n"
                },
                {
                    "date": 1645293455257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -165,9 +165,9 @@\n     std::vector<uchar> status1;\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n-    // TODO: Why do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+    // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n     // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n     calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n"
                },
                {
                    "date": 1645293462835,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -165,9 +165,10 @@\n     std::vector<uchar> status1;\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n-    // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious... ~ Alex\n+    // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious,\n+    // and if not, that's a bug... ~ Alex\n     // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n     calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n"
                },
                {
                    "date": 1645293481146,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,9 +166,9 @@\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n     // TODO: Do these 4 vectors neccessarily have the same length? Documentation seems suspicious,\n-    // and if not, that's a bug... ~ Alex\n+    // and if not, there's a big bug... ~ Alex\n     // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n     calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n"
                },
                {
                    "date": 1645293495027,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -175,9 +175,9 @@\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n-                         status4, err, winSize, 3, termcrit, 0, 0.001);\n+                         status3, err, winSize, 3, termcrit, 0, 0.001);\n \n     std::vector<uchar> status_all;\n     for(int i = 0; i < status3.size(); i++) {\n       status_all[i] = status[0] | status[1] | status[2] | status[3];\n"
                },
                {
                    "date": 1645293670802,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,9 +118,9 @@\n \n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points0_return, const std::vector<uchar> & status_all,\n+      std::vector<cv::Point2f> & points4, const std::vector<uchar> & status_all,\n       std::vector<int> & ages) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     int indexCorrection = 0;\n@@ -128,9 +128,9 @@\n       cv::Point2f pt0 = points0.at(i - indexCorrection);\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n-      cv::Point2f pt0_r = points0_return.at(i - indexCorrection);\n+      cv::Point2f pt0_r = points4.at(i - indexCorrection);\n \n       if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (pt2.x < 0) || (pt2.y < 0) ||\n            (pt1.x < 0) || (pt1.y < 0) ||\n@@ -138,9 +138,9 @@\n         points0.erase(points0.begin() + (i - indexCorrection));\n         points1.erase(points1.begin() + (i - indexCorrection));\n         points2.erase(points2.begin() + (i - indexCorrection));\n         points3.erase(points3.begin() + (i - indexCorrection));\n-        points0_return.erase(points0_return.begin() + (i - indexCorrection));\n+        points4.erase(points4.begin() + (i - indexCorrection));\n \n         ages.erase(ages.begin() + (i - indexCorrection));\n         indexCorrection++;\n       }\n"
                },
                {
                    "date": 1645293700721,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,9 +128,9 @@\n       cv::Point2f pt0 = points0.at(i - indexCorrection);\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n-      cv::Point2f pt0_r = points4.at(i - indexCorrection);\n+      cv::Point2f pt4 = points4.at(i - indexCorrection);\n \n       if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (pt2.x < 0) || (pt2.y < 0) ||\n            (pt1.x < 0) || (pt1.y < 0) ||\n"
                },
                {
                    "date": 1645293749444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -118,10 +118,10 @@\n \n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points4, const std::vector<uchar> & status_all,\n-      std::vector<int> & ages) {\n+      std::vector<cv::Point2f> & points4, std::vector<int> & ages,\n+      const std::vector<uchar> & status_all,) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     int indexCorrection = 0;\n     for (int i = 0; i < status_all.size(); i++) {\n"
                },
                {
                    "date": 1645293756788,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,9 +119,9 @@\n   void deleteUnmatchFeaturesCircle(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n       std::vector<cv::Point2f> & points4, std::vector<int> & ages,\n-      const std::vector<uchar> & status_all,) {\n+      const std::vector<uchar> & status_all) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     int indexCorrection = 0;\n     for (int i = 0; i < status_all.size(); i++) {\n"
                },
                {
                    "date": 1645293772737,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -183,9 +183,9 @@\n       status_all[i] = status[0] | status[1] | status[2] | status[3];\n     }\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n-                                points_l_0_return, status_all, current_features.ages);\n+                                points_l_0_return, current_features.ages, status_all);\n     for (int i = 0; i < current_features.ages.size(); ++i) {\n       current_features.ages[i] += 1;\n     }\n   }\n"
                },
                {
                    "date": 1645293848800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,9 +129,11 @@\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n       cv::Point2f pt4 = points4.at(i - indexCorrection);\n-\n+      // TODO: Why are we even considering the x/y coordinates here, should we do it for pt4, and\n+      // if we should consider it, should we consider the case where they are out of the image along the\n+      // image size axes?\n       if ((status_all.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (pt2.x < 0) || (pt2.y < 0) ||\n            (pt1.x < 0) || (pt1.y < 0) ||\n            (pt0.x < 0) || (pt0.y < 0)) {\n"
                },
                {
                    "date": 1645293882147,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,9 +181,9 @@\n                          status3, err, winSize, 3, termcrit, 0, 0.001);\n \n     std::vector<uchar> status_all;\n     for(int i = 0; i < status3.size(); i++) {\n-      status_all[i] = status[0] | status[1] | status[2] | status[3];\n+      status_all[i] = status0[i] | status1[i] | status2[i] | status3[i];\n     }\n \n     deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                 points_l_0_return, current_features.ages, status_all);\n"
                },
                {
                    "date": 1645390213416,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,9 +196,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n   // --------------------------------\n \n-  void integrateOdometryStereo(int frame_i, cv::Mat &frame_pose,\n+  void integrateOdometryStereo(const int frame_i, cv::Mat &frame_pose,\n                                const cv::Mat &rotation,\n                                const cv::Mat &translation_stereo) {\n     // std::cout << \"rotation\" << rotation << std::endl;\n     // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n"
                },
                {
                    "date": 1645390238943,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,9 +196,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n   // --------------------------------\n \n-  void integrateOdometryStereo(const int frame_i, cv::Mat &frame_pose,\n+  void integrateOdometryStereo(const int frame_i, const cv::Mat &frame_pose,\n                                const cv::Mat &rotation,\n                                const cv::Mat &translation_stereo) {\n     // std::cout << \"rotation\" << rotation << std::endl;\n     // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n"
                },
                {
                    "date": 1645390253678,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,9 +196,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n   // --------------------------------\n \n-  void integrateOdometryStereo(const int frame_i, const cv::Mat &frame_pose,\n+  void integrateOdometryStereo(const int frame_i, cv::Mat &frame_pose,\n                                const cv::Mat &rotation,\n                                const cv::Mat &translation_stereo) {\n     // std::cout << \"rotation\" << rotation << std::endl;\n     // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n"
                },
                {
                    "date": 1645390275596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,10 +196,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n   // --------------------------------\n \n-  void integrateOdometryStereo(const int frame_i, cv::Mat &frame_pose,\n-                               const cv::Mat &rotation,\n+  void integrateOdometryStereo(cv::Mat &frame_pose, const cv::Mat &rotation,\n                                const cv::Mat &translation_stereo) {\n     // std::cout << \"rotation\" << rotation << std::endl;\n     // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n \n"
                },
                {
                    "date": 1645390422429,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -198,21 +198,15 @@\n   // --------------------------------\n \n   void integrateOdometryStereo(cv::Mat &frame_pose, const cv::Mat &rotation,\n                                const cv::Mat &translation_stereo) {\n-    // std::cout << \"rotation\" << rotation << std::endl;\n-    // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n-\n     cv::Mat rigid_body_transformation;\n \n     cv::Mat addup = (cv::Mat_<double>(1, 4) << 0, 0, 0, 1);\n \n     cv::hconcat(rotation, translation_stereo, rigid_body_transformation);\n     cv::vconcat(rigid_body_transformation, addup, rigid_body_transformation);\n \n-    // std::cout << \"rigid_body_transformation\" << rigid_body_transformation <<\n-    // std::endl;\n-\n     const double scale = sqrt((translation_stereo.at<double>(0)) *\n                             (translation_stereo.at<double>(0)) +\n                         (translation_stereo.at<double>(1)) *\n                             (translation_stereo.at<double>(1)) +\n@@ -221,12 +215,9 @@\n \n     rigid_body_transformation = rigid_body_transformation.inv();\n     if (scale > 0.001 && scale < 10) // WHY DO WE NEED THIS\n     {\n-      // std::cout << \"Rpose\" << Rpose << std::endl;\n-\n       frame_pose = frame_pose * rigid_body_transformation;\n-\n     } else {\n       std::cout << \"[WARNING] scale below 0.1, or incorrect translation\"\n                 << std::endl;\n     }\n"
                },
                {
                    "date": 1645390802540,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -339,10 +339,9 @@\n                      pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n                      pointsLeftReturn_t0, currentVOFeatures);\n \n     // check if circled back points are in range of original points\n-    std::vector<bool> status;\n-    checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n+    std::vector<bool> status = checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n     removeInvalidPoints(pointsLeftT0,\n                         status); // can combine into one function\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n"
                },
                {
                    "date": 1645390820671,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,10 @@\n   // --------------------------------\n \n   void checkValidMatch(std::vector<cv::Point2f> & points,\n                        std::vector<cv::Point2f> & points_return,\n-                       std::vector<bool> & status, int threshold) {\n+                       int threshold) {\n+    std::vector<bool> status;\n     int offset;\n     for (int i = 0; i < points.size(); i++) {\n       offset = std::max(std::abs(points[i].x - points_return[i].x),\n                         std::abs(points[i].y - points_return[i].y));\n"
                },
                {
                    "date": 1645390829431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,11 +248,11 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n-  void checkValidMatch(std::vector<cv::Point2f> & points,\n-                       std::vector<cv::Point2f> & points_return,\n-                       int threshold) {\n+  void checkValidMatch(const std::vector<cv::Point2f> & points,\n+                       const std::vector<cv::Point2f> & points_return,\n+                       const int threshold) {\n     std::vector<bool> status;\n     int offset;\n     for (int i = 0; i < points.size(); i++) {\n       offset = std::max(std::abs(points[i].x - points_return[i].x),\n"
                },
                {
                    "date": 1645390922748,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -341,10 +341,9 @@\n                      pointsLeftReturn_t0, currentVOFeatures);\n \n     // check if circled back points are in range of original points\n     std::vector<bool> status = checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n-    removeInvalidPoints(pointsLeftT0,\n-                        status); // can combine into one function\n+    removeInvalidPoints(pointsLeftT0, status); // can combine into one function\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n     removeInvalidPoints(pointsRightT1, status);\n \n"
                },
                {
                    "date": 1645390972009,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,16 +248,16 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n-  void checkValidMatch(const std::vector<cv::Point2f> & points,\n-                       const std::vector<cv::Point2f> & points_return,\n+  void checkValidMatch(const std::vector<cv::Point2f> & original_points,\n+                       const std::vector<cv::Point2f> & recomputed_points,\n                        const int threshold) {\n     std::vector<bool> status;\n     int offset;\n-    for (int i = 0; i < points.size(); i++) {\n-      offset = std::max(std::abs(points[i].x - points_return[i].x),\n-                        std::abs(points[i].y - points_return[i].y));\n+    for (int i = 0; i < original_points.size(); i++) {\n+      offset = std::max(std::abs(original_points[i].x - recomputed_points[i].x),\n+                        std::abs(original_points[i].y - recomputed_points[i].y));\n       if (offset > threshold) {\n         status.push_back(false);\n       } else {\n         status.push_back(true);\n"
                },
                {
                    "date": 1645391068632,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,15 +249,15 @@\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n   void checkValidMatch(const std::vector<cv::Point2f> & original_points,\n-                       const std::vector<cv::Point2f> & recomputed_points,\n+                       const std::vector<cv::Point2f> & adjusted_points,\n                        const int threshold) {\n     std::vector<bool> status;\n     int offset;\n     for (int i = 0; i < original_points.size(); i++) {\n-      offset = std::max(std::abs(original_points[i].x - recomputed_points[i].x),\n-                        std::abs(original_points[i].y - recomputed_points[i].y));\n+      offset = std::max(std::abs(original_points[i].x - adjusted_points[i].x),\n+                        std::abs(original_points[i].y - adjusted_points[i].y));\n       if (offset > threshold) {\n         status.push_back(false);\n       } else {\n         status.push_back(true);\n"
                },
                {
                    "date": 1645391141686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,16 +248,16 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n-  void checkValidMatch(const std::vector<cv::Point2f> & original_points,\n-                       const std::vector<cv::Point2f> & adjusted_points,\n+  void checkValidMatch(const std::vector<cv::Point2f> & points_1,\n+                       const std::vector<cv::Point2f> & points_2,\n                        const int threshold) {\n     std::vector<bool> status;\n     int offset;\n-    for (int i = 0; i < original_points.size(); i++) {\n-      offset = std::max(std::abs(original_points[i].x - adjusted_points[i].x),\n-                        std::abs(original_points[i].y - adjusted_points[i].y));\n+    for (int i = 0; i < points_1.size(); i++) {\n+      offset = std::max(std::abs(points_1[i].x - points_2[i].x),\n+                        std::abs(points_1[i].y - points_2[i].y));\n       if (offset > threshold) {\n         status.push_back(false);\n       } else {\n         status.push_back(true);\n"
                },
                {
                    "date": 1645391157747,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,9 +248,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n-  void checkValidMatch(const std::vector<cv::Point2f> & points_1,\n+  void findUnmovedPoints(const std::vector<cv::Point2f> & points_1,\n                        const std::vector<cv::Point2f> & points_2,\n                        const int threshold) {\n     std::vector<bool> status;\n     int offset;\n@@ -340,9 +340,9 @@\n                      pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n                      pointsLeftReturn_t0, currentVOFeatures);\n \n     // check if circled back points are in range of original points\n-    std::vector<bool> status = checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n+    std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n     removeInvalidPoints(pointsLeftT0, status); // can combine into one function\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n     removeInvalidPoints(pointsRightT1, status);\n"
                },
                {
                    "date": 1645391256325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,9 +248,9 @@\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n   // --------------------------------\n \n-  void findUnmovedPoints(const std::vector<cv::Point2f> & points_1,\n+  std::vector<bool> findUnmovedPoints(const std::vector<cv::Point2f> & points_1,\n                        const std::vector<cv::Point2f> & points_2,\n                        const int threshold) {\n     std::vector<bool> status;\n     int offset;\n@@ -262,8 +262,9 @@\n       } else {\n         status.push_back(true);\n       }\n     }\n+    return status\n   }\n \n   void removeInvalidPoints(std::vector<cv::Point2f> & points,\n                            const std::vector<bool> &status) {\n"
                },
                {
                    "date": 1645391697366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -319,9 +319,9 @@\n       std::vector<cv::Point2f> &pointsLeftT1,\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n-                                                  // cicular mathcing validation\n+                                                  // cicular matching validation\n     // append new features with old features\n     currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1645391842192,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,9 +278,9 @@\n     }\n   }\n \n   void trackingFrame2Frame(\n-      cv::Mat & leftCameraProjection_, cv::Mat & rightCameraProjection_,\n+      const cv::Mat & leftCameraProjection_, cv::Mat & rightCameraProjection_,\n       std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n       cv::Mat & rotation, cv::Mat & translation, bool mono_rotation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n@@ -319,9 +319,9 @@\n       std::vector<cv::Point2f> &pointsLeftT1,\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n-                                                  // cicular matching validation\n+                                                  // circular matching validation\n     // append new features with old features\n     currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1645391907913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -276,11 +276,12 @@\n         index++;\n       }\n     }\n   }\n-\n+  // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n+  // a variable\n   void trackingFrame2Frame(\n-      const cv::Mat & leftCameraProjection_, cv::Mat & rightCameraProjection_,\n+      const cv::Mat & leftCameraProjection_,\n       std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n       cv::Mat & rotation, cv::Mat & translation, bool mono_rotation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n"
                },
                {
                    "date": 1645391985932,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n     // ---------------------\n     // Tracking transfomation\n     // ---------------------\n     // PnP: computes rotation and translation between pair of images\n-    trackingFrame2Frame(leftCameraProjection_, rightCameraProjection_,\n+    trackingFrame2Frame(leftCameraProjection_,\n                         pointsLeftT1, points3D_T0, rotation, translation,\n                         false);\n \n     // ------------------------------------------------\n@@ -281,9 +281,9 @@\n   // a variable\n   void trackingFrame2Frame(\n       const cv::Mat & leftCameraProjection_,\n       std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n-      cv::Mat & rotation, cv::Mat & translation, bool mono_rotation) {\n+      cv::Mat & rotation, cv::Mat & translation, cons tbool mono_rotation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n     cv::Mat intrinsic_matrix =\n"
                },
                {
                    "date": 1645392011022,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -281,9 +281,9 @@\n   // a variable\n   void trackingFrame2Frame(\n       const cv::Mat & leftCameraProjection_,\n       std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n-      cv::Mat & rotation, cv::Mat & translation, cons tbool mono_rotation) {\n+      cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n     cv::Mat intrinsic_matrix =\n"
                },
                {
                    "date": 1645392237259,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -280,9 +280,9 @@\n   // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n   // a variable\n   void trackingFrame2Frame(\n       const cv::Mat & leftCameraProjection_,\n-      std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n+      const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n       cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n"
                },
                {
                    "date": 1645392335214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -297,10 +297,9 @@\n          leftCameraProjection_.at<float>(1, 2),\n          leftCameraProjection_.at<float>(1, 3));\n \n     int iterationsCount = 500; // number of Ransac iterations.\n-    float reprojectionError =\n-        .5; // maximum allowed distance to consider it an inlier.\n+    float reprojectionError = .5; // maximum allowed distance to consider it an inlier.\n     float confidence = 0.999; // RANSAC successful confidence.\n     bool useExtrinsicGuess = true;\n     int flags = cv::SOLVEPNP_ITERATIVE;\n \n"
                },
                {
                    "date": 1645392422256,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,10 +67,9 @@\n     // Tracking transfomation\n     // ---------------------\n     // PnP: computes rotation and translation between pair of images\n     trackingFrame2Frame(leftCameraProjection_,\n-                        pointsLeftT1, points3D_T0, rotation, translation,\n-                        false);\n+                        pointsLeftT1, points3D_T0, rotation, translation);\n \n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n"
                },
                {
                    "date": 1645392430056,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,9 +67,9 @@\n     // Tracking transfomation\n     // ---------------------\n     // PnP: computes rotation and translation between pair of images\n     trackingFrame2Frame(leftCameraProjection_,\n-                        pointsLeftT1, points3D_T0, rotation, translation);\n+        pointsLeftT1, points3D_T0, rotation, translation);\n \n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n"
                },
                {
                    "date": 1645394820291,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,19 +57,19 @@\n \n     // ---------------------\n     // Triangulate 3D Points\n     // ---------------------\n-    cv::Mat points3D_T0, points4D_T0;\n+    cv::Mat world_points_T0, world_homogenous_points_T0;\n     cv::triangulatePoints(leftCameraProjection_, rightCameraProjection_,\n-                          pointsLeftT0, pointsRightT0, points4D_T0);\n-    cv::convertPointsFromHomogeneous(points4D_T0.t(), points3D_T0);\n+                          pointsLeftT0, pointsRightT0, world_homogenous_points_T0);\n+    cv::convertPointsFromHomogeneous(world_homogenous_points_T0.t(), world_points_T0);\n \n     // ---------------------\n     // Tracking transfomation\n     // ---------------------\n     // PnP: computes rotation and translation between pair of images\n     trackingFrame2Frame(leftCameraProjection_,\n-        pointsLeftT1, points3D_T0, rotation, translation);\n+        pointsLeftT1, world_points_T0, rotation, translation);\n \n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n@@ -279,9 +279,9 @@\n   // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n   // a variable\n   void trackingFrame2Frame(\n       const cv::Mat & leftCameraProjection_,\n-      const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & points3D_T0,\n+      const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & world_points_T0,\n       cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n@@ -302,9 +302,9 @@\n     bool useExtrinsicGuess = true;\n     int flags = cv::SOLVEPNP_ITERATIVE;\n \n     cv::Mat inliers;\n-    cv::solvePnPRansac(points3D_T0, pointsLeftT1, intrinsic_matrix, distCoeffs,\n+    cv::solvePnPRansac(world_points_T0, pointsLeftT1, intrinsic_matrix, distCoeffs,\n                        rvec, translation, useExtrinsicGuess, iterationsCount,\n                        reprojectionError, confidence, inliers, flags);\n \n     cv::Rodrigues(rvec, rotation);\n"
                },
                {
                    "date": 1645395074264,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n     // ---------------------\n     // Tracking transfomation\n     // ---------------------\n     // PnP: computes rotation and translation between pair of images\n-    trackingFrame2Frame(leftCameraProjection_,\n+    worldToCamera(leftCameraProjection_,\n         pointsLeftT1, world_points_T0, rotation, translation);\n \n     // ------------------------------------------------\n     // Integrating\n@@ -277,9 +277,9 @@\n     }\n   }\n   // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n   // a variable\n-  void trackingFrame2Frame(\n+  void worldToCamera(\n       const cv::Mat & leftCameraProjection_,\n       const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & world_points_T0,\n       cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n"
                },
                {
                    "date": 1645395147248,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,14 +73,13 @@\n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n     cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n+  // don't perform an update the pose if the output is unusually large, indicates a error.\n     if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n         abs(rotation_euler[2]) < 0.1) {\n       integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n \n-    } else {\n-      std::cout << \"Too large rotation\" << std::endl;\n     }\n     cv::Mat xyz = frame_pose.col(3).clone();\n     cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n \n"
                },
                {
                    "date": 1645395153555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n     cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n-  // don't perform an update the pose if the output is unusually large, indicates a error.\n+    // don't perform an update if the output is unusually large, indicates a error.\n     if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n         abs(rotation_euler[2]) < 0.1) {\n       integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n \n"
                },
                {
                    "date": 1645395164658,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,13 +73,12 @@\n     // ------------------------------------------------\n     // Integrating\n     // ------------------------------------------------\n     cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n-    // don't perform an update if the output is unusually large, indicates a error.\n+    // Don't perform an update if the output is unusually large, indicates a error elsewhere.\n     if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n         abs(rotation_euler[2]) < 0.1) {\n       integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n-\n     }\n     cv::Mat xyz = frame_pose.col(3).clone();\n     cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n \n"
                },
                {
                    "date": 1645395218146,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,8 @@\n \n     // ---------------------\n     // Tracking transfomation\n     // ---------------------\n-    // PnP: computes rotation and translation between pair of images\n     worldToCamera(leftCameraProjection_,\n         pointsLeftT1, world_points_T0, rotation, translation);\n \n     // ------------------------------------------------\n"
                },
                {
                    "date": 1645395227548,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n \n     // ---------------------\n     // Tracking transfomation\n     // ---------------------\n-    worldToCamera(leftCameraProjection_,\n+    cameraToWorld(leftCameraProjection_,\n         pointsLeftT1, world_points_T0, rotation, translation);\n \n     // ------------------------------------------------\n     // Integrating\n@@ -274,9 +274,9 @@\n     }\n   }\n   // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n   // a variable\n-  void worldToCamera(\n+  void cameraToWorld(\n       const cv::Mat & leftCameraProjection_,\n       const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & world_points_T0,\n       cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n"
                },
                {
                    "date": 1645395323808,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,36 +272,34 @@\n         index++;\n       }\n     }\n   }\n-  // TODO: Seems sus that rightCameraProjection_ was never used here, it was originally\n-  // a variable\n   void cameraToWorld(\n-      const cv::Mat & leftCameraProjection_,\n-      const std::vector<cv::Point2f> & pointsLeftT1, cv::Mat & world_points_T0,\n+      const cv::Mat & cameraProjection,\n+      const std::vector<cv::Point2f> & cameraPoints, cv::Mat & worldPoints,\n       cv::Mat & rotation, cv::Mat & translation) {\n     // Calculate frame to frame transformation\n     cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n     cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n     cv::Mat intrinsic_matrix =\n-        (cv::Mat_<float>(3, 3) << leftCameraProjection_.at<float>(0, 0),\n-         leftCameraProjection_.at<float>(0, 1),\n-         leftCameraProjection_.at<float>(0, 2),\n-         leftCameraProjection_.at<float>(1, 0),\n-         leftCameraProjection_.at<float>(1, 1),\n-         leftCameraProjection_.at<float>(1, 2),\n-         leftCameraProjection_.at<float>(1, 1),\n-         leftCameraProjection_.at<float>(1, 2),\n-         leftCameraProjection_.at<float>(1, 3));\n+        (cv::Mat_<float>(3, 3) << cameraProjection.at<float>(0, 0),\n+         cameraProjection.at<float>(0, 1),\n+         cameraProjection.at<float>(0, 2),\n+         cameraProjection.at<float>(1, 0),\n+         cameraProjection.at<float>(1, 1),\n+         cameraProjection.at<float>(1, 2),\n+         cameraProjection.at<float>(1, 1),\n+         cameraProjection.at<float>(1, 2),\n+         cameraProjection.at<float>(1, 3));\n \n     int iterationsCount = 500; // number of Ransac iterations.\n     float reprojectionError = .5; // maximum allowed distance to consider it an inlier.\n     float confidence = 0.999; // RANSAC successful confidence.\n     bool useExtrinsicGuess = true;\n     int flags = cv::SOLVEPNP_ITERATIVE;\n \n     cv::Mat inliers;\n-    cv::solvePnPRansac(world_points_T0, pointsLeftT1, intrinsic_matrix, distCoeffs,\n+    cv::solvePnPRansac(worldPoints, cameraPoints, intrinsic_matrix, distCoeffs,\n                        rvec, translation, useExtrinsicGuess, iterationsCount,\n                        reprojectionError, confidence, inliers, flags);\n \n     cv::Rodrigues(rvec, rotation);\n"
                },
                {
                    "date": 1645395897760,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -336,9 +336,9 @@\n                      pointsLeftReturn_t0, currentVOFeatures);\n \n     // check if circled back points are in range of original points\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n-    removeInvalidPoints(pointsLeftT0, status); // can combine into one function\n+    removeInvalidPoints(pointsLeftT0, status);\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n     removeInvalidPoints(pointsRightT1, status);\n \n"
                },
                {
                    "date": 1645395925323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -314,19 +314,19 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // circular matching validation\n-    // append new features with old features\n+    // Append new features with old features.\n     currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n     // --------------------------------------------------------\n-    // Feature tracking using KLT tracker, bucketing and circular matching\n+    // Feature tracking using KLT tracker, bucketing and circular matching.\n     // --------------------------------------------------------\n     int bucket_size =\n         std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n     int features_per_bucket = FEATURES_PER_BUCKET;\n \n-    // filter features in currentVOFeatures to leave just one per bucket\n+    // Filter features in currentVOFeatures to leave just one per bucket.\n     currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                       features_per_bucket);\n \n     pointsLeftT0 = currentVOFeatures.points;\n@@ -334,15 +334,15 @@\n     circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                      pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n                      pointsLeftReturn_t0, currentVOFeatures);\n \n-    // check if circled back points are in range of original points\n+    // Check if circled back points are in range of original points.\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n     removeInvalidPoints(pointsLeftT0, status);\n     removeInvalidPoints(pointsLeftT1, status);\n     removeInvalidPoints(pointsRightT0, status);\n     removeInvalidPoints(pointsRightT1, status);\n \n-    // update current tracked points\n+    // Update current tracked points.\n     currentVOFeatures.points = pointsLeftT1;\n   }\n } // namespace visual_odometry\n"
                },
                {
                    "date": 1645648660013,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,8 +22,12 @@\n   rightCameraProjection_ = rightCameraProjection;\n }\n \n VisualOdometry::~VisualOdometry() {}\n+void VisualOdometry::stereo_callback_imageconstptr(const sensor_msgs::ImageConstPtr& image_left,\n+  const sensor_msgs::ImageConstPtr& image_right) {\n+    return VisualOdometry::stereo_callback(image_left, image_right);\n+}\n void VisualOdometry::stereo_callback_(const cv::Mat &imageLeft,\n                                       const cv::Mat &imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n"
                },
                {
                    "date": 1645648676442,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n \n VisualOdometry::~VisualOdometry() {}\n void VisualOdometry::stereo_callback_imageconstptr(const sensor_msgs::ImageConstPtr& image_left,\n   const sensor_msgs::ImageConstPtr& image_right) {\n-    return VisualOdometry::stereo_callback(image_left, image_right);\n+    return VisualOdometry::stereo_callback(rosImage2CvMat(image_left), rosImage2CvMat(image_right));\n }\n void VisualOdometry::stereo_callback_(const cv::Mat &imageLeft,\n                                       const cv::Mat &imageRight) {\n     // Wait until we have at least two time steps of data\n"
                },
                {
                    "date": 1645648697742,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,12 +22,8 @@\n   rightCameraProjection_ = rightCameraProjection;\n }\n \n VisualOdometry::~VisualOdometry() {}\n-void VisualOdometry::stereo_callback_imageconstptr(const sensor_msgs::ImageConstPtr& image_left,\n-  const sensor_msgs::ImageConstPtr& image_right) {\n-    return VisualOdometry::stereo_callback(rosImage2CvMat(image_left), rosImage2CvMat(image_right));\n-}\n void VisualOdometry::stereo_callback_(const cv::Mat &imageLeft,\n                                       const cv::Mat &imageRight) {\n     // Wait until we have at least two time steps of data\n     // to begin predicting the change in pose.\n@@ -111,8 +107,13 @@\n     }\n     frame_id++;\n   }\n \n+  void VisualOdometry::stereo_callback_imageconstptr(const sensor_msgs::ImageConstPtr& image_left,\n+    const sensor_msgs::ImageConstPtr& image_right) {\n+      return VisualOdometry::stereo_callback(rosImage2CvMat(image_left), rosImage2CvMat(image_right));\n+  }\n+\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n   // --------------------------------\n \n"
                },
                {
                    "date": 1645649026139,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,13 +107,8 @@\n     }\n     frame_id++;\n   }\n \n-  void VisualOdometry::stereo_callback_imageconstptr(const sensor_msgs::ImageConstPtr& image_left,\n-    const sensor_msgs::ImageConstPtr& image_right) {\n-      return VisualOdometry::stereo_callback(rosImage2CvMat(image_left), rosImage2CvMat(image_right));\n-  }\n-\n   // --------------------------------\n   // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n   // --------------------------------\n \n"
                }
            ],
            "date": 1645281156817,
            "name": "Commit-0",
            "content": "/****************************************************************\n *\n * @file \t\tvo.cpp\n *\n * @brief \t\tThe Visual Odometry class being used for\n translation. The math can be found in Haidar Jamal's Thesis:\n *https://www.ri.cmu.edu/publications/localization-for-lunar-micro-rovers/\n *\n * @version \t1.0\n * @date \t\t02/09/2022\n *\n * @authors \tBen Kolligs, Alex Li\n * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n *\n ****************************************************************/\n#include \"vo.h\"\n\nnamespace visual_odometry {\n// --------------------------------\n// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/bucket.cpp\n// --------------------------------\nBucket::Bucket(int size) {\n    max_size = size;\n}\nBucket::~Bucket() {\n}\n\nint Bucket::size() {\n    return features.points.size();\n}\n\nvoid Bucket::add_feature(cv::Point2f point, int age) {\n    // won't add feature with age > 10;\n    int age_threshold = 10;\n    if (age < age_threshold) {\n        // insert any feature before bucket is full\n        if (size() < max_size) {\n            features.points.push_back(point);\n            features.ages.push_back(age);\n        } else\n        // insert feature with old age and remove youngest one\n        {\n            int age_min = features.ages[0];\n            int age_min_idx = 0;\n\n            for (int i = 0; i < size(); i++) {\n                if (age < age_min) {\n                    age_min = age;\n                    age_min_idx = i;\n                }\n            }\n            features.points[age_min_idx] = point;\n            features.ages[age_min_idx] = age;\n        }\n    }\n}\n\nvoid Bucket::get_features(FeatureSet& current_features) {\n    current_features.points.insert(current_features.points.end(),\n                                   features.points.begin(),\n                                   features.points.end());\n    current_features.ages.insert(current_features.ages.end(),\n                                 features.ages.begin(), features.ages.end());\n}\n\nVisualOdometry::VisualOdometry(cv::Mat leftCameraProjection,\n                               cv::Mat rightCameraProjection) {\n    leftCameraProjection_ = leftCameraProjection;\n    rightCameraProjection_ = rightCameraProjection;\n}\n\nVisualOdometry::~VisualOdometry() {\n}\nvoid VisualOdometry::stereo_callback(const cv::Mat& image_left,\n                     const cv::Mat& image_right) {\n    if (!frame_id) {\n        imageLeftT0_ = image_left;\n        imageRightT0_ = image_right;\n        frame_id++;\n        return;\n    }\n\n    imageLeftT1_ = image_left;\n    imageRightT1_ = image_right;\n\n    // run the pipeline\n    run();\n}\n\nvoid VisualOdometry::run() {\n    std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n        pointsRightT1;\n    matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                     currentVOFeatures, pointsLeftT0, pointsRightT0,\n                     pointsLeftT1, pointsRightT1);\n\n    // set new images as old images\n    imageLeftT0_ = imageLeftT1_;\n    imageRightT0_ = imageRightT1_;\n\n    if (currentVOFeatures.size() < 5)   // TODO should this be AND?\n    {\n        std::cout << \"not enough features matched for pose estimation\"\n                  << std::endl;\n        frame_id++;\n        return;\n    }\n\n    // ---------------------\n    // Triangulate 3D Points\n    // ---------------------\n    cv::Mat points3D_T0, points4D_T0;\n    cv::triangulatePoints(leftCameraProjection_, rightCameraProjection_,\n                          pointsLeftT0, pointsRightT0, points4D_T0);\n    cv::convertPointsFromHomogeneous(points4D_T0.t(), points3D_T0);\n\n    // ---------------------\n    // Tracking transfomation\n    // ---------------------\n    // PnP: computes rotation and translation between pair of images\n    trackingFrame2Frame(leftCameraProjection_, rightCameraProjection_,\n                        pointsLeftT1, points3D_T0, rotation, translation,\n                        false);\n\n    // ------------------------------------------------\n    // Integrating\n    // ------------------------------------------------\n    cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n    if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n        abs(rotation_euler[2]) < 0.1) {\n        integrateOdometryStereo(frame_id, frame_pose, rotation, translation);\n\n    } else {\n        std::cout << \"Too large rotation\" << std::endl;\n    }\n    cv::Mat xyz = frame_pose.col(3).clone();\n    cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n\n    // publish\n    // if (true) {\n    //     std::cout << xyz << std::endl;\n    //     static tf::TransformBroadcaster br;\n\n    //     tf::Transform transform;\n    //     transform.setOrigin(tf::Vector3(xyz.at<double>(0), xyz.at<double>(1),\n    //                                     xyz.at<double>(2)));\n    //     tf::Quaternion q;\n    //     tf::Matrix3x3 R_tf(\n    //         R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2),\n    //         R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2),\n    //         R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2));\n    //     R_tf.getRotation(q);\n    //     transform.setRotation(q);\n    //     br.sendTransform(tf::StampedTransform(transform, ros::Time::now(),\n    //                                           \"odom\", \"camera\"));\n\n    //     transform.setOrigin(tf::Vector3(0.0, 0.0, 0.0));\n    //     tf::Quaternion q2(0.5, -0.5, 0.5, -0.5);\n    //     transform.setRotation(q2);\n    //     br.sendTransform(\n    //         tf::StampedTransform(transform, ros::Time::now(), \"map\", \"odom\"));\n    // }\n    frame_id++;\n}\n\n// --------------------------------\n// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n// --------------------------------\n\n\nvoid featureDetectionFast(cv::Mat image, std::vector<cv::Point2f>& points) {\n    // uses FAST as for feature dection, modify parameters as necessary\n    std::vector<cv::KeyPoint> keypoints;\n    int fast_threshold = 20;\n    bool nonmaxSuppression = true;\n    cv::FAST(image, keypoints, fast_threshold, nonmaxSuppression);\n    cv::KeyPoint::convert(keypoints, points, std::vector<int>());\n}\n\nvoid deleteUnmatchFeaturesCircle(\n    std::vector<cv::Point2f>& points0, std::vector<cv::Point2f>& points1,\n    std::vector<cv::Point2f>& points2, std::vector<cv::Point2f>& points3,\n    std::vector<cv::Point2f>& points0_return, std::vector<uchar>& status0,\n    std::vector<uchar>& status1, std::vector<uchar>& status2,\n    std::vector<uchar>& status3, std::vector<int>& ages) {\n    // getting rid of points for which the KLT tracking failed or those who have\n    // gone outside the frame\n    for (int i = 0; i < ages.size(); ++i) {\n        ages[i] += 1;\n    }\n\n    int indexCorrection = 0;\n    for (int i = 0; i < status3.size(); i++) {\n        cv::Point2f pt0 = points0.at(i - indexCorrection);\n        cv::Point2f pt1 = points1.at(i - indexCorrection);\n        cv::Point2f pt2 = points2.at(i - indexCorrection);\n        cv::Point2f pt3 = points3.at(i - indexCorrection);\n        cv::Point2f pt0_r = points0_return.at(i - indexCorrection);\n\n        if ((status3.at(i) == 0) || (pt3.x < 0) || (pt3.y < 0) ||\n            (status2.at(i) == 0) || (pt2.x < 0) || (pt2.y < 0) ||\n            (status1.at(i) == 0) || (pt1.x < 0) || (pt1.y < 0) ||\n            (status0.at(i) == 0) || (pt0.x < 0) || (pt0.y < 0)) {\n            if ((pt0.x < 0) || (pt0.y < 0) || (pt1.x < 0) || (pt1.y < 0) ||\n                (pt2.x < 0) || (pt2.y < 0) || (pt3.x < 0) || (pt3.y < 0)) {\n                status3.at(i) = 0;\n            }\n            points0.erase(points0.begin() + (i - indexCorrection));\n            points1.erase(points1.begin() + (i - indexCorrection));\n            points2.erase(points2.begin() + (i - indexCorrection));\n            points3.erase(points3.begin() + (i - indexCorrection));\n            points0_return.erase(points0_return.begin() +\n                                 (i - indexCorrection));\n\n            ages.erase(ages.begin() + (i - indexCorrection));\n            indexCorrection++;\n        }\n    }\n}\n\nvoid circularMatching(cv::Mat img_l_0, cv::Mat img_r_0, cv::Mat img_l_1,\n                      cv::Mat img_r_1, std::vector<cv::Point2f>& points_l_0,\n                      std::vector<cv::Point2f>& points_r_0,\n                      std::vector<cv::Point2f>& points_l_1,\n                      std::vector<cv::Point2f>& points_r_1,\n                      std::vector<cv::Point2f>& points_l_0_return,\n                      FeatureSet& current_features) {\n    std::vector<float> err;\n\n    cv::Size winSize =\n        cv::Size(20, 20);   // Lucas-Kanade optical flow window size\n    cv::TermCriteria termcrit = cv::TermCriteria(\n        cv::TermCriteria::COUNT + cv::TermCriteria::EPS, 30, 0.01);\n\n    std::vector<uchar> status0;\n    std::vector<uchar> status1;\n    std::vector<uchar> status2;\n    std::vector<uchar> status3;\n\n    // sparse iterative version of the Lucas-Kanade optical flow in pyramids\n    calcOpticalFlowPyrLK(img_l_0, img_r_0, points_l_0, points_r_0, status0, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_r_0, img_r_1, points_r_0, points_r_1, status1, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_r_1, img_l_1, points_r_1, points_l_1, status2, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_l_1, img_l_0, points_l_1, points_l_0_return,\n                         status3, err, winSize, 3, termcrit, 0, 0.001);\n\n    deleteUnmatchFeaturesCircle(points_l_0, points_r_0, points_r_1, points_l_1,\n                                points_l_0_return, status0, status1, status2,\n                                status3, current_features.ages);\n}\n\nvoid bucketingFeatures(cv::Mat& image, FeatureSet& current_features,\n                       int bucket_size, int features_per_bucket) {\n    int image_height = image.rows;\n    int image_width = image.cols;\n    int buckets_nums_height = image_height / bucket_size;\n    int buckets_nums_width = image_width / bucket_size;\n    int buckets_number = buckets_nums_height * buckets_nums_width;\n\n    std::vector<Bucket> Buckets;\n\n    // initialize all the buckets\n    for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n         buckets_idx_height++) {\n        for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n             buckets_idx_width++) {\n            Buckets.push_back(Bucket(features_per_bucket));\n        }\n    }\n\n    // bucket all current features into buckets by their location\n    int buckets_nums_height_idx, buckets_nums_width_idx, buckets_idx;\n    for (int i = 0; i < current_features.points.size(); ++i) {\n        buckets_nums_height_idx = current_features.points[i].y / bucket_size;\n        buckets_nums_width_idx = current_features.points[i].x / bucket_size;\n        buckets_idx = buckets_nums_height_idx * buckets_nums_width +\n                      buckets_nums_width_idx;\n        Buckets[buckets_idx].add_feature(current_features.points[i],\n                                         current_features.ages[i]);\n    }\n\n    // get features back from buckets\n    current_features.clear();\n    for (int buckets_idx_height = 0; buckets_idx_height <= buckets_nums_height;\n         buckets_idx_height++) {\n        for (int buckets_idx_width = 0; buckets_idx_width <= buckets_nums_width;\n             buckets_idx_width++) {\n            buckets_idx =\n                buckets_idx_height * buckets_nums_width + buckets_idx_width;\n            Buckets[buckets_idx].get_features(current_features);\n        }\n    }\n}\n\nvoid appendNewFeatures(cv::Mat& image, FeatureSet& current_features) {\n    std::vector<cv::Point2f> points_new;\n    featureDetectionFast(image, points_new);\n    current_features.points.insert(current_features.points.end(),\n                                   points_new.begin(), points_new.end());\n    std::vector<int> ages_new(points_new.size(), 0);\n    current_features.ages.insert(current_features.ages.end(), ages_new.begin(),\n                                 ages_new.end());\n}\n\n// --------------------------------\n// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n// --------------------------------\n\nvoid integrateOdometryStereo(int frame_i, cv::Mat& frame_pose,\n                             const cv::Mat& rotation,\n                             const cv::Mat& translation_stereo) {\n    // std::cout << \"rotation\" << rotation << std::endl;\n    // std::cout << \"translation_stereo\" << translation_stereo << std::endl;\n\n    cv::Mat rigid_body_transformation;\n\n    cv::Mat addup = (cv::Mat_<double>(1, 4) << 0, 0, 0, 1);\n\n    cv::hconcat(rotation, translation_stereo, rigid_body_transformation);\n    cv::vconcat(rigid_body_transformation, addup, rigid_body_transformation);\n\n    // std::cout << \"rigid_body_transformation\" << rigid_body_transformation <<\n    // std::endl;\n\n    double scale = sqrt((translation_stereo.at<double>(0)) *\n                            (translation_stereo.at<double>(0)) +\n                        (translation_stereo.at<double>(1)) *\n                            (translation_stereo.at<double>(1)) +\n                        (translation_stereo.at<double>(2)) *\n                            (translation_stereo.at<double>(2)));\n\n    // frame_pose = frame_pose * rigid_body_transformation;\n    std::cout << \"scale: \" << scale << std::endl;\n\n    rigid_body_transformation = rigid_body_transformation.inv();\n    // if ((scale>0.1)&&(translation_stereo.at<double>(2) >\n    // translation_stereo.at<double>(0)) && (translation_stereo.at<double>(2) >\n    // translation_stereo.at<double>(1)))\n    /// if (scale > 0.05 && scale < 10)\n    if (scale > 0.001 && scale < 10)   // WHY DO WE NEED THIS\n    {\n        // std::cout << \"Rpose\" << Rpose << std::endl;\n\n        frame_pose = frame_pose * rigid_body_transformation;\n\n    } else {\n        std::cout << \"[WARNING] scale below 0.1, or incorrect translation\"\n                  << std::endl;\n    }\n}\n\nbool isRotationMatrix(cv::Mat& R) {\n    cv::Mat Rt;\n    transpose(R, Rt);\n    cv::Mat shouldBeIdentity = Rt * R;\n    cv::Mat I = cv::Mat::eye(3, 3, shouldBeIdentity.type());\n\n    return norm(I, shouldBeIdentity) < 1e-6;\n}\n\n// Calculates rotation matrix to euler angles\n// The result is the same as MATLAB except the order\n// of the euler angles ( x and z are swapped ).\ncv::Vec3f rotationMatrixToEulerAngles(cv::Mat& R) {\n    assert(isRotationMatrix(R));\n\n    float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                    R.at<double>(1, 0) * R.at<double>(1, 0));\n\n    bool singular = sy < 1e-6;   // If\n\n    float x, y, z;\n    if (!singular) {\n        x = atan2(R.at<double>(2, 1), R.at<double>(2, 2));\n        y = atan2(-R.at<double>(2, 0), sy);\n        z = atan2(R.at<double>(1, 0), R.at<double>(0, 0));\n    } else {\n        x = atan2(-R.at<double>(1, 2), R.at<double>(1, 1));\n        y = atan2(-R.at<double>(2, 0), sy);\n        z = 0;\n    }\n    return cv::Vec3f(x, y, z);\n}\n\n// --------------------------------\n// https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n// --------------------------------\n\nvoid checkValidMatch(std::vector<cv::Point2f>& points,\n                     std::vector<cv::Point2f>& points_return,\n                     std::vector<bool>& status, int threshold) {\n    int offset;\n    for (int i = 0; i < points.size(); i++) {\n        offset = std::max(std::abs(points[i].x - points_return[i].x),\n                          std::abs(points[i].y - points_return[i].y));\n        if (offset > threshold) {\n            status.push_back(false);\n        } else {\n            status.push_back(true);\n        }\n    }\n}\n\nvoid removeInvalidPoints(std::vector<cv::Point2f>& points,\n                         const std::vector<bool>& status) {\n    int index = 0;\n    for (int i = 0; i < status.size(); i++) {\n        if (status[i] == false) {\n            points.erase(points.begin() + index);\n        } else {\n            index++;\n        }\n    }\n}\n\nvoid trackingFrame2Frame(cv::Mat& leftCameraProjection_,\n                         cv::Mat& rightCameraProjection_,\n                         std::vector<cv::Point2f>& pointsLeftT1,\n                         cv::Mat& points3D_T0, cv::Mat& rotation,\n                         cv::Mat& translation, bool mono_rotation) {\n    // Calculate frame to frame transformation\n    cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n    cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n    cv::Mat intrinsic_matrix =\n        (cv::Mat_<float>(3, 3) << leftCameraProjection_.at<float>(0, 0),\n         leftCameraProjection_.at<float>(0, 1),\n         leftCameraProjection_.at<float>(0, 2),\n         leftCameraProjection_.at<float>(1, 0),\n         leftCameraProjection_.at<float>(1, 1),\n         leftCameraProjection_.at<float>(1, 2),\n         leftCameraProjection_.at<float>(1, 1),\n         leftCameraProjection_.at<float>(1, 2),\n         leftCameraProjection_.at<float>(1, 3));\n\n    int iterationsCount = 500;   // number of Ransac iterations.\n    float reprojectionError =\n        .5;   // maximum allowed distance to consider it an inlier.\n    float confidence = 0.999;   // RANSAC successful confidence.\n    bool useExtrinsicGuess = true;\n    int flags = cv::SOLVEPNP_ITERATIVE;\n\n    cv::Mat inliers;\n    cv::solvePnPRansac(points3D_T0, pointsLeftT1, intrinsic_matrix, distCoeffs,\n                       rvec, translation, useExtrinsicGuess, iterationsCount,\n                       reprojectionError, confidence, inliers, flags);\n\n    cv::Rodrigues(rvec, rotation);\n}\n\nvoid matchingFeatures(cv::Mat& imageLeft_t0, cv::Mat& imageRight_t0,\n                      cv::Mat& imageLeft_t1, cv::Mat& imageRight_t1,\n                      FeatureSet& currentVOFeatures,\n                      std::vector<cv::Point2f>& pointsLeftT0,\n                      std::vector<cv::Point2f>& pointsRightT0,\n                      std::vector<cv::Point2f>& pointsLeftT1,\n                      std::vector<cv::Point2f>& pointsRightT1) {\n    // ----------------------------\n    // Feature detection using FAST\n    // ----------------------------\n    std::vector<cv::Point2f>\n        pointsLeftReturn_t0;   // feature points to check cicular mathcing\n                               // validation\n\n    // add new features if current number of features is below a threshold. TODO\n    // PARAM\n    if (currentVOFeatures.size() < 2000) {\n        // append new features with old features\n        appendNewFeatures(imageLeft_t0, currentVOFeatures);\n    }\n\n    // --------------------------------------------------------\n    // Feature tracking using KLT tracker, bucketing and circular matching\n    // --------------------------------------------------------\n    int bucket_size =\n        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / 10;   // TODO PARAM\n    int features_per_bucket = 1;                               // TODO PARAM\n    std::cout << \"number of features before bucketing: \"\n              << currentVOFeatures.points.size() << std::endl;\n\n    // filter features in currentVOFeatures so that one per bucket\n    bucketingFeatures(imageLeft_t0, currentVOFeatures, bucket_size,\n                      features_per_bucket);\n    pointsLeftT0 = currentVOFeatures.points;\n\n    circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                     pointsLeftT0, pointsRightT0, pointsLeftT1,\n                     pointsRightT1, pointsLeftReturn_t0, currentVOFeatures);\n\n    // check if circled back points are in range of original points\n    std::vector<bool> status;\n    checkValidMatch(pointsLeftT0, pointsLeftReturn_t0, status, 0);\n    removeInvalidPoints(pointsLeftT0,\n                        status);   // can combine into one function\n    removeInvalidPoints(pointsLeftT1, status);\n    removeInvalidPoints(pointsRightT0, status);\n    removeInvalidPoints(pointsRightT1, status);\n\n    // update current tracked points\n    currentVOFeatures.points = pointsLeftT1;\n}\n}   // namespace visual_odometry\n"
        }
    ]
}
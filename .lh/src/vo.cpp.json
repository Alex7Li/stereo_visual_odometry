{
    "sourceFile": "src/vo.cpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 21,
            "patches": [
                {
                    "date": 1647376570930,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1647468795123,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n  * @authors \tBen Kolligs, Alex Li\n  * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n  *\n  ****************************************************************/\n-#include \"vo.h\"\n+#include \"../include/vo.h\"\n \n namespace visual_odometry {\n VisualOdometry::VisualOdometry(const cv::Mat leftCameraProjection,\n                                const cv::Mat rightCameraProjection) {\n"
                },
                {
                    "date": 1647468801805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n  * @authors \tBen Kolligs, Alex Li\n  * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n  *\n  ****************************************************************/\n-#include \"../include/vo.h\"\n+#include \"vo.h\"\n \n namespace visual_odometry {\n VisualOdometry::VisualOdometry(const cv::Mat leftCameraProjection,\n                                const cv::Mat rightCameraProjection) {\n"
                },
                {
                    "date": 1647469573930,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n  * @authors \tBen Kolligs, Alex Li\n  * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n  *\n  ****************************************************************/\n-#include \"vo.h\"\n+#include \"../include/vo.h\"\n \n namespace visual_odometry {\n VisualOdometry::VisualOdometry(const cv::Mat leftCameraProjection,\n                                const cv::Mat rightCameraProjection) {\n"
                },
                {
                    "date": 1647543002150,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,10 +125,10 @@\n       cv::Point2f pt1 = points1.at(i - indexCorrection);\n       cv::Point2f pt2 = points2.at(i - indexCorrection);\n       cv::Point2f pt3 = points3.at(i - indexCorrection);\n       cv::Point2f pt4 = points4.at(i - indexCorrection);\n-      // // no need to check bounds for pt4 since it's equal to pt0 at\n-      // // all valid locations\n+      // no need to check bounds for pt4 since it's equal to pt0 at\n+      // all valid locations\n       if ((status_all.at(i) == 0)) {\n         points0.erase(points0.begin() + (i - indexCorrection));\n         points1.erase(points1.begin() + (i - indexCorrection));\n         points2.erase(points2.begin() + (i - indexCorrection));\n"
                },
                {
                    "date": 1647545963075,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n \n   void deleteFeaturesWithFailureStatus(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points4, std::vector<int> & ages,\n+      std::vector<cv::Point2f> & points4, FeatureSet& current_Features,\n       const std::vector<bool> & status_all) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     int indexCorrection = 0;\n"
                },
                {
                    "date": 1647545997433,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n \n   void deleteFeaturesWithFailureStatus(\n       std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n       std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n-      std::vector<cv::Point2f> & points4, FeatureSet& current_Features,\n+      std::vector<cv::Point2f> & points4, FeatureSet& current_features,\n       const std::vector<bool> & status_all) {\n     // getting rid of points for which the KLT tracking failed or those who have\n     // gone outside the frame\n     int indexCorrection = 0;\n@@ -134,9 +134,10 @@\n         points2.erase(points2.begin() + (i - indexCorrection));\n         points3.erase(points3.begin() + (i - indexCorrection));\n         points4.erase(points4.begin() + (i - indexCorrection));\n \n-        ages.erase(ages.begin() + (i - indexCorrection));\n+        current_features.ages.erase(current_features.ages.begin() + (i - indexCorrection));\n+        current_features.strengths.erase(current_features.strengths.begin() + (i - indexCorrection));\n         indexCorrection++;\n       }\n     }\n   }\n"
                },
                {
                    "date": 1647546395382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -330,8 +330,10 @@\n \n \n     // Check if circled back points are in range of original points.\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, 0);\n+    \n+    // Only keep points that were matched correctly and are in the image bounds\n     for(int i = 0; i < status.size(); i++) {\n       if(!status[i] || !matchingStatus[i] ||\n           (pointsLeftT0[i].x < 0) || (pointsLeftT0[i].y < 0) ||\n               (pointsLeftT0[i].x >= imageLeft_t0.rows) || (pointsLeftT0[i].y >= imageLeft_t0.cols) ||\n@@ -347,9 +349,9 @@\n     }\n \n     deleteFeaturesWithFailureStatus(\n         pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1, pointsLeftReturn_t0,\n-        currentVOFeatures.ages, status);\n+        currentVOFeatures, status);\n \n     for (int i = 0; i < currentVOFeatures.ages.size(); ++i) {\n       currentVOFeatures.ages[i] += 1;\n     }\n"
                },
                {
                    "date": 1647546518088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,8 +168,12 @@\n     calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err,\n                          winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_2, img_0, points_2, points_0_return,\n                          status3, err, winSize, 3, termcrit, 0, 0.001);\n+    calcOpticalFlowPyrLK(img_0, img_1, points_0, points_1, status0, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_1, img_3, points_1, points_3, status1, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_2, img_0, points_1, points_0_return, status3, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n     if (status3.size() != status0.size() or points_0.size() != points_0_return.size()) {\n       std::cerr << \"Size of returned points was not correct!!\\n\";\n     }\n     std::vector<uchar> status_all;\n@@ -331,9 +335,9 @@\n \n     // Check if circled back points are in range of original points.\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, 0);\n     \n-    // Only keep points that were matched correctly and are in the image bounds\n+    // Only keep points that were matched correctly and are in the image bounds.\n     for(int i = 0; i < status.size(); i++) {\n       if(!status[i] || !matchingStatus[i] ||\n           (pointsLeftT0[i].x < 0) || (pointsLeftT0[i].y < 0) ||\n               (pointsLeftT0[i].x >= imageLeft_t0.rows) || (pointsLeftT0[i].y >= imageLeft_t0.cols) ||\n"
                },
                {
                    "date": 1647546534583,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,16 +160,8 @@\n     std::vector<uchar> status2;\n     std::vector<uchar> status3;\n \n     // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n-    calcOpticalFlowPyrLK(img_0, img_1, points_0, points_1, status0, err,\n-                         winSize, 3, termcrit, 0, 0.001);\n-    calcOpticalFlowPyrLK(img_1, img_3, points_1, points_3, status1, err,\n-                         winSize, 3, termcrit, 0, 0.001);\n-    calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err,\n-                         winSize, 3, termcrit, 0, 0.001);\n-    calcOpticalFlowPyrLK(img_2, img_0, points_2, points_0_return,\n-                         status3, err, winSize, 3, termcrit, 0, 0.001);\n     calcOpticalFlowPyrLK(img_0, img_1, points_0, points_1, status0, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n     calcOpticalFlowPyrLK(img_1, img_3, points_1, points_3, status1, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n     calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n     calcOpticalFlowPyrLK(img_2, img_0, points_1, points_0_return, status3, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n"
                },
                {
                    "date": 1647546606285,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,11 +161,11 @@\n     std::vector<uchar> status3;\n \n     // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n     calcOpticalFlowPyrLK(img_0, img_1, points_0, points_1, status0, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n-    calcOpticalFlowPyrLK(img_1, img_3, points_1, points_3, status1, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n-    calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n-    calcOpticalFlowPyrLK(img_2, img_0, points_1, points_0_return, status3, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_1, img_2, points_1, points_2, status1, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_2, img_3, points_2, points_3, status2, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n+    calcOpticalFlowPyrLK(img_3, img_0, points_1, points_0_return, status3, err, winSize, 3, termcrit, cv::OPTFLOW_LK_GET_MIN_EIGENVALS, 0.01);\n     if (status3.size() != status0.size() or points_0.size() != points_0_return.size()) {\n       std::cerr << \"Size of returned points was not correct!!\\n\";\n     }\n     std::vector<uchar> status_all;\n@@ -318,10 +318,10 @@\n                       features_per_bucket);\n \n     pointsLeftT0 = currentVOFeatures.points;\n \n-    std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n-                     pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n+    std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n+                     pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, \n                      pointsLeftReturn_t0);\n \n \n \n"
                },
                {
                    "date": 1647546711651,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -329,9 +329,9 @@\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, 0);\n     \n     // Only keep points that were matched correctly and are in the image bounds.\n     for(int i = 0; i < status.size(); i++) {\n-      if(!status[i] || !matchingStatus[i] ||\n+      if(!matchingStatus[i] ||\n           (pointsLeftT0[i].x < 0) || (pointsLeftT0[i].y < 0) ||\n               (pointsLeftT0[i].x >= imageLeft_t0.rows) || (pointsLeftT0[i].y >= imageLeft_t0.cols) ||\n               (pointsLeftT1[i].x < 0) || (pointsLeftT1[i].y < 0) ||\n               (pointsLeftT1[i].x >= imageLeft_t1.rows) || (pointsLeftT1[i].y >= imageLeft_t1.cols) ||\n"
                },
                {
                    "date": 1647546940692,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,23 +301,24 @@\n       std::vector<cv::Point2f> &pointsRightT1) {\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // circular matching validation\n+    if(currentVOFeatures.size() < 4000) {\n+        // Append new features with old features.\n+        currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n-    // Append new features with old features.\n-    currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n+        // --------------------------------------------------------\n+        // Feature tracking using KLT tracker, bucketing and circular matching.\n+        // --------------------------------------------------------\n+        int bucket_size =\n+            std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n+        int features_per_bucket = FEATURES_PER_BUCKET;\n \n-    // --------------------------------------------------------\n-    // Feature tracking using KLT tracker, bucketing and circular matching.\n-    // --------------------------------------------------------\n-    int bucket_size =\n-        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n-    int features_per_bucket = FEATURES_PER_BUCKET;\n+        // Filter features in currentVOFeatures to leave just one per bucket.\n+        currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n+                        features_per_bucket);\n+    }\n \n-    // Filter features in currentVOFeatures to leave just one per bucket.\n-    currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n-                      features_per_bucket);\n-\n     pointsLeftT0 = currentVOFeatures.points;\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n                      pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, \n"
                },
                {
                    "date": 1647547063606,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,20 +305,14 @@\n     if(currentVOFeatures.size() < 4000) {\n         // Append new features with old features.\n         currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n \n-        // --------------------------------------------------------\n-        // Feature tracking using KLT tracker, bucketing and circular matching.\n-        // --------------------------------------------------------\n-        int bucket_size =\n-            std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n-        int features_per_bucket = FEATURES_PER_BUCKET;\n-\n-        // Filter features in currentVOFeatures to leave just one per bucket.\n-        currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n-                        features_per_bucket);\n     }\n \n+    // --------------------------------------------------------\n+    // Feature tracking using KLT tracker, bucketing and circular matching.\n+    // --------------------------------------------------------\n+\n     pointsLeftT0 = currentVOFeatures.points;\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n                      pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, \n"
                },
                {
                    "date": 1647547098012,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -304,9 +304,8 @@\n                                                   // circular matching validation\n     if(currentVOFeatures.size() < 4000) {\n         // Append new features with old features.\n         currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n-\n     }\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching.\n"
                },
                {
                    "date": 1647547113968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -302,9 +302,9 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // circular matching validation\n     if(currentVOFeatures.size() < 4000) {\n-        // Append new features with old features.\n+        // update feature set with features from the image.\n         currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n     }\n \n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1647547120886,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -302,9 +302,9 @@\n     \n     std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                   // circular matching validation\n     if(currentVOFeatures.size() < 4000) {\n-        // update feature set with features from the image.\n+        // update feature set with detected features from the image.\n         currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n     }\n \n     // --------------------------------------------------------\n"
                },
                {
                    "date": 1647547144099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,8 +311,9 @@\n     // Feature tracking using KLT tracker, bucketing and circular matching.\n     // --------------------------------------------------------\n \n     pointsLeftT0 = currentVOFeatures.points;\n+    if (currentVOFeatures.points.size() == 0) return; // early exit\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n                      pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, \n                      pointsLeftReturn_t0);\n"
                },
                {
                    "date": 1647547167005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -314,10 +314,9 @@\n     pointsLeftT0 = currentVOFeatures.points;\n     if (currentVOFeatures.points.size() == 0) return; // early exit\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n-                     pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, \n-                     pointsLeftReturn_t0);\n+                     pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, pointsLeftReturn_t0);\n \n \n \n     // Check if circled back points are in range of original points.\n"
                },
                {
                    "date": 1647547178805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -316,10 +316,8 @@\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n                      pointsLeftT0, pointsRightT0, pointsRightT1, pointsLeftT1, pointsLeftReturn_t0);\n \n-\n-\n     // Check if circled back points are in range of original points.\n     std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, 0);\n     \n     // Only keep points that were matched correctly and are in the image bounds.\n"
                },
                {
                    "date": 1647547203004,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -309,9 +309,8 @@\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching.\n     // --------------------------------------------------------\n-\n     pointsLeftT0 = currentVOFeatures.points;\n     if (currentVOFeatures.points.size() == 0) return; // early exit\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n"
                },
                {
                    "date": 1647547224755,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -309,8 +309,9 @@\n \n     // --------------------------------------------------------\n     // Feature tracking using KLT tracker, bucketing and circular matching.\n     // --------------------------------------------------------\n+\n     pointsLeftT0 = currentVOFeatures.points;\n     if (currentVOFeatures.points.size() == 0) return; // early exit\n \n     std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageRight_t1, imageLeft_t1, \n"
                }
            ],
            "date": 1647376570929,
            "name": "Commit-0",
            "content": "/****************************************************************\n *\n * @file \t\tvo.cpp\n *\n * @brief \t\tThe Visual Odometry class being used for\n translation. The math can be found in Haidar Jamal's Thesis:\n *https://www.ri.cmu.edu/publications/localization-for-lunar-micro-rovers/\n *\n * @version \t1.0\n * @date \t\t02/09/2022\n *\n * @authors \tBen Kolligs, Alex Li\n * @author \t\tCarnegie Mellon University, Planetary Robotics Lab\n *\n ****************************************************************/\n#include \"vo.h\"\n\nnamespace visual_odometry {\nVisualOdometry::VisualOdometry(const cv::Mat leftCameraProjection,\n                               const cv::Mat rightCameraProjection) {\n  leftCameraProjection_ = leftCameraProjection;\n  rightCameraProjection_ = rightCameraProjection;\n}\n\nVisualOdometry::~VisualOdometry() {}\nvoid VisualOdometry::stereo_callback(const cv::Mat &imageLeft,\n                                      const cv::Mat &imageRight) {\n    // Wait until we have at least two time steps of data\n    // to begin predicting the change in pose.\n    if (!frame_id) {\n      imageLeftT0_ = imageLeft;\n      imageRightT0_ = imageRight;\n      frame_id++;\n      return;\n    }\n\n    imageLeftT1_ = imageLeft;\n    imageRightT1_ = imageRight;\n\n    std::vector<cv::Point2f> pointsLeftT0, pointsRightT0, pointsLeftT1,\n        pointsRightT1;\n    \n    matchingFeatures(imageLeftT0_, imageRightT0_, imageLeftT1_, imageRightT1_,\n                     currentVOFeatures, pointsLeftT0, pointsRightT0,\n                     pointsLeftT1, pointsRightT1);\n\n    // Set new images as old images.\n    imageLeftT0_ = imageLeftT1_;\n    imageRightT0_ = imageRightT1_;\n\n    if (currentVOFeatures.size() < 5) {\n      // There are not enough features to fully determine\n      // equations for pose estimation, so presume nothing and exit.\n      frame_id++;\n      return;\n    }\n\n    // ---------------------\n    // Triangulate 3D Points\n    // ---------------------\n    cv::Mat world_points_T0, world_homogenous_points_T0;\n    cv::triangulatePoints(leftCameraProjection_, rightCameraProjection_,\n                          pointsLeftT0, pointsRightT0, world_homogenous_points_T0);\n    cv::convertPointsFromHomogeneous(world_homogenous_points_T0.t(), world_points_T0);\n\n    // ---------------------\n    // Tracking transfomation\n    // ---------------------\n    cameraToWorld(leftCameraProjection_,\n        pointsLeftT1, world_points_T0, rotation, translation);\n\n    // ------------------------------------------------\n    // Integrating\n    // ------------------------------------------------\n    cv::Vec3f rotation_euler = rotationMatrixToEulerAngles(rotation);\n    // Don't perform an update if the output is unusually large, indicates a error elsewhere.\n    if (abs(rotation_euler[1]) < 0.1 && abs(rotation_euler[0]) < 0.1 &&\n        abs(rotation_euler[2]) < 0.1) {\n      integrateOdometryStereo(frame_pose, rotation, translation);\n    }\n    cv::Mat xyz = frame_pose.col(3).clone();\n    cv::Mat R = frame_pose(cv::Rect(0, 0, 3, 3));\n\n    // publish\n    if (true) {\n        static tf::TransformBroadcaster br;\n\n        tf::Transform transform;\n        transform.setOrigin(tf::Vector3(xyz.at<double>(0), xyz.at<double>(1),\n                                        xyz.at<double>(2)));\n        tf::Quaternion q;\n        tf::Matrix3x3 R_tf(\n            R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2),\n            R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2),\n            R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2));\n        R_tf.getRotation(q);\n        transform.setRotation(q);\n        br.sendTransform(tf::StampedTransform(transform, ros::Time::now(),\n                                              \"odom\", \"camera\"));\n\n        transform.setOrigin(tf::Vector3(0.0, 0.0, 0.0));\n        tf::Quaternion q2(0.5, -0.5, 0.5, -0.5);\n        transform.setRotation(q2);\n        br.sendTransform(\n            tf::StampedTransform(transform, ros::Time::now(), \"map\",\n            \"odom\"));\n    }\n    frame_id++;\n  }\n\n  // --------------------------------\n  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/feature.cpp\n  // --------------------------------\n\n  void deleteFeaturesWithFailureStatus(\n      std::vector<cv::Point2f> & points0, std::vector<cv::Point2f> & points1,\n      std::vector<cv::Point2f> & points2, std::vector<cv::Point2f> & points3,\n      std::vector<cv::Point2f> & points4, std::vector<int> & ages,\n      const std::vector<bool> & status_all) {\n    // getting rid of points for which the KLT tracking failed or those who have\n    // gone outside the frame\n    int indexCorrection = 0;\n    for (int i = 0; i < status_all.size(); i++) {\n      cv::Point2f pt0 = points0.at(i - indexCorrection);\n      cv::Point2f pt1 = points1.at(i - indexCorrection);\n      cv::Point2f pt2 = points2.at(i - indexCorrection);\n      cv::Point2f pt3 = points3.at(i - indexCorrection);\n      cv::Point2f pt4 = points4.at(i - indexCorrection);\n      // // no need to check bounds for pt4 since it's equal to pt0 at\n      // // all valid locations\n      if ((status_all.at(i) == 0)) {\n        points0.erase(points0.begin() + (i - indexCorrection));\n        points1.erase(points1.begin() + (i - indexCorrection));\n        points2.erase(points2.begin() + (i - indexCorrection));\n        points3.erase(points3.begin() + (i - indexCorrection));\n        points4.erase(points4.begin() + (i - indexCorrection));\n\n        ages.erase(ages.begin() + (i - indexCorrection));\n        indexCorrection++;\n      }\n    }\n  }\n\n  std::vector<uchar> circularMatching(const cv::Mat img_0, const cv::Mat img_1, const cv::Mat img_2,\n                        const cv::Mat img_3, std::vector<cv::Point2f> & points_0,\n                        std::vector<cv::Point2f> & points_1,\n                        std::vector<cv::Point2f> & points_2,\n                        std::vector<cv::Point2f> & points_3,\n                        std::vector<cv::Point2f> & points_0_return) {\n    std::vector<float> err;\n\n    cv::Size winSize =\n        cv::Size(20, 20); // Lucas-Kanade optical flow window size\n    cv::TermCriteria termcrit = cv::TermCriteria(\n        cv::TermCriteria::COUNT + cv::TermCriteria::EPS, 30, 0.01);\n\n    std::vector<uchar> status0;\n    std::vector<uchar> status1;\n    std::vector<uchar> status2;\n    std::vector<uchar> status3;\n\n    // Sparse iterative version of the Lucas-Kanade optical flow in pyramids.\n    calcOpticalFlowPyrLK(img_0, img_1, points_0, points_1, status0, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_1, img_3, points_1, points_3, status1, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_3, img_2, points_3, points_2, status2, err,\n                         winSize, 3, termcrit, 0, 0.001);\n    calcOpticalFlowPyrLK(img_2, img_0, points_2, points_0_return,\n                         status3, err, winSize, 3, termcrit, 0, 0.001);\n    if (status3.size() != status0.size() or points_0.size() != points_0_return.size()) {\n      std::cerr << \"Size of returned points was not correct!!\\n\";\n    }\n    std::vector<uchar> status_all;\n    for(int i = 0; i < status3.size(); i++) {\n      status_all[i] = status0[i] | status1[i] | status2[i] | status3[i];\n    }\n    return status_all;\n  }\n\n\n  // --------------------------------\n  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/utils.cpp\n  // --------------------------------\n\n  void integrateOdometryStereo(cv::Mat &frame_pose, const cv::Mat &rotation,\n                               const cv::Mat &translation_stereo) {\n    cv::Mat rigid_body_transformation;\n\n    cv::Mat addup = (cv::Mat_<double>(1, 4) << 0, 0, 0, 1);\n\n    cv::hconcat(rotation, translation_stereo, rigid_body_transformation);\n    cv::vconcat(rigid_body_transformation, addup, rigid_body_transformation);\n\n    const double scale = sqrt((translation_stereo.at<double>(0)) *\n                            (translation_stereo.at<double>(0)) +\n                        (translation_stereo.at<double>(1)) *\n                            (translation_stereo.at<double>(1)) +\n                        (translation_stereo.at<double>(2)) *\n                            (translation_stereo.at<double>(2)));\n\n    rigid_body_transformation = rigid_body_transformation.inv();\n    if (scale > 0.001 && scale < 10) // WHY DO WE NEED THIS\n    {\n      frame_pose = frame_pose * rigid_body_transformation;\n    } else {\n      std::cout << \"[WARNING] scale below 0.1, or incorrect translation\"\n                << std::endl;\n    }\n  }\n\n  // Calculates rotation matrix to euler angles\n  // The result is the same as MATLAB except the order\n  // of the euler angles ( x and z are swapped ).\n  cv::Vec3f rotationMatrixToEulerAngles(const cv::Mat & R) {\n    float sy = sqrt(R.at<double>(0, 0) * R.at<double>(0, 0) +\n                    R.at<double>(1, 0) * R.at<double>(1, 0));\n\n    bool singular = sy < 1e-6;\n\n    float x, y, z;\n    if (!singular) {\n      x = atan2(R.at<double>(2, 1), R.at<double>(2, 2));\n      y = atan2(-R.at<double>(2, 0), sy);\n      z = atan2(R.at<double>(1, 0), R.at<double>(0, 0));\n    } else {\n      x = atan2(-R.at<double>(1, 2), R.at<double>(1, 1));\n      y = atan2(-R.at<double>(2, 0), sy);\n      z = 0;\n    }\n    return cv::Vec3f(x, y, z);\n  }\n\n  // --------------------------------\n  // https://github.com/hjamal3/stereo_visual_odometry/blob/main/src/visualOdometry.cpp\n  // --------------------------------\n\n  std::vector<bool> findUnmovedPoints(const std::vector<cv::Point2f> & points_1,\n                       const std::vector<cv::Point2f> & points_2,\n                       const int threshold) {\n    std::vector<bool> status;\n    int offset;\n    for (int i = 0; i < points_1.size(); i++) {\n      offset = std::max(std::abs(points_1[i].x - points_2[i].x),\n                        std::abs(points_1[i].y - points_2[i].y));\n      if (offset > threshold) {\n        status.push_back(false);\n      } else {\n        status.push_back(true);\n      }\n    }\n    return status;\n  }\n\n  void removeInvalidPoints(std::vector<cv::Point2f> & points,\n                           const std::vector<bool> &status) {\n    int index = 0;\n    for (int i = 0; i < status.size(); i++) {\n      if (status[i] == false) {\n        points.erase(points.begin() + index);\n      } else {\n        index++;\n      }\n    }\n  }\n  void cameraToWorld(\n      const cv::Mat & cameraProjection,\n      const std::vector<cv::Point2f> & cameraPoints, const cv::Mat & worldPoints,\n      cv::Mat & rotation, cv::Mat & translation) {\n    // Calculate frame to frame transformation\n    cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);\n    cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n    cv::Mat intrinsic_matrix =\n        (cv::Mat_<float>(3, 3) << cameraProjection.at<float>(0, 0),\n         cameraProjection.at<float>(0, 1),\n         cameraProjection.at<float>(0, 2),\n         cameraProjection.at<float>(1, 0),\n         cameraProjection.at<float>(1, 1),\n         cameraProjection.at<float>(1, 2),\n         cameraProjection.at<float>(1, 1),\n         cameraProjection.at<float>(1, 2),\n         cameraProjection.at<float>(1, 3));\n\n    int iterationsCount = 500; // number of Ransac iterations.\n    float reprojectionError = .5; // maximum allowed distance to consider it an inlier.\n    float confidence = 0.999; // RANSAC successful confidence.\n    bool useExtrinsicGuess = true;\n    int flags = cv::SOLVEPNP_ITERATIVE;\n\n    cv::Mat inliers;\n    cv::solvePnPRansac(worldPoints, cameraPoints, intrinsic_matrix, distCoeffs,\n                       rvec, translation, useExtrinsicGuess, iterationsCount,\n                       reprojectionError, confidence, inliers, flags);\n\n    cv::Rodrigues(rvec, rotation);\n  }\n\n  void matchingFeatures(\n      const cv::Mat &imageLeft_t0, const cv::Mat &imageRight_t0,\n      const cv::Mat &imageLeft_t1, const cv::Mat &imageRight_t1,\n      FeatureSet &currentVOFeatures, std::vector<cv::Point2f> &pointsLeftT0,\n      std::vector<cv::Point2f> &pointsRightT0,\n      std::vector<cv::Point2f> &pointsLeftT1,\n      std::vector<cv::Point2f> &pointsRightT1) {\n    \n    std::vector<cv::Point2f> pointsLeftReturn_t0; // feature points to check\n                                                  // circular matching validation\n\n    // Append new features with old features.\n    currentVOFeatures.appendFeaturesFromImage(imageLeft_t0);\n\n    // --------------------------------------------------------\n    // Feature tracking using KLT tracker, bucketing and circular matching.\n    // --------------------------------------------------------\n    int bucket_size =\n        std::min(imageLeft_t0.rows, imageLeft_t0.cols) / BUCKETS_PER_AXIS;\n    int features_per_bucket = FEATURES_PER_BUCKET;\n\n    // Filter features in currentVOFeatures to leave just one per bucket.\n    currentVOFeatures.filterByBucketLocation(imageLeft_t0, bucket_size,\n                      features_per_bucket);\n\n    pointsLeftT0 = currentVOFeatures.points;\n\n    std::vector<uchar> matchingStatus = circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                     pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1,\n                     pointsLeftReturn_t0);\n\n\n\n    // Check if circled back points are in range of original points.\n    std::vector<bool> status = findUnmovedPoints(pointsLeftT0, pointsLeftReturn_t0, 0);\n    for(int i = 0; i < status.size(); i++) {\n      if(!status[i] || !matchingStatus[i] ||\n          (pointsLeftT0[i].x < 0) || (pointsLeftT0[i].y < 0) ||\n              (pointsLeftT0[i].x >= imageLeft_t0.rows) || (pointsLeftT0[i].y >= imageLeft_t0.cols) ||\n              (pointsLeftT1[i].x < 0) || (pointsLeftT1[i].y < 0) ||\n              (pointsLeftT1[i].x >= imageLeft_t1.rows) || (pointsLeftT1[i].y >= imageLeft_t1.cols) ||\n              (pointsRightT0[i].x < 0) || (pointsRightT0[i].y < 0) ||\n              (pointsRightT0[i].x >= imageRight_t0.rows) || (pointsRightT0[i].y >= imageRight_t0.cols) ||\n              (pointsRightT1[i].x < 0) || (pointsRightT1[i].y < 0) ||\n              (pointsRightT1[i].x >= imageRight_t1.rows) || (pointsRightT1[i].y >= imageRight_t1.cols)\n              ) {\n          status[i] = false;\n      }\n    }\n\n    deleteFeaturesWithFailureStatus(\n        pointsLeftT0, pointsRightT0, pointsLeftT1, pointsRightT1, pointsLeftReturn_t0,\n        currentVOFeatures.ages, status);\n\n    for (int i = 0; i < currentVOFeatures.ages.size(); ++i) {\n      currentVOFeatures.ages[i] += 1;\n    }\n\n    // Update current tracked points.\n    currentVOFeatures.points = pointsLeftT1;\n  }\n} // namespace visual_odometry\n"
        }
    ]
}
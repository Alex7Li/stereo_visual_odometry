{
    "sourceFile": "src/visualOdometry.cpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1644160154547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1644160165937,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,10 @@\n #include \"stereo_visual_odometry/visualOdometry.h\"\n using namespace cv;\n \n+// Alex: How does a vector transform into a rotation matrix??\n cv::Mat euler2rot(cv::Mat& rotationMatrix, const cv::Mat & euler)\n {\n-    // How does a vector transform into a rotation matrix??\n \n     double x = euler.at<double>(0);\n     double y = euler.at<double>(1);\n     double z = euler.at<double>(2);\n"
                },
                {
                    "date": 1644160646983,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n #include \"stereo_visual_odometry/visualOdometry.h\"\n using namespace cv;\n \n // Alex: How does a vector transform into a rotation matrix??\n+// actually this function is never used\n cv::Mat euler2rot(cv::Mat& rotationMatrix, const cv::Mat & euler)\n {\n \n     double x = euler.at<double>(0);\n"
                }
            ],
            "date": 1644160154547,
            "name": "Commit-0",
            "content": "#include \"stereo_visual_odometry/visualOdometry.h\"\nusing namespace cv;\n\ncv::Mat euler2rot(cv::Mat& rotationMatrix, const cv::Mat & euler)\n{\n    // How does a vector transform into a rotation matrix??\n\n    double x = euler.at<double>(0);\n    double y = euler.at<double>(1);\n    double z = euler.at<double>(2);\n\n    // Assuming the angles are in radians.\n    double ch = cos(z);\n    double sh = sin(z);\n    double ca = cos(y);\n    double sa = sin(y);\n    double cb = cos(x);\n    double sb = sin(x);\n\n    double m00, m01, m02, m10, m11, m12, m20, m21, m22;\n\n    m00 = ch * ca;\n    m01 = sh*sb - ch*sa*cb;\n    m02 = ch*sa*sb + sh*cb;\n    m10 = sa;\n    m11 = ca*cb;\n    m12 = -ca*sb;\n    m20 = -sh*ca;\n    m21 = sh*sa*cb + ch*sb;\n    m22 = -sh*sa*sb + ch*cb;\n\n    rotationMatrix.at<double>(0,0) = m00;\n    rotationMatrix.at<double>(0,1) = m01;\n    rotationMatrix.at<double>(0,2) = m02;\n    rotationMatrix.at<double>(1,0) = m10;\n    rotationMatrix.at<double>(1,1) = m11;\n    rotationMatrix.at<double>(1,2) = m12;\n    rotationMatrix.at<double>(2,0) = m20;\n    rotationMatrix.at<double>(2,1) = m21;\n    rotationMatrix.at<double>(2,2) = m22;\n\n    return rotationMatrix;\n}\n\nvoid checkValidMatch(std::vector<cv::Point2f>& points, std::vector<cv::Point2f>& points_return, std::vector<bool>& status, int threshold)\n{\n    int offset;\n    for (int i = 0; i < points.size(); i++)\n    {\n        offset = std::max(std::abs(points[i].x - points_return[i].x), std::abs(points[i].y - points_return[i].y));\n        // std::cout << offset << \", \";\n\n        if(offset > threshold)\n        {\n            status.push_back(false);\n        }\n        else\n        {\n            status.push_back(true);\n        }\n    }\n}\n\nvoid removeInvalidPoints(std::vector<cv::Point2f>& points, const std::vector<bool>& status)\n{\n    int index = 0;\n    for (int i = 0; i < status.size(); i++)\n    {\n        if (status[i] == false)\n        {\n            points.erase(points.begin() + index);\n        }\n        else\n        {\n            index ++;\n        }\n    }\n}\n\nvoid matchingFeatures(cv::Mat& imageLeft_t0, cv::Mat& imageRight_t0,\n                      cv::Mat& imageLeft_t1, cv::Mat& imageRight_t1, \n                      FeatureSet& currentVOFeatures,\n                      std::vector<cv::Point2f>&  pointsLeft_t0, \n                      std::vector<cv::Point2f>&  pointsRight_t0, \n                      std::vector<cv::Point2f>&  pointsLeft_t1, \n                      std::vector<cv::Point2f>&  pointsRight_t1)\n{\n    // ----------------------------\n    // Feature detection using FAST\n    // ----------------------------\n    std::vector<cv::Point2f>  pointsLeftReturn_t0;   // feature points to check cicular mathcing validation\n\n    // add new features if current number of features is below a threshold. TODO PARAM\n    if (currentVOFeatures.size() < 2000)\n    {\n\n        // append new features with old features\n        appendNewFeatures(imageLeft_t0, currentVOFeatures);   \n        //std::cout << \"Current feature set size: \" << currentVOFeatures.points.size() << std::endl;\n    }\n\n    // --------------------------------------------------------\n    // Feature tracking using KLT tracker, bucketing and circular matching\n    // --------------------------------------------------------\n    int bucket_size = std::min(imageLeft_t0.rows,imageLeft_t0.cols)/10; // TODO PARAM\n    int features_per_bucket = 1; // TODO PARAM\n    std::cout << \"number of features before bucketing: \" << currentVOFeatures.points.size() << std::endl;\n\n    // feature detector points before bucketing\n    //displayPoints(imageLeft_t0,currentVOFeatures.points);\n\n    // filter features in currentVOFeatures so that one per bucket\n    bucketingFeatures(imageLeft_t0, currentVOFeatures, bucket_size, features_per_bucket);\n    pointsLeft_t0 = currentVOFeatures.points;\n\n    // feature detector points after bucketing\n    //displayPoints(imageLeft_t0,currentVOFeatures.points);\n\n    #if USE_CUDA\n        circularMatching_gpu(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                     pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1, pointsLeftReturn_t0, currentVOFeatures);\n    #else\n\t    circularMatching(imageLeft_t0, imageRight_t0, imageLeft_t1, imageRight_t1,\n                     pointsLeft_t0, pointsRight_t0, pointsLeft_t1, pointsRight_t1, pointsLeftReturn_t0, currentVOFeatures);\n    #endif\n\n    // check if circled back points are in range of original points\n    std::vector<bool> status;\n    checkValidMatch(pointsLeft_t0, pointsLeftReturn_t0, status, 0);\n    removeInvalidPoints(pointsLeft_t0, status); // can combine into one function\n    removeInvalidPoints(pointsLeft_t1, status);\n    removeInvalidPoints(pointsRight_t0, status);\n    removeInvalidPoints(pointsRight_t1, status);\n\n    std::cout << \"number of features after bucketing: \" << currentVOFeatures.points.size() << std::endl;\n\n    // update current tracked points\n    currentVOFeatures.points = pointsLeft_t1;\n\n    std::cout << \"number of features after circular matching: \" << currentVOFeatures.points.size() << std::endl;\n\n    // feature detector points after circular matching\n    //displayPoints(imageLeft_t0,currentVOFeatures.points);\n\n}\n\n// The transformation is parameterized using 6 parameters: 3 for rotation, 3 for translation\nstruct Error2D {\n  Error2D(double pt_x, double pt_y, double pt_z, double pix_x, double pix_y)\n      : pt_x(pt_x), pt_y(pt_y), pt_z(pt_z), pix_x(pix_x), pix_y(pix_y) {}\n  template <typename T>\n  bool operator()(const T* const camera,\n                  T* residuals) const {\n    // camera[0,1,2] are the angle-axis rotation.\n    T p[3];\n    T point[3];\n    point[0] = T(pt_x);\n    point[1] = T(pt_y);\n    point[2] = T(pt_z);\n    ceres::AngleAxisRotatePoint(camera, point, p);\n  \n    // camera[3,4,5] are the translation.\n    p[0] += camera[3];\n    p[1] += camera[4];\n    p[2] += camera[5];\n\n    // z transform\n    T xp = p[0] / p[2];\n    T yp = p[1] / p[2];\n\n    // Compute final projected point position.\n    const T fx = T(718.8560);\n    const T fy = T(718.8560);\n    const T cx = T(607.1928);\n    const T cy = T(185.2157);\n    T predicted_x = fx * xp + cx;\n    T predicted_y = fy * yp + cy;\n\n    // The error is the difference between the predicted and observed position.\n    residuals[0] = predicted_x - pix_x;\n    residuals[1] = predicted_y - pix_y;\n\n    return true;\n  }\n  const double pt_x;\n  const double pt_y;\n  const double pt_z;\n  const double pix_x;\n  const double pix_y;\n};\n\n\n\n// optimizes rotation and translation with nonlinear optimization (minimizing reprojection error).\n// optimizes inliers from PnP only.\nvoid optimize_transformation(cv::Mat& rotation, cv::Mat& translation, cv::Mat & points3D, \n    std::vector<cv::Point2f>& pointsLeft, cv::Mat& inliers, cv::Mat & projection_matrix)\n{\n    static bool init = false;\n    if (!init)\n    {\n        init = true;\n        google::InitGoogleLogging(\"vo\"); // TODO PUT SOMEWHERE ELSE\n    }\n    // initial transformation\n    double* camera = new double[6];\n    camera[0] = rotation.at<double>(0,0);\n    camera[1] = rotation.at<double>(1,0);\n    camera[2] = rotation.at<double>(2,0);\n    camera[3] = translation.at<double>(0,0);\n    camera[4] = translation.at<double>(1,0);\n    camera[5] = translation.at<double>(2,0);\n\n    int num_inliers = int(inliers.size().height);\n    double camera_init[6];\n    for (int i = 0; i < 6; i++) camera_init[i] = camera[i];\n    ceres::Problem problem;\n    for (int i = 0; i < num_inliers; i++)\n    {\n        int idx = inliers.at<int>(0,i);\n        problem.AddResidualBlock(\n        new ceres::AutoDiffCostFunction<Error2D, 2, 6>( // dimension of residual, dimension of camera\n            new Error2D(points3D.at<float>(idx,0), points3D.at<float>(idx,1), points3D.at<float>(idx,2), pointsLeft[idx].x, pointsLeft[idx].y)),\n        NULL,\n        camera);\n    }\n\n    ceres::Solver::Options options;\n    options.max_num_iterations = 25;\n    options.linear_solver_type = ceres::DENSE_SCHUR;\n    options.minimizer_progress_to_stdout = true;\n    ceres::Solver::Summary summary;\n    ceres::Solve(options, &problem, &summary);\n    std::cout << summary.BriefReport() << \"\\n\";\n    std::cout << \"Initial r: \" << camera_init[0] << \" \" << camera_init[1] << \" \" << camera_init[2] << \"\\n\";\n    std::cout << \"Initial t: \" << camera_init[3] << \" \" << camera_init[4] << \" \" << camera_init[5] << \"\\n\";\n    std::cout << \"Final r: \" << camera[0] << \" \" << camera[1] << \" \" << camera[2] << \"\\n\";\n    std::cout << \"Final t: \" << camera[3] << \" \" << camera[4] << \" \" << camera[5] << \"\\n\";\n\n}\n\n\nvoid trackingFrame2Frame(cv::Mat& projMatrl, cv::Mat& projMatrr,\n                         std::vector<cv::Point2f>&  pointsLeft_t1, \n                         cv::Mat& points3D_t0,\n                         cv::Mat& rotation,\n                         cv::Mat& translation,\n                         bool mono_rotation)\n{\n\n    // Calculate frame to frame transformation\n    cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);   \n    cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);\n    cv::Mat intrinsic_matrix = (cv::Mat_<float>(3, 3) << projMatrl.at<float>(0, 0), projMatrl.at<float>(0, 1), projMatrl.at<float>(0, 2),\n                                                projMatrl.at<float>(1, 0), projMatrl.at<float>(1, 1), projMatrl.at<float>(1, 2),\n                                                projMatrl.at<float>(1, 1), projMatrl.at<float>(1, 2), projMatrl.at<float>(1, 3));\n\n    int iterationsCount = 500;        // number of Ransac iterations.\n    float reprojectionError = .5;    // maximum allowed distance to consider it an inlier.\n    float confidence = 0.999;          // RANSAC successful confidence.\n    bool useExtrinsicGuess = true;\n    int flags =cv::SOLVEPNP_ITERATIVE;\n\n    #if 1\n    cv::Mat inliers; \n    cv::solvePnPRansac( points3D_t0, pointsLeft_t1, intrinsic_matrix, distCoeffs, rvec, translation,\n                        useExtrinsicGuess, iterationsCount, reprojectionError, confidence,\n                        inliers, flags );\n    #endif\n    #if 0\n    std::vector<int> inliers;\n    cv::cuda::solvePnPRansac(points3D_t0.t(), cv::Mat(1, (int)pointsLeft_t1.size(), CV_32FC2, &pointsLeft_t1[0]),\n                        intrinsic_matrix, cv::Mat(1, 8, CV_32F, cv::Scalar::all(0)),\n                        rvec, translation, false, 200, 0.5, 20, &inliers);\n    #endif\n\n    // nonlinear optimization after, minimizing reprojection error\n    //optimize_transformation(rvec,translation,points3D_t0,pointsLeft_t1,inliers, projMatrl);\n    cv::Rodrigues(rvec, rotation);\n    std::cout << \"[trackingFrame2Frame] inliers size: \" << inliers.size()  << \" out of \" << pointsLeft_t1.size() << std::endl;\n\n}\n\nvoid displayPoints(cv::Mat& image, std::vector<cv::Point2f>&  points)\n{\n    int radius = 2;\n    cv::Mat vis;\n\n    cv::cvtColor(image, vis, cv::COLOR_GRAY2BGR, 3);\n\n    for (int i = 0; i < points.size(); i++)\n    {\n        cv::circle(vis, cv::Point(points[i].x, points[i].y), radius, CV_RGB(0,255,0));\n    }\n\n    cv::imshow(\"vis \", vis );  \n    cv::waitKey(1);\n}\n\nvoid displayTracking(cv::Mat& imageLeft_t1, \n                     std::vector<cv::Point2f>&  pointsLeft_t0,\n                     std::vector<cv::Point2f>&  pointsLeft_t1)\n{\n    // -----------------------------------------\n    // Display feature racking\n    // -----------------------------------------\n    int radius = 2;\n    cv::Mat vis;\n\n    cv::cvtColor(imageLeft_t1, vis, cv::COLOR_GRAY2BGR, 3);\n\n    for (int i = 0; i < pointsLeft_t0.size(); i++)\n    {\n      cv::circle(vis, cv::Point(pointsLeft_t0[i].x, pointsLeft_t0[i].y), radius, CV_RGB(0,255,0));\n    }\n\n    for (int i = 0; i < pointsLeft_t1.size(); i++)\n    {\n      cv::circle(vis, cv::Point(pointsLeft_t1[i].x, pointsLeft_t1[i].y), radius, CV_RGB(255,0,0));\n    }\n\n    for (int i = 0; i < pointsLeft_t1.size(); i++)\n    {\n      cv::line(vis, pointsLeft_t0[i], pointsLeft_t1[i], CV_RGB(0,255,0));\n    }\n\n    cv::imshow(\"vis \", vis );  \n    cv::waitKey(1);\n}\n"
        }
    ]
}